{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 - Graph Embeddings\n",
    "\n",
    "In this notebook, we illustrate several graph embedding algorithms, we show how we can compare embeddings using an unsupervised framework, and we look at various applications such as visualization, clustering, link prediction and classification.\n",
    "\n",
    "### Things to install:\n",
    "\n",
    "We use a Julia package from https://github.com/KrainskiL/CGE.jl to compare graph embeddings.\n",
    "Follow the instructions from that GitHub repository to install it.\n",
    "\n",
    "The CLI script to call CGE is also available in ```CGE/CGE_CLI.jl``` in the book's GitHub repository: https://github.com/ftheberge/GraphMiningNotebooks\n",
    "\n",
    "Results presented in the book were run on MacOS. Most results are identical on Linux (we use seeds), but we found that Node2Vec can yield slightly different results. This can lead to small differences in some results, but not in the conclusions. \n",
    "\n",
    "Set the path(s) in the second cell below. \n",
    "\n",
    "### Windows users:\n",
    "\n",
    "You need to change ```cp``` to ```copy``` in a the ```test_embeddings``` function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import partition_igraph\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.sparse.linalg as lg\n",
    "import subprocess\n",
    "import os\n",
    "import umap\n",
    "import fastnode2vec as n2v\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "from sklearn.metrics import calinski_harabasz_score as CHS\n",
    "from sklearn.metrics import adjusted_mutual_info_score as AMI\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import pickle\n",
    "import random \n",
    "import sknetwork as skn\n",
    "from sknetwork.classification import get_accuracy_score\n",
    "import math\n",
    "from scipy.stats import kendalltau as tau\n",
    "import warnings \n",
    "## node and edge greyscale colors\n",
    "cls_edges = 'gainsboro'\n",
    "cls = ['silver','dimgray','black']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the data directory\n",
    "datadir = '../Datasets/'\n",
    "\n",
    "## path to julia\n",
    "julia = '~/julia-1.7.2/bin/julia'\n",
    "#julia = '/Applications/Julia-1.7.app/Contents/Resources/julia/bin/julia ' ## you may need the full path here\n",
    "\n",
    "## location of the CGE Julia script (Comparing Graph Embeddings framework)\n",
    "CGE = '../CGE/CGE_CLI.jl '\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## as defined in Table 1 of node2vec paper for link prediction:\n",
    "## https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf\n",
    "    \n",
    "def binary_operator(u, v, op='had'):\n",
    "    if op=='had':\n",
    "        return u * v\n",
    "    if op=='l1':\n",
    "        return np.abs(u - v)\n",
    "    if op=='l2':\n",
    "        return (u - v) ** 2\n",
    "    if op=='avg':\n",
    "        return (u + v) / 2.0\n",
    "\n",
    "## read embedding from disk, in node2vec format\n",
    "def readEmbedding(fn=\"_embed\"):\n",
    "    D = pd.read_csv(fn, sep=' ', skiprows=1, header=None)\n",
    "    D = D.dropna(axis=1)\n",
    "    Y = np.array(D.iloc[:,1:])\n",
    "    return Y\n",
    "\n",
    "## Read embedding from file in node2vec format\n",
    "## Map to 2d layout format, using UMAP if dim > 2\n",
    "def embed2layout(fn=\"_embed\", seed=123, n_jobs=1):\n",
    "    D = pd.read_csv(fn, sep=' ', skiprows=1, header=None)\n",
    "    D = D.dropna(axis=1)\n",
    "    D = D.sort_values(by=0)\n",
    "    Y = np.array(D.iloc[:,1:])\n",
    "    if Y.shape[1]>2:\n",
    "        Y = umap.UMAP(random_state=seed, n_jobs=n_jobs).fit_transform(Y)\n",
    "    ly = []\n",
    "    for v in range(Y.shape[0]):\n",
    "        ly.append((Y[v][0],Y[v][1]))\n",
    "    return ly\n",
    "\n",
    "\n",
    "## Computing Jensen-Shannon (JS) divergence with the Julia CGE framework code \n",
    "## given files: edgelist, communities and embedding\n",
    "def JS(edge_file, comm_file, embed_file, return_local=True, seed=123):\n",
    "    cmd = julia+' '+CGE+' -g '+edge_file+' -c '+comm_file+' -e '+embed_file+' --seed '+str(seed)+' 2>_stderr'\n",
    "    s = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "    x = s.stdout.decode().split(',')\n",
    "    if len(x[0]) == 0:\n",
    "        print('Error running CLI command:\\n', cmd )\n",
    "        print('Make sure Julia and CGE are correctly installed')\n",
    "        return -1\n",
    "    if return_local:\n",
    "        return (float(x[1]),float(x[5]))\n",
    "    else:\n",
    "        return float(x[1])\n",
    "\n",
    "## save embedding to disk to compute divergence with Julia CGE framework\n",
    "def saveEmbedding(X, g, fn='_embed'):\n",
    "    with open(fn,'w') as f:\n",
    "        f.write(str(X.shape[0]) + \" \" + str(X.shape[1])+'\\n')\n",
    "        for i in range(X.shape[0]):\n",
    "            f.write(g.vs[i]['name']+' ')\n",
    "            for j in range(X.shape[1]):\n",
    "                f.write(str(X[i][j])+' ')\n",
    "            f.write('\\n')\n",
    "\n",
    "## Hope embedding with various similarity functions\n",
    "def Hope(g, sim='katz', dim=2, verbose=False, beta=.01, alpha=.5):\n",
    "    ## For undirected graphs, embedding as source and target are identical\n",
    "    if g.is_directed() == False:\n",
    "        dim = dim*2\n",
    "    A = np.array(g.get_adjacency().data)\n",
    "    beta = beta\n",
    "    alpha = alpha\n",
    "    n = g.vcount()\n",
    "    ## Katz\n",
    "    if sim == 'katz':\n",
    "        M_g = np.eye(n) - beta * A\n",
    "        M_l = beta * A\n",
    "    ## Adamic-Adar\n",
    "    if sim == 'aa':\n",
    "        M_g = np.eye(n)\n",
    "        ## fix bug 1/x and take log();\n",
    "        D = np.diag([1/np.log(x) if x>1 else 0 for x in g.degree()]) \n",
    "        # D = np.diag([1/np.log(max(2,x)) for x in g.degree()]) \n",
    "        M_l = np.dot(np.dot(A,D),A)\n",
    "        np.fill_diagonal(M_l,0)\n",
    "    ## Common neighbors\n",
    "    if sim == 'cn':\n",
    "        M_g = np.eye(n)\n",
    "        M_l = np.dot(A,A)\n",
    "    ## presonalized page rank\n",
    "    if sim == 'ppr':\n",
    "        P = []\n",
    "        for i in range(n):\n",
    "            s = np.sum(A[i])\n",
    "            if s>0:\n",
    "                P.append([x/s for x in A[i]])\n",
    "            else:\n",
    "                P.append([1/n for x in A[i]])\n",
    "        P = np.transpose(np.array(P)) ## fix bug - take transpose\n",
    "        M_g = np.eye(n)-alpha*P\n",
    "        M_l = (1-alpha)*np.eye(n)\n",
    "    S = np.dot(np.linalg.inv(M_g), M_l)\n",
    "    u, s, vt = lg.svds(S, k=dim // 2)\n",
    "    X1 = np.dot(u, np.diag(np.sqrt(s)))\n",
    "    X2 = np.dot(vt.T, np.diag(np.sqrt(s)))\n",
    "    X = np.concatenate((X1, X2), axis=1)\n",
    "    p_d_p_t = np.dot(u, np.dot(np.diag(s), vt))\n",
    "    eig_err = np.linalg.norm(p_d_p_t - S)\n",
    "    if verbose:\n",
    "        print('SVD error (low rank): %f' % eig_err)\n",
    "    ## undirected graphs have identical source and target embeddings\n",
    "    if g.is_directed() == False:\n",
    "        d = dim//2\n",
    "        return X[:,:d]\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "## Laplacian eigenmaps embedding\n",
    "def LE(g, dim=2):\n",
    "    L_sym = np.array(g.laplacian(normalized=True))\n",
    "    w, v = lg.eigs(L_sym, k=dim + 1, which='SM')\n",
    "    idx = np.argsort(w) # sort eigenvalues\n",
    "    w = w[idx]\n",
    "    v = v[:, idx]\n",
    "    X = v[:, 1:]\n",
    "    return X.real\n",
    "\n",
    "# ## Returns a LaTeX bmatrix\n",
    "def bmatrix(a):\n",
    "    if len(a.shape) > 2:\n",
    "        raise ValueError('bmatrix can at most display two dimensions')\n",
    "    lines = str(a).replace('[', '').replace(']', '').splitlines()\n",
    "    rv = [r'\\begin{bmatrix}']\n",
    "    rv += ['  ' + ' & '.join(l.split()) + r'\\\\' for l in lines]\n",
    "    rv +=  [r'\\end{bmatrix}']\n",
    "    return '\\n'.join(rv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 6.2 in the Book\n",
    "\n",
    "This is to illustrate random walks on (directed) graphs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4321)\n",
    "g = ig.Graph.Erdos_Renyi(n=4,p=0,directed=True)\n",
    "g.vs['label'] = ['A','B','C','D']\n",
    "g.vs['color'] = 'white'\n",
    "g.add_edges([(0,1),(1,2),(1,3),(2,1),(3,2)])\n",
    "ly = g.layout_fruchterman_reingold()\n",
    "#ig.plot(g,'tiny.eps', layout=ly, bbox=(0,0,300,200),vertex_label_size=10)\n",
    "ig.plot(g, layout=ly, bbox=(0,0,300,200), vertex_label_size=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "* ```abcd```: is a small ABCD graph (100 nodes), mainly for visualization and quick examples\n",
    "* ```ABCD1```: is a larger ABCD graph (1000 nodes), for experiments. It is noisy with $\\xi=0.6$.\n",
    "* ```ABCD2```: Similar to ```ABCD1``` but less noisy with $\\xi=0.2$.\n",
    "* ```zac```: Zachary (karate club) graph, mainly for visualzation\n",
    "\n",
    "The small ```abcd``` graph was generated with the following parameters:\n",
    "\n",
    "```\n",
    "n = \"100\"                     # number of vertices in graph\n",
    "t1 = \"3\"                      # power-law exponent for degree distribution\n",
    "d_min = \"5\"                   # minimum degree\n",
    "d_max = \"15\"                  # maximum degree\n",
    "d_max_iter = \"1000\"           # maximum number of iterations for sampling degrees\n",
    "t2 = \"2\"                      # power-law exponent for cluster size distribution\n",
    "c_min = \"25\"                  # minimum cluster size\n",
    "c_max = \"50\"                  # maximum cluster size\n",
    "xi = \"0.2\"                    # fraction of edges to fall in background graph\n",
    "```\n",
    "\n",
    "The larger ```ABCD1``` and ```ABCD2``` graphs were generated with the following parameters:\n",
    "\n",
    "```\n",
    "n = \"1000\"                     # number of vertices in graph\n",
    "t1 = \"3\"                       # power-law exponent for degree distribution\n",
    "d_min = \"10\"                   # minimum degree\n",
    "d_max = \"100\"                  # maximum degree\n",
    "d_max_iter = \"1000\"            # maximum number of iterations for sampling degrees\n",
    "t2 = \"2\"                       # power-law exponent for cluster size distribution\n",
    "c_min = \"50\"                   # minimum cluster size\n",
    "c_max = \"150\"                  # maximum cluster size\n",
    "xi = \"0.6\" or \"0.2\"            # fraction of edges to fall in background graph\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the small ABCD graph and visualize\n",
    "\n",
    "Beware: node names are 1-based and are distinct from vertex ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read graph and communities\n",
    "abcd = ig.Graph.Read_Ncol(datadir+'ABCD/abcd_100.dat',directed=False)\n",
    "c = np.loadtxt(datadir+'ABCD/abcd_100_comms.dat',dtype='uint16',usecols=(1))\n",
    "\n",
    "## assign correct community based on node names\n",
    "abcd.vs['comm'] = [c[int(x['name'])-1] for x in abcd.vs]\n",
    "\n",
    "## print a few stats\n",
    "print(abcd.vcount(),'vertices,',abcd.ecount(),'edges,','mean degreee:',np.mean(abcd.degree()),\n",
    "      ', no. of communities:',max(abcd.vs['comm']))\n",
    "\n",
    "## define the colors and node sizes here\n",
    "abcd.vs['size'] = 7\n",
    "abcd.es['color'] = cls_edges\n",
    "abcd.vs['color'] = [cls[i-1] for i in abcd.vs['comm']]\n",
    "ig.plot(abcd, bbox=(0,0,300,200))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the larger ABCD graphs and visualize\n",
    "\n",
    "```ABCD1```: this is a larger graph with lots of noise edges ($\\xi$=0.6). Node colours refer to the communities.\n",
    "With this amount of noise, the communities are far from obvious on a 2-dimensional layout.\n",
    "\n",
    "```ABCD2```: the second graph has stronger communities ($\\xi$=0.2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read graph and communities\n",
    "ABCD1 = ig.Graph.Read_Ncol(datadir+'ABCD/abcd_1000.dat',directed=False)\n",
    "c = np.loadtxt(datadir+'ABCD/abcd_1000_comms.dat',dtype='uint16',usecols=(1))\n",
    "ABCD1.vs['comm'] = [c[int(x['name'])-1] for x in ABCD1.vs]\n",
    "\n",
    "## print a few stats\n",
    "print(ABCD1.vcount(),'vertices,',ABCD1.ecount(),'edges,','mean degreee:',np.mean(ABCD1.degree()),\n",
    "      ', no. of communities:',max(ABCD1.vs['comm']))\n",
    "\n",
    "## define the colors and node sizes here\n",
    "## node colors map to communities\n",
    "ABCD1.vs['size'] = 5\n",
    "ABCD1.es['color'] = 'gainsboro'\n",
    "pal = ig.RainbowPalette(n=max(ABCD1.vs['comm'])+1) \n",
    "ABCD1.vs['color'] = [pal.get(int(i)) for i in ABCD1.vs['comm']]\n",
    "\n",
    "ig.plot(ABCD1, bbox=(0,0,400,300)) ## communities are far from obvious in 2d layout!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read graph and communities\n",
    "ABCD2 = ig.Graph.Read_Ncol(datadir+'ABCD/abcd_1000_xi2.dat',directed=False)\n",
    "c = np.loadtxt(datadir+'ABCD/abcd_1000_xi2_comms.dat',dtype='uint16',usecols=(1))\n",
    "ABCD2.vs['comm'] = [c[int(x['name'])-1] for x in ABCD2.vs]\n",
    "\n",
    "## print a few stats\n",
    "print(ABCD2.vcount(),'vertices,',ABCD2.ecount(),'edges,','mean degreee:',np.mean(ABCD2.degree()),\n",
    "      ', no. of communities:',max(ABCD2.vs['comm']))\n",
    "\n",
    "## define the colors and node sizes here\n",
    "## node colors map to communities\n",
    "ABCD2.vs['size'] = 5\n",
    "ABCD2.es['color'] = 'gainsboro'\n",
    "pal = ig.RainbowPalette(n=max(ABCD2.vs['comm'])+1) \n",
    "ABCD2.vs['color'] = [pal.get(int(i)) for i in ABCD2.vs['comm']]\n",
    "\n",
    "ig.plot(ABCD2, bbox=(0,0,400,300)) ## communities are clear here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph layouts \n",
    "\n",
    "We show a variety of graph layout functions available in `igraph` on the Zachary graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zachary (karate club) graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zac = ig.Graph.Read_Ncol(datadir+'Zachary/zachary.edgelist',directed=False)\n",
    "c = np.loadtxt(datadir+'Zachary/zachary.communities',dtype='uint16')\n",
    "zac.vs['comm'] = [c[int(x['name'])] for x in zac.vs]\n",
    "\n",
    "## layout stored from Chapter 5 notebook\n",
    "## this was generated using the Fruchterman-Reingold method\n",
    "with open(datadir+\"Zachary/layout.pkl\",\"rb\") as fn:\n",
    "    ly_zac = pickle.load(fn)\n",
    "zac['layout'] = [ly_zac[int(x['name'])] for x in zac.vs] \n",
    "\n",
    "## other parameters\n",
    "zac.vs['size'] = 7\n",
    "zac.es['color'] = cls_edges\n",
    "zac.vs['color'] = [cls[i*2] for i in zac.vs['comm']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fruchterman-Reingold layout (saved layout)\n",
    "#ig.plot(zac, 'layout_fr.eps', layout=zac.vs['layout'], bbox=(0,0,300,200))\n",
    "ig.plot(zac, bbox=(0,0,300,200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kamada-Kawai layout\n",
    "random.seed(123)\n",
    "ly = zac.layout('kk')\n",
    "\n",
    "#ig.plot(zac, 'layout_kk.eps', layout=ly, bbox=(0,0,300,200))\n",
    "ig.plot(zac, layout=ly, bbox=(0,0,300,200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multidimensional scaling layout\n",
    "random.seed(123)\n",
    "ly = zac.layout('mds')\n",
    "\n",
    "#ig.plot(zac, 'layout_mds.eps', layout=ly, bbox=(0,0,300,200))\n",
    "ig.plot(zac, layout=ly, bbox=(0,0,300,200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Circular layout\n",
    "random.seed(123)\n",
    "ly = zac.layout('circle')\n",
    "\n",
    "#ig.plot(zac, 'layout_circle.eps', layout=ly, bbox=(0,0,300,200))\n",
    "ig.plot(zac, layout=ly, bbox=(0,0,300,200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grid layout\n",
    "random.seed(123)\n",
    "ly = zac.layout('grid')\n",
    "\n",
    "#ig.plot(zac, 'layout_grid.eps', layout=ly, bbox=(0,0,300,200))\n",
    "ig.plot(zac, layout=ly, bbox=(0,0,300,200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sugiyama (tree-like) layout\n",
    "random.seed(123)\n",
    "ly = zac.layout('sugiyama')\n",
    "\n",
    "#ig.plot(zac, 'layout_tree.eps', layout=ly, bbox=(0,0,300,200))\n",
    "ig.plot(zac, layout=ly, bbox=(0,0,300,200))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and compare several embeddings -- Zachary graph\n",
    "\n",
    "We run a few graph embedding algorithms on the Zachary graph with\n",
    "different parameters. \n",
    "\n",
    "For example, we try different embedding dimensions.\n",
    "\n",
    "We run the following:\n",
    "* node2vec with different values for $p$ and $q$\n",
    "* HOPE with different similarities\n",
    "* Laplacian Eigenmaps (LE)\n",
    "\n",
    "For each embedding, we use the communities obtained with **ECG** along with the **CGE** framework to compute the **graph embedding divergence** with the **CGE** Julia package. We visualize some good and bad results w.r.t. the global divergence score.\n",
    "\n",
    "For embeddings with low divergence, we see good separation of the communities (even in 2-dim projection, using **UMAP**), while this is not the case for embeddings with high divergence.\n",
    "\n",
    "Since we are going to compare embeddings for several graphs, we write the procedure as a function below. This function keeps a local copy of the best (`_embed_best`) and worst (`_embed_worst`) embeddings on disk, and returns the JS divergence (including local) for every test.\n",
    "\n",
    "#### Windows users:\n",
    "\n",
    "change ```cp``` to ```copy``` below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### compare several embedding methods\n",
    "def test_embeddings(G, edgefile, commfile, run_hope=True, run_le=True, run_n2v=True, \n",
    "                    Dims=[2,4,8], local=False, verbose=False, seed=123):\n",
    "    L = []         ## to store results\n",
    "    best_jsd = 1   ## we'll keep track of best (global or local) JS-divergence,\n",
    "    worst_jsd = 0  ## and worst one.\n",
    "    \n",
    "    if(run_hope):\n",
    "        for dim in Dims:\n",
    "            for sim in ['katz','ppr','cn','aa']:\n",
    "                X = Hope(G, sim=sim, dim=dim) \n",
    "                saveEmbedding(X, G)\n",
    "                div = JS(edgefile, commfile,'_embed')      \n",
    "                jsd = div[int(local)]\n",
    "                if verbose:\n",
    "                    print(dim,sim,jsd)\n",
    "                if jsd < best_jsd:\n",
    "                    os.system('cp _embed _embed_best')\n",
    "                    best_jsd = jsd\n",
    "                if jsd > worst_jsd:\n",
    "                    os.system('cp _embed _embed_worst')\n",
    "                    worst_jsd = jsd\n",
    "                L.append([dim,'hope',sim,div[0],div[1]])\n",
    "                \n",
    "    if(run_le):\n",
    "        for dim in Dims:\n",
    "            X = LE(G, dim=dim)\n",
    "            saveEmbedding(X, G)\n",
    "            div = JS(edgefile, commfile,'_embed')\n",
    "            jsd = div[int(local)]\n",
    "            if verbose:\n",
    "                print(dim,jsd)\n",
    "            if jsd < best_jsd:\n",
    "                os.system('cp _embed _embed_best')\n",
    "                best_jsd = jsd\n",
    "            if jsd > worst_jsd:\n",
    "                os.system('cp _embed _embed_worst')\n",
    "                worst_jsd = jsd\n",
    "            L.append([dim,'le',' ',div[0],div[1]])\n",
    "\n",
    "    if(run_n2v):\n",
    "        graph = n2v.Graph(G.to_tuple_list(), directed=False, weighted=False)\n",
    "        for dim in Dims:\n",
    "            for (p,q) in [(1,0.5),(0.5,1),(1,1)]:\n",
    "                ## using default values from original node2vec code\n",
    "                nv = n2v.Node2Vec(graph, dim=dim, p=p, q=q, walk_length=80, window=5, seed=seed)\n",
    "                nv.train(epochs=10, verbose=False)\n",
    "                Y = np.array([nv.wv[i] for i in range(len(nv.wv))])\n",
    "                saveEmbedding(Y, G)\n",
    "                div = JS(edgefile,commfile, '_embed')\n",
    "                jsd = div[int(local)]\n",
    "                if verbose:\n",
    "                    print(dim,p,q,jsd)\n",
    "                if jsd < best_jsd:\n",
    "                    os.system('cp _embed _embed_best')\n",
    "                    best_jsd = jsd\n",
    "                if jsd > worst_jsd:\n",
    "                    os.system('cp _embed _embed_worst')\n",
    "                    worst_jsd = jsd\n",
    "\n",
    "                ## store results\n",
    "                L.append([dim,'n2v',str(p)+' '+str(q),div[0],div[1]])\n",
    "    \n",
    "    ## store results in a dataframe and sort\n",
    "    D = pd.DataFrame(L,columns=['dim','algo','param','jsd','local_jsd'])\n",
    "    if local:\n",
    "        D = D.sort_values(by='local_jsd',axis=0)\n",
    "    else:\n",
    "        D = D.sort_values(by='jsd',axis=0)\n",
    "    return D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below takes about one minute to run as several embeddings are tested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "D = test_embeddings(zac, Dims=[2,4], edgefile=datadir+'Zachary/zachary.edgelist', \n",
    "                    commfile=datadir+'Zachary/zachary.ecg')\n",
    "D.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot top results\n",
    "l = embed2layout(fn=\"_embed_best\", seed=123)\n",
    "ly = [l[int(v['name'])] for v in zac.vs]\n",
    "#ig.plot(zac, 'zac_low_div.eps', layout=zac.vs['ly'], bbox=(0,0,300,200))\n",
    "ig.plot(zac, layout=ly, bbox=(0,0,300,200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## plot result with largest divergence\n",
    "l = embed2layout(fn=\"_embed_worst\", seed=123)\n",
    "ly = [l[int(v['name'])] for v in zac.vs]\n",
    "#ig.plot(zac, 'zac_high_div.eps', layout=zac.vs['ly'], bbox=(0,0,300,200))\n",
    "ig.plot(zac,layout=ly, bbox=(0,0,300,200))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare embeddings -- small ABCD  graph\n",
    "\n",
    "This is the same exercise as what we did above, this time for the 100-nodes ABCD graph.\n",
    "\n",
    "We look at slightly higher embedding dimension as there are more nodes than the Zachary graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### effect of local divergence score\n",
    "\n",
    "So far we considered the global Jenssen-Shannon divergence, where the objective is to preserve the community structure.\n",
    "\n",
    "We show the best result with respect to the global divergence below, and we see that it preserves the community structure. We may want better separation of the nodes within community, based on their connectivity. This is what the local Jenssen-Shannon divergence can provide. \n",
    "\n",
    "Below we also show an embedding with lower local divergence. The result is an embedding that still preserves community structure, but nodes within community are more separated than with the global divergence.\n",
    "\n",
    "The code below takes about one minute to run as several embeddings are tested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "D = test_embeddings(abcd, Dims=[2,16], edgefile=datadir+'ABCD/abcd_100.dat', \n",
    "                    commfile=datadir+'ABCD/abcd_100.ecg')\n",
    "D.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(D['jsd']/min(D['jsd']),D['local_jsd']/min(D['local_jsd']),'o',color='black', base=2)\n",
    "plt.xlabel('Global divergence score (normalized)', fontsize=14)\n",
    "plt.ylabel('Local divergence score (normalized)', fontsize=14)\n",
    "#plt.savefig('abcd_div_1.eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: warning below (and a few more later) caused by UMAP; not changed yet to maintain backward compatibility\n",
    "\n",
    "run the line below to ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot result with lowest global JS divergence\n",
    "l = embed2layout(fn=\"_embed_best\", seed=123)\n",
    "abcd.vs['ly'] = [l[int(v['name'])-1] for v in abcd.vs]\n",
    "ig.plot(abcd, layout=abcd.vs['ly'], bbox=(0,0,300,200))\n",
    "#ig.plot(abcd, \"abcd_div_2.eps\", layout=abcd.vs['ly'], bbox=(0,0,300,200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## look at results with low local divergence\n",
    "D.sort_values(by='local_jsd').head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot one of the top ones\n",
    "X = Hope(abcd, sim='ppr', dim=16)\n",
    "saveEmbedding(X, abcd)\n",
    "l = embed2layout(seed=123)\n",
    "abcd.vs['ly'] = [l[int(v['name'])-1] for v in abcd.vs]\n",
    "ig.plot(abcd, layout=abcd.vs['ly'], bbox=(0,0,300,200))\n",
    "#ig.plot(abcd, \"abcd_div_3.eps\", layout=abcd.vs['ly'], bbox=(0,0,300,200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot one of the bottom ones\n",
    "X = Hope(abcd, sim='katz', dim=2)\n",
    "saveEmbedding(X, abcd)\n",
    "l = embed2layout(seed=123)\n",
    "abcd.vs['ly'] = [l[int(v['name'])-1] for v in abcd.vs]\n",
    "ig.plot(abcd, layout=abcd.vs['ly'], bbox=(0,0,300,200))\n",
    "#ig.plot(abcd, \"abcd_div_4.eps\", layout=abcd.vs['ly'], bbox=(0,0,300,200))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on the larger ABCD graph\n",
    "\n",
    "We saw that embedding can be used to visualize graphs. Below we use graph embedding as a way to define a feature vector (a point in vector space) for each node, and we use this representation to train a classifier.\n",
    "\n",
    "We use the ```ABCD1``` (noisy) graph.\n",
    "\n",
    "We use a saved embedding (48-dimension running HOPE with 'ppr' similarity).\n",
    "\n",
    "We split the data (the nodes) into a training and testing set. Using the training set, we build a **random forest** classification model where the classes are the communities for each node.\n",
    "\n",
    "We then apply this model to the test set.\n",
    "\n",
    "The graph has 1000 nodes; we use 250 nodes for training and the rest for testing; we obtain good accuracy (around 90%).\n",
    "\n",
    "What do you think will happen if we increase/decrease the size of the training set?\n",
    "\n",
    "We also report the confusion matrix (details in section 6.7 of the book).\n",
    "\n",
    "Finally, we compare with results obtained via a baseline **random** classifier where we supply the correct number of classes and their relative sizes. We see that our random forest model gives much better results than with a random classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load a saved embedding for the ABCD graph\n",
    "X = readEmbedding(fn=datadir+\"ABCD/abcd_1000_embed_best\")\n",
    "y = ABCD1.vs['comm']\n",
    "\n",
    "## train/test split\n",
    "np.random.seed(1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with 100 trees\n",
    "model = RandomForestClassifier(n_estimators=100, \n",
    "                               bootstrap = True,\n",
    "                               max_features = 'sqrt')\n",
    "# Fit on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Class predictions on test data\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "## percent correct -- this can vary slightly as we split train/test randomly\n",
    "print('\\naccuracy:',sum(cm.diagonal())/sum(sum(cm)),'\\n')\n",
    "#print(bmatrix(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare with random classifier\n",
    "## assuming we know the number of classes (12) \n",
    "## and using class proportions from training data\n",
    "np.random.seed(123)\n",
    "ctr = Counter(y_train)\n",
    "x = [ctr[i+1] for i in range(12)]\n",
    "s = np.sum(x)\n",
    "p = [i/s for i in x]\n",
    "acc = []\n",
    "for rep in range(30): ## repeat 30 times, we'll take average\n",
    "    y_pred = [x+1 for x in np.random.choice(12, size=len(y_test), replace=True,p=p)]\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    acc.append(sum(cm.diagonal())/sum(sum(cm)))\n",
    "## accuracy\n",
    "print('\\nAverage accuracy:',np.mean(acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering in embedded space\n",
    "\n",
    "Again using the larger (noisy) ```ABCD1``` graph, we run some graph clustering algorithms (Leiden and ECG).\n",
    "\n",
    "We run each algorithm several times are report two statistics:\n",
    "* the modularity score of the clustering, and\n",
    "* the adjusted mutual information (AMI) score when comparing with ground-truth (GT) communities.\n",
    "\n",
    "We also try seeding Leiden with initial clusters obtained with k-means (in embedded space) where k=100.\n",
    "\n",
    "We also run k-means (with 5 choices for k, including correct value) in embedded vector space.\n",
    "\n",
    "We use the same saved embedding than in the previous experiment. \n",
    "\n",
    "This time, we report:\n",
    "* the CHS score (Calinski and Harabasz score, or Variance Ratio Criterion); higher value is indicative of better quality clustering\n",
    "* the adjusted mutual information (AMI) score when comparing with ground-truth (GT) communities.\n",
    "\n",
    "In practical applications where we do not have access to the ground-truth, we need some other measure to quantify the quality of the clusters we obtain, such as modularity or CHS. We report AMI for runs with highest score (w.r.t. modularity or CHS) for each clustering algorithm.\n",
    "\n",
    "The cell below can take a few minutes to run. Results are provided in a pkl file. Uncomment the cell below to re-run.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "%%time\n",
    "## load the saved low divergence embedding\n",
    "ABCD1_emb = readEmbedding(fn=datadir+\"ABCD/abcd_1000_embed_best\")\n",
    "\n",
    "L = [] ## to store results\n",
    "K = [6,9,12,15,24] ## for k-means (real number of clusters is 12)\n",
    "Repeats = 30 ## number of repeats; decrease for faster run\n",
    "\n",
    "for i in range(Repeats):\n",
    "    \n",
    "    ## run kmeans\n",
    "    for k in K:\n",
    "        cl = KMeans(n_clusters=k, n_init=10).fit(ABCD1_emb)\n",
    "        d = {k:v for k,v in enumerate(cl.labels_)}\n",
    "        scr = CHS(ABCD1_emb, cl.labels_) ## CHS\n",
    "        ami = AMI(ABCD1.vs['comm'], list(d.values())) ## AMI vs ground truth\n",
    "        L.append(['km'+str(k),scr,ami])\n",
    "\n",
    "    ## ECG\n",
    "    ec = ABCD1.community_ecg().membership\n",
    "    scr = ABCD1.modularity(ec) ## modularity\n",
    "    ami = AMI(ABCD1.vs['comm'],ec) ## AMI vs ground truth\n",
    "    L.append(['ecg',scr,ami])\n",
    "    \n",
    "    ## Leiden \n",
    "    lei = ABCD1.community_leiden(objective_function='modularity')\n",
    "    scr = ABCD1.modularity(lei) ## modularity\n",
    "    ami = AMI(ABCD1.vs['comm'],lei.membership) ## AMI vs ground truth\n",
    "    L.append(['lei', scr, ami])\n",
    "    \n",
    "    ## kmeans+Leiden\n",
    "    cl = KMeans(n_clusters=100, n_init=10).fit(ABCD1_emb)\n",
    "    d = {k:v for k,v in enumerate(cl.labels_)}\n",
    "    lei = ABCD1.community_leiden(objective_function='modularity', initial_membership=cl.labels_)\n",
    "    scr = ABCD1.modularity(lei) ## modularity\n",
    "    ami = AMI(ABCD1.vs['comm'],lei.membership) ## AMI vs ground truth\n",
    "    L.append(['km+lei', scr, ami])\n",
    "\n",
    "## store in dataframe\n",
    "D = pd.DataFrame(L,columns=['algo','scr','ami'])\n",
    "\n",
    "## save results\n",
    "#pickle.dump( D, open( datadir+\"ABCD/abcd_1000_clustering.pkl\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load results from above cell\n",
    "## load test results\n",
    "with open(datadir+\"ABCD/abcd_1000_clustering.pkl\",\"rb\") as fn:\n",
    "    D = pickle.load(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AMI results with best scoring clustering for the 3 algorithms\n",
    "x = list(D[[x.startswith('km') for x in D['algo']]].sort_values(by='scr',ascending=False)['ami'])[0]\n",
    "print('K-Means best, AMI:',x)\n",
    "\n",
    "x = list(D[D['algo']=='lei'].sort_values(by='scr',ascending=False)['ami'])[0]\n",
    "print('Leiden best, AMI:',x)\n",
    "\n",
    "x = list(D[D['algo']=='ecg'].sort_values(by='scr',ascending=False)['ami'])[0]\n",
    "print('ECG best, AMI:',x)\n",
    "\n",
    "x = list(D[D['algo']=='km+lei'].sort_values(by='scr',ascending=False)['ami'])[0]\n",
    "print('K-Means+Leiden best, AMI:',x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we summarize the results for all runs in a boxplot. \n",
    "\n",
    "Results with k-means are best when we supply the correct number of clusters (12). \n",
    "\n",
    "We see excellent results when using ECG or Leiden, in particular with the initial partition provided by k-means with large k=100.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## boxplot the AMI results \n",
    "A = []\n",
    "algo = ['km6','km9','km12','km15','km24','lei','km+lei','ecg']\n",
    "for a in algo:\n",
    "    A.append(D[D['algo']==a]['ami'])\n",
    "\n",
    "B = pd.DataFrame(np.transpose(A), \n",
    "                 columns=['k-means(6)','k-means(9)','k-means(12)','k-means(15)',\n",
    "                          'k-means(24)','Leiden','k-Leiden','ECG'])\n",
    "B.boxplot(rot=30,figsize=(7,5))\n",
    "plt.ylabel('Adjusted Mutual Information (AMI)')\n",
    "plt.subplots_adjust(bottom=.125)\n",
    "#plt.savefig('embed_cluster.eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we cluster using the DBSCAN algorithm after reducing the dimension via UMAP.\n",
    "Running a good dimension reduction algorithm such as UMAP before clustering in vector space often gives better results. This is for illustration and you can experiment with different choices of parameter below as well as diffferent clustering algorithms such as HDBSCAN.\n",
    "\n",
    "DBSCAN does not always cluster all the points, which can be quite useful in practice. Some points can be tagged as *outliers*. Below, we compute AMI with and without the outlying points. \n",
    "Result without outliers is quite good (recall that unlike k-means, we do not supply the number of communities here).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the saved low divergence embedding\n",
    "ABCD1_emb = readEmbedding(fn=datadir+\"ABCD/abcd_1000_embed_best\")\n",
    "\n",
    "## We tried a few 'min_sample' and 'dim' with good results using 8 and 16 resp.\n",
    "top = 0\n",
    "dim = 16  ## reduce to this dimension\n",
    "ms = 8    ## min-sample in DBSCAN\n",
    "U = umap.UMAP(n_components=dim, random_state=1234, n_jobs=1).fit_transform(ABCD1_emb)\n",
    "## We try various 'eps' and pick the best via calinski_harabasz_score (CHS)\n",
    "for e in np.arange(.4,.5,.0025): \n",
    "    cl = DBSCAN(eps=e, min_samples=ms).fit(U)\n",
    "    labels = cl.labels_\n",
    "    s = CHS(U,labels) ## CHS score\n",
    "    if s > top:\n",
    "        top=s\n",
    "        e_top=e\n",
    "\n",
    "## result with best CHS score\n",
    "cl = DBSCAN(eps=e_top, min_samples=ms).fit(U)\n",
    "b = [x>-1 for x in cl.labels_]\n",
    "l = ABCD1.vs['comm']\n",
    "v = [l[i] for i in range(len(l)) if b[i]]\n",
    "print('AMI without outliers:',AMI(v,cl.labels_[b]))\n",
    "print('AMI with outliers:',AMI(ABCD1.vs['comm'],cl.labels_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "U = umap.UMAP(n_components=16, random_state=1234, n_jobs=1).fit_transform(ABCD1_emb)\n",
    "cl = hdbscan.HDBSCAN()\n",
    "cl.fit(U)\n",
    "b = [x>-1 for x in cl.labels_]\n",
    "l = ABCD1.vs['comm']\n",
    "v = [l[i] for i in range(len(l)) if b[i]]\n",
    "print('AMI without outliers:',AMI(v,cl.labels_[b]))\n",
    "print('AMI with outliers:',AMI(ABCD1.vs['comm'],cl.labels_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link prediction\n",
    "\n",
    "Given a graph, link prediction aims at finding pairs of nodes not linked by an edge that are the most likely to actually have an edge between them. This could happen if we have a partial view of a graph. For example if edges \n",
    "are observed over some period of time, which new edges are we most likely to observe next?\n",
    "\n",
    "In order to simulate this situation, we take the ```ABCD1``` graph with 1,000 nodes and drop 10% of the edges.\n",
    "We re-compute the embedding (since the graph has changed), train a logistic regression model using pairs\n",
    "of nodes with and without an edge, and apply the model to a test set consisting of the dropped edges, and other \n",
    "pairs of nodes not linked by an edge.\n",
    "\n",
    "Given node embeddings $e(u)$ and $e(v)$, a representation for the node pair $(u,v)$ is obtained via some binary operator $B(e(u),e(v))$. We defined the same 4 operators as in https://arxiv.org/pdf/1607.00653.pdf at the beginning of this notebook, and will use the **Hadamar** operator in the experiments below.\n",
    "\n",
    "We build the training data by considering all edges in the reduced graph, and an equal number of node pairs without an edge. From this data, we build a logistic regression model to predict edges vs non-edges. We then apply the model to the test set which includes the dropped edges, and the same number of non-edges.\n",
    "\n",
    "First we tried with the ```ABCD1``` graph with noise parameter $\\xi=0.6$.\n",
    "Given the large number of \"noise\" edges, results are not very good, as expected.\n",
    "\n",
    "We do another set of tests, this time with the ```ABCD2``` graph with lower noise parameter $\\xi=0.2$, with better results.\n",
    "\n",
    "Since we are going to run this experiment for several graphs, we write the procedure as a function below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## default: model with Hadamard binary operator (other choices are 'l1', 'l2 and 'avg')\n",
    "## test_eid: the list of edges in the test set (dropped edges)\n",
    "## Emb: the embedding \n",
    "def link_pred_exp(G, test_eid, Emb, seed=123, op='had', return_plot=False, save_plot=False, verbose=True):\n",
    "\n",
    "    ## make a copy of the graph and drop some edges\n",
    "    Gp = G.copy()\n",
    "    Gp.delete_edges(test_eid)\n",
    "    X = Emb\n",
    "    \n",
    "    ## Build training data, first the edges\n",
    "    F = []\n",
    "    for e in Gp.es:\n",
    "        F.append(binary_operator(X[e.source],X[e.target],op=op))\n",
    "    size = len(F)\n",
    "    f = [1]*size\n",
    "\n",
    "    ## then for equal number of non-edges (we over-sample to drop edges or collisions from the list)\n",
    "    ## nb: those could include some of the dropped edges, but avoiding those would not be realistic \n",
    "    e = [tuple(np.random.choice(Gp.vcount(),size=2,replace=False)) for i in range(2*size)]\n",
    "    e = [(min(x),max(x)) for x in e if Gp.get_eid(x[0],x[1],directed=False,error=False) == -1]\n",
    "    non_edges = list(set(e))[:size]\n",
    "    for e in non_edges:\n",
    "        F.append(binary_operator(X[e[0]],X[e[1]],op=op))\n",
    "    F = np.array(F)\n",
    "    f.extend([0]*size)\n",
    "\n",
    "    ## train the model, here a logistic regression\n",
    "    logreg = LogisticRegression(random_state=seed)\n",
    "    logreg.fit(F,f)\n",
    "\n",
    "    ## prepare test set, first with all dropped edges from G \n",
    "    X_test = []\n",
    "    for i in test_eid:\n",
    "        e = G.es[i]\n",
    "        X_test.append(binary_operator(X[e.tuple[0]],X[e.tuple[1]],op=op))\n",
    "    size = len(X_test)\n",
    "    y_test = [1]*size\n",
    "\n",
    "    ## then for equal number of non-edges (we over-sample to drop edges and collisions from the list)\n",
    "    np.random.seed(seed)\n",
    "    e = [tuple(np.random.choice(G.vcount(),size=2,replace=False)) for i in range(2*size)]\n",
    "    e = [(min(x),max(x)) for x in e if G.get_eid(x[0],x[1],directed=False,error=False) == -1]\n",
    "    non_edges = list(set(e))[:size]\n",
    "    for e in non_edges:\n",
    "        X_test.append(binary_operator(X[e[0]],X[e[1]],op=op))\n",
    "    X_test = np.array(X_test)\n",
    "    y_test.extend([0]*size)\n",
    "\n",
    "    ## apply the model to test data\n",
    "    _acc = logreg.score(X_test, y_test)\n",
    "    _auc = roc_auc_score(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "    if verbose:\n",
    "        print('Accuracy of logistic regression classifier with',op,\n",
    "              'on test set: {:.2f}'.format(_acc))\n",
    "        print('AUC:',_auc)    \n",
    "\n",
    "    if return_plot:\n",
    "        logit_roc_auc = roc_auc_score(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, color='gray',label='Logistic Regression (AUC = %0.2f)' % logit_roc_auc)\n",
    "        plt.plot([0, 1], [0, 1],'k--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate', fontsize=14)\n",
    "        plt.ylabel('True Positive Rate', fontsize=14)\n",
    "        plt.title('')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        if save_plot:\n",
    "            plt.savefig('embed_link.eps')\n",
    "        plt.show();\n",
    "    \n",
    "    if verbose==False:\n",
    "        return _acc, _auc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link prediction with noisy ABCD graph $\\xi = 0.6$\n",
    "\n",
    "The results are better than random, but not great; recall that $\\xi$=0.6, so the majority of edges are noise to start with, so link prediction is very hard in this case. We try with less noisy graph next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick 10% edges at random for test set, save new graph as Gp\n",
    "np.random.seed(123)\n",
    "test_size = int(np.round(.1*ABCD1.ecount()))\n",
    "test_eid = np.random.choice(ABCD1.ecount(),size=test_size,replace=False)\n",
    "Gp = ABCD1.copy()\n",
    "Gp.delete_edges(test_eid)\n",
    "\n",
    "## select a low divergence embedding (from separate tests)\n",
    "X = LE(Gp, dim=8)\n",
    "\n",
    "## run the experiment - link prediction\n",
    "link_pred_exp(ABCD1, test_eid, X, return_plot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link prediction with less noisy ABCD graph $\\xi = 0.2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test all embeddings on the modified ```ABCD2``` graph, after removing the test set edges.\n",
    "\n",
    "We also perform the link prediction experiment for several different embeddings, and store the **accuracy** and **AUC** for each run.\n",
    "\n",
    "Again the cell below can take a few minutes - uncomment to run.\n",
    "\n",
    "We saved the results in a pkl file."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## pick 10% edges at random, save new graph ABCD2_sampled\n",
    "test_size = int(np.round(.1*ABCD2.ecount()))\n",
    "np.random.seed(123) ## for reproducibility\n",
    "test_eid = np.random.choice(ABCD2.ecount(),size=test_size,replace=False)\n",
    "ABCD2_sampled = ABCD2.copy()\n",
    "ABCD2_sampled.delete_edges(test_eid)\n",
    "\n",
    "## save edgelist and ecg communities for use by the framework\n",
    "Edges = np.array( [ [int(ABCD2_sampled.vs[e.source]['name']), \n",
    "                     int(ABCD2_sampled.vs[e.target]['name'])] for e in ABCD2_sampled.es])\n",
    "np.savetxt('_edges.dat', Edges, delimiter='\\t', fmt=\"%d\")\n",
    "ecg = [x+1 for x in ABCD2_sampled.community_ecg(ens_size=32, final='leiden').membership]\n",
    "np.savetxt('_ecg.dat', ecg, delimiter='\\t', fmt=\"%d\")\n",
    "\n",
    "## test and rank some embeddings w.r.t. the local JS divergence score\n",
    "## on ABCD_sampled, i.e.  with 10% edges removed\n",
    "D = test_embeddings(ABCD2_sampled, Dims=[2,4,8], edgefile='_edges.dat', \n",
    "                    commfile='_ecg.dat', local=True)\n",
    "\n",
    "## link prediction for each embedding in turn\n",
    "L = []\n",
    "graph = n2v.Graph(ABCD2_sampled.to_tuple_list(), directed=False, weighted=False)\n",
    "for x in D.iloc:\n",
    "    if x.algo == 'le':\n",
    "        X = LE(ABCD2_sampled, dim=x.dim)\n",
    "        L.append(link_pred_exp(ABCD2, test_eid, X, verbose=False))\n",
    "    else:\n",
    "        if x.algo == 'hope':\n",
    "            X = Hope(ABCD2_sampled, dim=x.dim, sim=x.param)\n",
    "            L.append(link_pred_exp(ABCD2, test_eid, X, verbose=False))\n",
    "        else:\n",
    "            p,q = (float(i) for i in x.param.split(' '))\n",
    "            nv = n2v.Node2Vec(graph, dim=x.dim, p=p, q=q, \n",
    "                              walk_length=80, window=5, seed=123)\n",
    "            nv.train(epochs=10, verbose=False)\n",
    "            X = np.array([nv.wv[i] for i in range(len(nv.wv))])\n",
    "            L.append(link_pred_exp(ABCD2, test_eid, X, verbose=False))\n",
    "D['acc'] = [x[0] for x in L]\n",
    "D['auc'] = [x[1] for x in L]\n",
    "\n",
    "## save data frame\n",
    "#pickle.dump( D, open( datadir+\"ABCD/abcd_1000_xi2_linkpred.pkl\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load results from pickle file\n",
    "with open(datadir+\"ABCD/abcd_1000_xi2_linkpred.pkl\",\"rb\") as f:\n",
    "    D = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## accuracy values vary from about 60% to 83% - top values shown here\n",
    "D.sort_values(by='acc', ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## kendall-tau divengence: Accuracy vs global/local divergence scores\n",
    "print('global divergence:', tau(D['jsd'],D['acc']).statistic)\n",
    "print('local divergence:', tau(D['local_jsd'],D['acc']).statistic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_balls = D.shape[0]\n",
    "\n",
    "## normalize the scores (divide by the min values)\n",
    "x = D['jsd']/min(D['jsd'])\n",
    "y = D['local_jsd']/min(D['local_jsd'])\n",
    "\n",
    "## plot results as 'balls' with area proportional to the accuracy\n",
    "acc = D['acc']\n",
    "areas = [(25*i-10)**2 for i in acc]\n",
    "plt.figure()\n",
    "plt.scatter(x, y, s=areas, alpha=0.85, color='dimgrey')\n",
    "plt.xlabel(\"Global divergence score (normalized)\", fontsize=13)\n",
    "plt.ylabel(\"Local divergence score (normalized)\", fontsize=13)\n",
    "\n",
    "## pick markersize to approximate the size ball sizes corresponding to 60% and 83% accuracy\n",
    "line1 = mpl.lines.Line2D([], [], color='white', marker='o',markersize=8, markerfacecolor=\"dimgrey\")\n",
    "line2 = mpl.lines.Line2D([], [], color='white',marker='o', markersize=11,  markerfacecolor=\"dimgrey\")\n",
    "plt.legend((line1, line2), ('60%', '83%',), numpoints=1, loc=4, title='test set accuracy')\n",
    "\n",
    "#plt.savefig('embed_link_bubbles.eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## redo experiment for best case and draw the ROC curve\n",
    "\n",
    "## pick 10% edges at random, save new graph as Gp\n",
    "test_size = int(np.round(.1*ABCD2.ecount()))\n",
    "np.random.seed(123) ## for reproducibility\n",
    "test_eid = np.random.choice(ABCD2.ecount(),size=test_size,replace=False)\n",
    "ABCD2_sampled = ABCD2.copy()\n",
    "ABCD2_sampled.delete_edges(test_eid)\n",
    "\n",
    "## low divergence embedding from tests in previous cells\n",
    "graph = n2v.Graph(ABCD2_sampled.to_tuple_list(), directed=False, weighted=False)\n",
    "nv = n2v.Node2Vec(graph, dim=8, p=1.0, q=0.5, walk_length=80, window=5, seed=123)\n",
    "nv.train(epochs=10, verbose=False)\n",
    "X = np.array([nv.wv[i] for i in range(len(nv.wv))])\n",
    "link_pred_exp(ABCD2, test_eid, X, return_plot=True, save_plot=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now try with high local divergence embedding from test in previous cell\n",
    "X = Hope(ABCD2_sampled, sim='cn', dim=2)\n",
    "link_pred_exp(ABCD2, test_eid, X, return_plot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning - using classification accuracy to compare embeddings\n",
    "\n",
    "We saw earlier an **unsupervised** method for selecting good graph embeddings where we computed some divergence score. \n",
    "\n",
    "In **supervised** case, it is usually better to take advantage of the known labels to compare embeddings.\n",
    "\n",
    "With this experiment, we do the following using the 1,000 nodes ```ABCD1``` graph. Recall that in this case, the class is the ground-truth community for each node. \n",
    "\n",
    "* we partition the nodes into training, validation and test sets in proportion 25%/25%/50%\n",
    "* we generate 40 different embeddings (3 algorithms, different parameters)\n",
    "* from each embedding, \n",
    "  * we compute the JS divergences (unsupervised score)\n",
    "  * we use the training data to build a classification model (random forest)\n",
    "  * we apply this model to the validation set \n",
    "  * we compute the accuracy score (supervised score) \n",
    "\n",
    "The code to do this is commented out in the cell below as this can take several minutes to run. \n",
    "A pickle file with the results is included in data directory and can be read directly.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "# train/val/test, split the id's in proportion 25/25/50\n",
    "np.random.seed(123)\n",
    "ids = [i for i in range(ABCD1.vcount())]\n",
    "id_trainval, id_test = train_test_split(ids, test_size=.5)     ## split test\n",
    "id_train, id_val = train_test_split(id_trainval, test_size=.5) ## split train/val\n",
    "\n",
    "y_all = ABCD1.vs['comm']\n",
    "y_train = [y_all[i] for i in id_train]\n",
    "y_trainval = [y_all[i] for i in id_trainval]\n",
    "y_val = [y_all[i] for i in id_val]\n",
    "y_test = [y_all[i] for i in id_test]\n",
    "\n",
    "## loop over several algos, parameters\n",
    "L = []\n",
    "DIM = [2,4,8,16,32]\n",
    "\n",
    "## LE\n",
    "print('running LE')\n",
    "for dim in DIM:\n",
    "    X = LE(ABCD1, dim=dim)\n",
    "    X_train = X[id_train,:]\n",
    "    X_val = X[id_val,:]\n",
    "    saveEmbedding(X,ABCD1)\n",
    "    jsd=JS(datadir+'ABCD/abcd_1000.dat',datadir+'ABCD/abcd_1000.ecg','_embed',return_local=True)\n",
    "\n",
    "    # Create the model with 100 trees\n",
    "    model = RandomForestClassifier(n_estimators=100, \n",
    "                                   bootstrap = True,\n",
    "                                   max_features = 'sqrt',\n",
    "                                   random_state=123)\n",
    "    # Fit on training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Actual class predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val,y_pred)\n",
    "    L.append([dim,'le',0,jsd[0],jsd[1],acc])    \n",
    "    \n",
    "## HOPE\n",
    "print('running HOPE')\n",
    "for dim in DIM:\n",
    "    for sim in ['katz','aa','cn','ppr']:    \n",
    "        X = Hope(ABCD1,sim=sim,dim=dim) \n",
    "        X_train = X[id_train,:]\n",
    "        X_val = X[id_val,:]\n",
    "        saveEmbedding(X,ABCD1)\n",
    "        jsd = JS(datadir+'ABCD/abcd_1000.dat',datadir+'ABCD/abcd_1000.ecg','_embed',return_local=True)\n",
    "\n",
    "        # Create the model with 100 trees\n",
    "        model = RandomForestClassifier(n_estimators=100, \n",
    "                                       bootstrap = True,\n",
    "                                       max_features = 'sqrt',\n",
    "                                       random_state=123)\n",
    "        # Fit on training data\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Actual class predictions\n",
    "        y_pred = model.predict(X_val)\n",
    "        acc = accuracy_score(y_val,y_pred)\n",
    "        L.append([dim,'hope',sim,jsd[0],jsd[1],acc])\n",
    "\n",
    "## node2vec\n",
    "print('running node2vec')\n",
    "graph = n2v.Graph(ABCD1.to_tuple_list(), directed=False, weighted=False)\n",
    "for dim in DIM:\n",
    "    for (p,q) in [(1,.5),(.5,1),(1,1)]:\n",
    "        nv = n2v.Node2Vec(graph, dim=dim, p=p, q=q, walk_length=25, window=5, seed=123)\n",
    "        nv.train(epochs=10, verbose=False)\n",
    "        X = np.array([nv.wv[i] for i in range(len(nv.wv))])\n",
    "        saveEmbedding(X, ABCD1)\n",
    "        jsd = JS(datadir+'ABCD/abcd_1000.dat',datadir+'ABCD/abcd_1000.ecg','_embed',return_local=True)\n",
    "        X_train = X[id_train,:]\n",
    "        X_val = X[id_val,:]\n",
    "        # Create the model with 100 trees\n",
    "        model = RandomForestClassifier(n_estimators=100, \n",
    "                                       bootstrap = True,\n",
    "                                       max_features = 'sqrt',\n",
    "                                       random_state=123)\n",
    "\n",
    "        # Fit on training data\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Actual class predictions\n",
    "        y_pred = model.predict(X_val)\n",
    "        acc = accuracy_score(y_val,y_pred)\n",
    "        L.append([dim,'n2v',str(p)+' '+str(q),jsd[0],jsd[1],acc])\n",
    "\n",
    "## save L and train/val/test ids\n",
    "#pickle.dump( (id_train,id_val,id_trainval,id_test,L), open( datadir+\"ABCD/abcd_1000_embeddings.pkl\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load results from the above \n",
    "with open(datadir+\"ABCD/abcd_1000_embeddings.pkl\",\"rb\") as f:\n",
    "    id_train,id_val,id_trainval,id_test,L = pickle.load(f)\n",
    "\n",
    "## labels for train/validation/test sets\n",
    "y_all = ABCD1.vs['comm']\n",
    "y_train = [y_all[i] for i in id_train]\n",
    "y_trainval = [y_all[i] for i in id_trainval] ## training+validation sets\n",
    "y_val = [y_all[i] for i in id_val]\n",
    "y_test = [y_all[i] for i in id_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we compute the rank-based  Kendall-tau correlation between the divergence score (unsupervised) and the accuracy score (supervised). We see negative correlation which is to be expected since respectively low divergence and high accuracy are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## correlation: divergence and accuracy\n",
    "R = pd.DataFrame(L,columns=['dim','algo','param','div_glb','div_loc','val_acc'])\n",
    "print('global divergence:',tau(R['div_glb'],R['val_acc']).statistic)\n",
    "print('local divergence:',tau(R['div_loc'],R['val_acc']).statistic)\n",
    "R.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next 2 cells, we show the top results on the validation set respectively for the divergence and accuracy scores. We also add two columns with the respective ranks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort by JS-divergence on validation set\n",
    "size = R.shape[0]\n",
    "R = R.sort_values(by='div_glb',axis=0,ascending=True)\n",
    "R['rank_div_glb'] = np.arange(1,size+1,1)\n",
    "R = R.sort_values(by='div_loc',axis=0,ascending=True)\n",
    "R['rank_div_loc'] = np.arange(1,size+1,1)\n",
    "R['rank_div'] = (R['rank_div_glb'] + R['rank_div_loc'])/2\n",
    "R = R.sort_values(by='rank_div',axis=0,ascending=True)\n",
    "R.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort by Accuracy on validation set\n",
    "R = R.sort_values(by='val_acc',axis=0,ascending=False)\n",
    "size = R.shape[0]\n",
    "R['rank_val_acc'] = np.arange(1,size+1,1)\n",
    "R.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show the lowest accuracy results. We see that there is quite a range of accuracy on the validation set!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Apply the models to the test set. \n",
    "\n",
    "In the previous cells, we built a table ranking the different algorithms w.r.t. accuracy and divergence using the training and validation sets. Here, we go through the same algorithms in (decreasing) order of accuracy, re-train with each model using the training and validation sets, and apply to the test set.\n",
    "\n",
    "A pickle file is provided with the results.\n",
    "\n",
    "Uncomment the cell below to re-run.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "## retrain and score in order of validation set's accuracy\n",
    "np.random.seed(123)\n",
    "top_acc = []\n",
    "graph = n2v.Graph(ABCD1.to_tuple_list(), directed=False, weighted=False)\n",
    "for i in range(size):\n",
    "    dim, algo, param, div_g, div_l, acc, rk_dg, rk_dl, rk_d, rk_a = R.iloc[i]\n",
    "    if algo=='n2v':\n",
    "        s = param.split()\n",
    "        p = float(s[0])\n",
    "        q = float(s[1])\n",
    "        nv = n2v.Node2Vec(graph, dim=dim, p=p, q=q, walk_length=25, window=5, seed=123)\n",
    "        nv.train(epochs=10, verbose=False)\n",
    "        X = np.array([nv.wv[i] for i in range(len(nv.wv))])\n",
    "    if algo=='hope':\n",
    "        X = Hope(ABCD1, sim=param, dim=dim)\n",
    "    if algo=='le':\n",
    "        X = LE(ABCD1, dim=dim)\n",
    "        \n",
    "    X_trainval = X[id_trainval,:]\n",
    "    X_test = X[id_test,:]\n",
    "    # Create the model with 100 trees\n",
    "    model = RandomForestClassifier(n_estimators=100, \n",
    "                                   bootstrap = True,\n",
    "                                   max_features = 'sqrt',\n",
    "                                   random_state=123)\n",
    "    # Fit on training data\n",
    "    model.fit(X_trainval, y_trainval)\n",
    "\n",
    "    # Actual class predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test,y_pred)\n",
    "    top_acc.append(acc)\n",
    "\n",
    "#pickle.dump( top_acc, open( datadir+\"ABCD/abcd_1000_embeddings_test.pkl\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load test results\n",
    "with open(datadir+\"ABCD/abcd_1000_embeddings_test.pkl\",\"rb\") as fn:\n",
    "    top_acc = pickle.load(fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add test results\n",
    "R['test_acc'] = top_acc\n",
    "print('mean accuracy over all models on the test set:',np.mean(R['test_acc']))\n",
    "\n",
    "## top results w.r.t. accuracy on the test set\n",
    "R = R.sort_values(by='test_acc',axis=0,ascending=False)\n",
    "R['rank_test_acc'] = np.arange(1,size+1,1)\n",
    "R.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we take the top-10 algorithms w.r.t. divergence on the validation set, and the top-10 algorithms w.r.t. accuracy on the validation set. \n",
    "\n",
    "We then plot the distribution of results (accuracy) over the test set via box-plots.\n",
    "\n",
    "As expected, using accuracy (supervised score) yields better results, but the results obtained with the (unsupervised) global divergence score are also quite good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## top results on test set w.r.t. divergence on validation set\n",
    "R = R.sort_values(by='rank_div',axis=0,ascending=True)\n",
    "top_div = R['test_acc'].iloc[:10]\n",
    "\n",
    "## top results on test set w.r.t. accuracy on validation set\n",
    "R = R.sort_values(by='val_acc',axis=0,ascending=False)\n",
    "top_acc = R['test_acc'].iloc[:10]\n",
    "\n",
    "## pd with mu\n",
    "B = pd.DataFrame(np.transpose(np.array([top_acc,top_div])), \n",
    "                 columns=['Top-10 validation set accuracy','Top-10 divergence score'])\n",
    "B.boxplot(rot=0,figsize=(7,5), widths=.33)\n",
    "plt.ylim((0,1))\n",
    "plt.ylabel('Test set accuracy',fontsize=14)\n",
    "#plt.savefig('embed_classify.eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to compare the results is to plot the accuracy results on the test set as a function of the rank of the algorithms w.r.t. the accuracy score on the validation set (next cell) or the divergence score on the validation set (second next cell).\n",
    "\n",
    "The correlation is very clear in the first case, and is still quite strong in the second case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(R['rank_val_acc'],R['test_acc'],'.',color='black')\n",
    "plt.xlabel('Rank (vadidation set accuracy)',fontsize=14)\n",
    "plt.ylabel('Test set accuracy',fontsize=14);\n",
    "#plt.savefig('rank_accuracy.eps');\n",
    "print('correlation:',np.corrcoef(R['rank_val_acc'],R['test_acc'])[0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(R['rank_div'],R['test_acc'],'.',color='black')\n",
    "plt.xlabel('Rank (divergence score)',fontsize=14)\n",
    "plt.ylabel('Test set accuracy',fontsize=14)\n",
    "#plt.savefig('rank_divergence.eps')\n",
    "plt.show()\n",
    "\n",
    "print('correlation:',np.corrcoef(R['rank_div'],R['test_acc'])[0,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compare with accuracy obtained with a random classifier, averaging over several runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random classification\n",
    "np.random.seed(123)\n",
    "ctr = Counter(y_trainval)\n",
    "x = [ctr[i+1] for i in range(12)]\n",
    "s = np.sum(x)\n",
    "p = [i/s for i in x]\n",
    "acc = []\n",
    "for rep in range(30):\n",
    "    y_pred = [x+1 for x in np.random.choice(12,size=len(y_test),replace=True,p=p)]\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    acc.append(sum(cm.diagonal())/sum(sum(cm)))\n",
    "print('\\nRandom classifier average accuracy on test set:',np.mean(acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set accuracy vs both divergence scores\n",
    "\n",
    "Recall that we took the average rank of the global and local and divergence scores to obtain the (unsupervised) ranking of the embeddings. \n",
    "\n",
    "Below, we plot the test set accuracy vs both divergence scores.\n",
    "As we already saw with the correlation valuers, the global score is a better predictor here, but the local score is still correlated as expected, with a few points having high local divergence, low global divergence and high accuracy (so using only the local score would ranked those as bad).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_balls = R.shape[0]\n",
    "x = R['div_glb']/min(R['div_glb'])\n",
    "y = R['div_loc']/min(R['div_loc'])\n",
    "acc = R['test_acc']\n",
    "areas = [(10*x)**2 for x in acc]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y, s=areas, alpha=0.85, color='dimgrey')\n",
    "#plt.axis([0.0, 1.0, 0.0, 1.0])\n",
    "plt.xlabel(\"Global divergence score (normalized)\", fontsize=13)\n",
    "plt.ylabel(\"Local divergence score (normalized)\", fontsize=13)\n",
    "line1 = mpl.lines.Line2D([], [], color='white', marker='o',markersize=5.5, markerfacecolor=\"dimgrey\")\n",
    "line2 = mpl.lines.Line2D([], [], color='white',marker='o', markersize=8.5, markerfacecolor=\"dimgrey\")\n",
    "line3 = mpl.lines.Line2D([], [], color='white',marker='o', markersize=11.5,  markerfacecolor=\"dimgrey\")\n",
    "plt.legend((line1, line2, line3), ('35%', '65%', '95%',), numpoints=1, loc=4, title='test set accuracy')\n",
    "#plt.savefig('embed_classify_bubbles.eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN structural embedding of Zachary graph\n",
    "\n",
    "In the cells below, we embed the nodes from the Zachary graph using a simple GCN model (graph convolution net) with one hidden layer and 3-dimensional output. \n",
    "\n",
    "We use the implementation from the ```sknetwork``` package. \n",
    "\n",
    "For **structural** node features, we use each node's degree and number of edges in its egonet. We cluster the resulting embedding with k-means setting k=3. \n",
    "\n",
    "We plot the embedding (after dimension reduction via UMAP) with colors representing the k-means clusters.\n",
    "We see good separation between the 3 clusters.\n",
    "\n",
    "In the next cell we plot the graph this time using the Fruchterman-Reingold layout we saw before.\n",
    "We see that this embedding finds the central/intermediate/peripheral nodes as its clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GCN embedding of karate graph + kmeans with 3 communities\n",
    "zac_A = zac.get_adjacency_sparse()\n",
    "\n",
    "## GCN\n",
    "hidden_dim = 5\n",
    "n_labels = 3\n",
    "gnn = skn.gnn.GNNClassifier(dims=[hidden_dim, n_labels],\n",
    "                    layer_types='Conv',\n",
    "                    activations='ReLu',\n",
    "                    verbose=False)\n",
    "\n",
    "## for structural features, use degree and number of edges in egonet\n",
    "features = np.stack( (\n",
    "    np.array(zac.degree()),\n",
    "    np.array([zac.subgraph(V).ecount() for V in zac.neighborhood()]),\n",
    "    ), axis=1 \n",
    ")\n",
    "labels = np.zeros(zac.vcount()) ## embedding, no need for node labels\n",
    "\n",
    "## compute the embedding\n",
    "zac_emb = gnn.fit_transform(zac_A, features, labels=labels, n_epochs=25, random_state=42)\n",
    "\n",
    "## apply k-means to this embedding and color the nodes \n",
    "kmeans = KMeans(n_clusters=3, random_state=123, n_init=\"auto\").fit(zac_emb)\n",
    "zac.vs['kmeans'] = kmeans.labels_\n",
    "greys = mpl.colormaps['Greys']\n",
    "cls = [greys(0),greys(.45),greys(.25)]\n",
    "zac.vs['color'] = [cls[i] for i in zac.vs['kmeans']]\n",
    "zac.vs['size'] = 14\n",
    "zac.vs['label_size'] = 8\n",
    "\n",
    "## map the structural embedding to 2-d via UMAP for visualization\n",
    "Y = umap.UMAP(random_state=123, n_jobs=1).fit_transform(zac_emb)\n",
    "ig.plot(zac, layout = Y, bbox=(0,0,400,300), vertex_label=zac.vs['name'])\n",
    "#ig.plot(zac, 'zac_gcn_1.eps', layout = Y, bbox=(0,0,400,300), vertex_label=zac.vs['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now with Fruchterman-Reingold layout\n",
    "ig.plot(zac, bbox=(0,0,400,300), vertex_label=zac.vs['name'])\n",
    "#ig.plot(zac, 'zac_gcn_2.eps', layout = zac.vs['layout'], bbox=(0,0,400,300), vertex_label=zac.vs['name'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we train another GCN, but with 1-dimension output layer (using same node features), so we get a 1dimensional embedding that we can use to order the nodes, which we show in the second cell below (where the node labels are replaced by their respective ranks in the ordering). \n",
    "\n",
    "We see a clear ranking from the most central nodes to the peripherial nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1-d embedding allows for node ordering\n",
    "n_labels = 1\n",
    "gnn = skn.gnn.GNNClassifier(dims=[hidden_dim, n_labels],\n",
    "                    layer_types='Conv',\n",
    "                    activations='ReLu',\n",
    "                    verbose=False)\n",
    "emb_1 = gnn.fit_transform(zac_A, features, labels=labels, n_epochs=20, random_state=42)\n",
    "roles = np.argsort(emb_1.flatten())\n",
    "for i in range(len(roles)):\n",
    "    zac.vs[roles[i]]['label'] = i+1\n",
    "ig.plot(zac, bbox=(400,300), edge_color='lightgrey', \n",
    "        vertex_size=.1, vertex_label_color='black', vertex_label_size=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## semi-supervised learning with GCN\n",
    "\n",
    "Below we consider the ground-truth labels in the Zachary graphs; recall that there are two communities. \n",
    "\n",
    "We mask 1/3 of the labels and train a GCN model using the other 2/3, given the ground-truth labels for those. \n",
    "\n",
    "We then use the trained model to predict the labels for the masked 1/3 of the nodes (a.k.a. the test set).\n",
    "\n",
    "We see that we get a good accuracy, close to 90%, on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## classification - karate graph\n",
    "labels = np.array(zac.vs['comm'], dtype='int')\n",
    "\n",
    "np.random.seed(123)\n",
    "train_mask = np.random.random(size=len(labels)) < 0.33 ## mask 1/3 of the nodes for training\n",
    "labels[train_mask] = -1 ## the negative labels are ignored in the training\n",
    "\n",
    "# GNN classifier with a single hidden layer\n",
    "hidden_dim = 5\n",
    "n_labels = 2 ## 2 ground-truth communities \n",
    "gnn = skn.gnn.GNNClassifier(dims=[hidden_dim, n_labels],\n",
    "                    layer_types='Conv',\n",
    "                    activations='ReLu',\n",
    "                    verbose=False)\n",
    "\n",
    "## for features, we simply use the adjacency matrix\n",
    "features = zac_A\n",
    "\n",
    "## fit the GCN\n",
    "Pred = gnn.fit_predict(zac_A, features, labels=labels, n_epochs=50, random_state=42)\n",
    "\n",
    "## apply to test set and compute accuracy\n",
    "acc = get_accuracy_score(np.array(zac.vs['comm'])[train_mask], Pred[train_mask])\n",
    "print('accuracy on the test set:', acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding the Twitch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "from sklearn.manifold import TSNE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### running node2vec and UMAP/t-SNE\n",
    "\n",
    "Here are the steps to follow to re-create the experiment below:\n",
    "* get the dataset from https://snap.stanford.edu/data/twitch_gamers.html (168,114 nodes, 6,797,557 edges)\n",
    "* build a dataframe with all node features\n",
    "* build the graph from the edge list and run node2vec on the graph\n",
    "* run UMAP and t-SNE to get 2-dim mappings for visualization, \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "## get the node features\n",
    "twitch = pd.read_csv('~/Twitch_Embedding/large_twitch_features.csv', header=0)\n",
    "\n",
    "## get the edgelist\n",
    "df = pd.read_csv('~/Twitch_Embedding/large_twitch_edges.csv', header=0)\n",
    "edges = list(zip(df.numeric_id_1, df.numeric_id_2))\n",
    "\n",
    "## build the graph and compute node2vec embedding\n",
    "graph = n2v.Graph(edges, directed=False, weighted=False)\n",
    "nv = n2v.Node2Vec(graph, dim=128, p=1.0, q=1.0, walk_length=80, window=5, seed=123, workers=32)\n",
    "nv.train(epochs=10, verbose=True)\n",
    "Embedding = np.array([nv.wv[i] for i in range(len(nv.wv))])\n",
    "Embedding.shape\n",
    "\n",
    "## compute UMAP projection in 2-dim for visualization\n",
    "Embedding_umap = umap.UMAP(n_jobs=32).fit_transform(Embedding)\n",
    "\n",
    "## add 2-dim node coordinates to the node features dataframe\n",
    "twitch['X'] = Embedding_umap[:,0]\n",
    "twitch['Y'] = Embedding_umap[:,1]\n",
    "\n",
    "## same with t-SNE\n",
    "Embedding_tsne = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3, n_jobs=32).fit_transform(Embedding)\n",
    "twitch['X_tsne'] = Embedding_tsne[:,0]\n",
    "twitch['Y_tsne'] = Embedding_tsne[:,1]\n",
    "\n",
    "## save the dataframe and the node2vec embedding\n",
    "#pickle.dump(twitch, open(datadir+\"Twitch/twitch.pkl\", \"wb\" ) )\n",
    "#pickle.dump(Embedding, open(datadir+\"Twitch/twitch_embedding.pkl\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the data obtained from the above experiment\n",
    "with open(datadir+\"Twitch/twitch.pkl\", \"rb\") as fn:\n",
    "    twitch = pickle.load(fn)\n",
    "twitch.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### greyscale plot - highlight a few languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a greyscale palette without the extremes\n",
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "cmap = plt.get_cmap('binary')\n",
    "greyscale = truncate_colormap(cmap, 0.1,1.0)\n",
    "\n",
    "## select a few languages to highlight\n",
    "languages = ['French','Spanish','German']\n",
    "language_codes = ['FR','ES','DE']\n",
    "twitch['color'] = 0\n",
    "twitch.loc[twitch.language==language_codes[0], 'color'] = 1\n",
    "twitch.loc[twitch.language==language_codes[1], 'color'] = 2\n",
    "twitch.loc[twitch.language==language_codes[2], 'color'] = 3\n",
    "\n",
    "## plot - change ticklabels as required\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(twitch.X, twitch.Y, c=twitch.color, cmap=greyscale, s=5)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "ax = plt.colorbar(boundaries=np.arange(5)-0.5, shrink=0.7)\n",
    "ax.set_ticks(np.arange(4)+.2)\n",
    "ax.set_ticklabels(['Other']+languages, rotation=90, fontsize=12)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Twitch dataset embedding', fontsize=20)\n",
    "#plt.savefig('twitch_1.eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute the average position for every language in mapped 2-dim space\n",
    "L = twitch.groupby(by='language').mean( ['X','Y'])\n",
    "X = list(L.X)\n",
    "Y = list(L.Y)\n",
    "Z = list(L.index)\n",
    "fig, ax = plt.subplots(figsize=(9,9))\n",
    "ax.scatter(X, Y, s=0)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlim((min(X)-2,max(X)+2))\n",
    "plt.ylim((min(Y)-2,max(Y)+2))\n",
    "for i, txt in enumerate(Z):\n",
    "    if txt != 'OTHER':\n",
    "        ax.annotate(txt, (X[i], Y[i]), color='black', size=14)\n",
    "#plt.savefig('twitch_2.eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## colorplot - all languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.sort([x for x in set(twitch.language) if x!='OTHER'])\n",
    "lang_list = np.concatenate( ( np.array(['OTHER']), np.array([x[i] for i in np.arange(len(x)-1,-1,-1)])) )\n",
    "num_lang = len(lang_list)\n",
    "lang_dict = dict(zip(lang_list,np.arange(num_lang)))\n",
    "\n",
    "## plot - change ticklabels as required\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(twitch.X, twitch.Y, c=[lang_dict[i] for i in twitch.language], cmap='Spectral', s=5)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "ax = plt.colorbar(boundaries=np.arange(num_lang+1)-0.5, shrink=0.9)\n",
    "ax.set_ticks(np.arange(num_lang)+.2)\n",
    "ax.set_ticklabels(lang_dict.keys(), fontsize=12)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Twitch dataset embedding', fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filtering - users with high number of view only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitch['log_num_views'] = [int(x) for x in np.round(np.log10(twitch.views+1))]\n",
    "\n",
    "## plot \n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(twitch[twitch.log_num_views>5].X, twitch[twitch.log_num_views>5].Y, c=[lang_dict[i] for i in twitch[twitch.log_num_views>5].language], cmap='Spectral', s=5)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "ax = plt.colorbar(boundaries=np.arange(num_lang+1)-0.5, shrink=0.9)\n",
    "ax.set_ticks(np.arange(num_lang)+.2)\n",
    "ax.set_ticklabels(lang_dict.keys(), fontsize=12)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Twitch dataset embedding - subset', fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE views of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot - change ticklabels as required\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(twitch.X_tsne, twitch.Y_tsne, c=twitch.color, cmap=greyscale, s=5)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "ax = plt.colorbar(boundaries=np.arange(5)-0.5, shrink=0.7)\n",
    "ax.set_ticks(np.arange(4)+.2)\n",
    "ax.set_ticklabels(['Other']+languages, rotation=90, fontsize=12)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Twitch dataset embedding', fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot - change ticklabels as required\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(twitch.X_tsne, twitch.Y_tsne, c=[lang_dict[i] for i in twitch.language], cmap='Spectral', s=5)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "ax = plt.colorbar(boundaries=np.arange(num_lang+1)-0.5, shrink=0.9)\n",
    "ax.set_ticks(np.arange(num_lang)+.2)\n",
    "ax.set_ticklabels(lang_dict.keys(), fontsize=12)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Twitch dataset embedding', fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot \n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(twitch[twitch.log_num_views>5].X_tsne, twitch[twitch.log_num_views>5].Y_tsne, c=[lang_dict[i] for i in twitch[twitch.log_num_views>5].language], cmap='Spectral', s=5)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "ax = plt.colorbar(boundaries=np.arange(num_lang+1)-0.5, shrink=0.9)\n",
    "ax.set_ticks(np.arange(num_lang)+.2)\n",
    "ax.set_ticklabels(lang_dict.keys(), fontsize=12)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Twitch dataset embedding - subset', fontsize=20)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complexnetworks",
   "language": "python",
   "name": "complexnetworks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
