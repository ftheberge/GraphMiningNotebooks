{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 - Community Detection\n",
    "\n",
    "In this notebook, we explore several algorithms to find communities in graphs.\n",
    "\n",
    "In some cells, we use the ABCD benchmark to generate synthetic graphs with communities. \n",
    "\n",
    "We use the Python implementation of ABCD: ```pip install abcd-graph```\n",
    "\n",
    "A faster version of ABCD is available in Julia: https://github.com/bkamins/ABCDGraphGenerator.jl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set accordingly\n",
    "datadir = '../Datasets/'\n",
    "\n",
    "## uncomment to get wide display\n",
    "# from IPython.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import Counter\n",
    "import os\n",
    "import umap\n",
    "import pickle\n",
    "import partition_igraph\n",
    "import subprocess\n",
    "from sklearn.metrics import adjusted_mutual_info_score as AMI\n",
    "import random \n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from abcd_graph import ABCDGraph, ABCDParams\n",
    "\n",
    "## we use those for the book, but you can change to other colors\n",
    "cls_edges = 'gainsboro'\n",
    "cls = ['white','gray','dimgray','black']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zachary (karate) graph\n",
    "\n",
    "This is a small graph with 34 nodes and two ground-truth communities.\n",
    "Modularity-based algorithms will typically find 4 or 5 communities.\n",
    "In the next cells, we look at this small graph from several different angles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "random.seed(12345)\n",
    "g_zac = ig.Graph.Famous('zachary')\n",
    "g_zac.vs['size'] = 14\n",
    "g_zac.vs['label'] = g_zac.vs['name'] = [str(i) for i in range(g_zac.vcount())]\n",
    "g_zac.vs['label_size'] = 8\n",
    "g_zac.es['color'] = cls_edges\n",
    "g_zac.vs['comm'] = [0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "g_zac.vs['color'] = [cls[i] for i in g_zac.vs['comm']]\n",
    "ly_zac = g_zac.layout_fruchterman_reingold() \n",
    "g_zac['layout'] = ly_zac\n",
    "#ig.plot(g_zac, 'zachary_gt.eps', bbox=(0,0,400,300))\n",
    "ig.plot(g_zac, bbox=(0,0,400,300))\n",
    "\n",
    "## save layout for other notebooks\n",
    "# pickle.dump( ly_zac, open( datadir+\"Zachary/layout.pkl\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Roles\n",
    " \n",
    "We compute $z(v)$ (normalized within module degree) and $p(v)$ (participation coefficients) as defined in section 5.2 of the book for the Zachary graph `g_zac`. \n",
    "\n",
    "We identify 3 types of nodes, as described in the book.\n",
    "\n",
    "* provincial hubs\n",
    "* peripheral nodes (non-hubs)\n",
    "* ultra peripheral nodes (non-hubs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalized within-module degree (z(v))\n",
    "def nwmd(G, A):\n",
    "    # within module degrees\n",
    "    deg_in = [sum([A[v] == A[i] for i in G.neighbors(v)]) for v in range(G.vcount())]\n",
    "    deg_in_mean = [ np.mean([deg_in[i] for i in range(G.vcount()) if A[i] == j]) for j in set(A) ]\n",
    "    deg_in_std = [ np.std([deg_in[i] for i in range(G.vcount()) if A[i] == j], ddof=1) for j in set(A) ]\n",
    "    return [ (deg_in[v]-deg_in_mean[A[v]]) / deg_in_std[A[v]] for v in range(G.vcount()) ]\n",
    "\n",
    "## participation coefficient\n",
    "def pc(G, A):\n",
    "    deg = G.degree()\n",
    "    coef = []\n",
    "    for v in range(G.vcount()):\n",
    "        coef.append(1-np.sum([(x/deg[v])**2 for x in Counter([A[i] for i in G.neighbors(v)]).values()]))\n",
    "    return coef\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute z (normalized within-module degree) and p (participation coefficient)\n",
    "g_zac.vs['z'] = nwmd(g_zac, g_zac.vs['comm'])\n",
    "g_zac.vs['p'] = pc(g_zac, g_zac.vs['comm']) \n",
    "\n",
    "## store the results and sort\n",
    "D = pd.DataFrame(np.array([g_zac.vs['z'],g_zac.vs['p']]).transpose(),columns=['z','p']).sort_values(by='z',ascending=False)\n",
    "D.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at $z(v)$ and $p(v)$\n",
    "\n",
    "Below, we plot the Zachary graph with respect to $z(v)$ where $z(v) > 2.5$ are **hubs**, which we show as **white square** nodes.\n",
    "\n",
    "The largest values are for node 0 (instructor), node 33 (president) and node 32.\n",
    "Nodes 0 and 33 are the key nodes for the division of the group into factions.\n",
    "\n",
    "The **ultra-peripherial** nodes are shown with darker color.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zachary graph w.r.t. roles\n",
    "g_zac.vs['color'] = cls[3]\n",
    "g_zac.vs['shape'] = 'circle'\n",
    "for v in g_zac.vs:\n",
    "    if v['z'] < 2.5: ## non-hub\n",
    "        if v['p'] < .62 and v['p'] >= .05: ## peripheral\n",
    "            v['color'] = cls[0]\n",
    "        if v['p'] < .05: ## ultra-peripheral\n",
    "            v['color'] = cls[1]\n",
    "    if v['z'] >= 2.5 and v['p'] < .3: ## hubs (all provincial here)            \n",
    "        v['color'] = cls[0]\n",
    "        v['shape'] = 'square'\n",
    "#ig.plot(g_zac, 'zachary_roles_1.eps', layout=ly_zac, bbox=(0,0,400,300))\n",
    "ig.plot(g_zac, bbox=(0,0,400,300))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 5.3(b)\n",
    "\n",
    "The code below is to generate Figure 5.3(b) in the book, again comparing node roles in the Zachary graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Figure 5.3(b) -- comparing the roles\n",
    "fig, ax = plt.subplots(figsize=(12,9))\n",
    "ax.scatter(g_zac.vs['p'],g_zac.vs['z'],marker='o',s=75, color='k')\n",
    "\n",
    "plt.plot([0, .5], [2.5, 2.5], color='k', linestyle='-', linewidth=2)\n",
    "plt.plot([.05, .05], [-.5, 2.4], color='k', linestyle='-', linewidth=2)\n",
    "\n",
    "ax.annotate('node 0', (g_zac.vs['p'][0],g_zac.vs['z'][0]-.05), xytext=(g_zac.vs['p'][0]+.01,g_zac.vs['z'][0]-.3), \n",
    "            fontsize=14,\n",
    "            arrowprops = dict(  arrowstyle=\"-\",connectionstyle=\"angle3,angleA=0,angleB=-90\"))\n",
    "\n",
    "ax.annotate('node 33', (g_zac.vs['p'][33],g_zac.vs['z'][33]-.05), xytext=(g_zac.vs['p'][33]-.07,g_zac.vs['z'][33]-.3), \n",
    "            fontsize=14,\n",
    "            arrowprops = dict(  arrowstyle=\"-\",connectionstyle=\"angle3,angleA=0,angleB=-90\"))\n",
    "\n",
    "ax.annotate('node 32', (g_zac.vs['p'][32]-.005,g_zac.vs['z'][32]), xytext=(g_zac.vs['p'][32]-.07,g_zac.vs['z'][32]), \n",
    "            fontsize=14,\n",
    "            arrowprops = dict(  arrowstyle=\"-\",connectionstyle=\"angle3,angleA=0,angleB=-90\"))\n",
    "\n",
    "ax.annotate('node 1', (g_zac.vs['p'][1],g_zac.vs['z'][1]-.05), xytext=(g_zac.vs['p'][1]-.07,g_zac.vs['z'][1]-.3), \n",
    "            fontsize=14,\n",
    "            arrowprops = dict(  arrowstyle=\"-\",connectionstyle=\"angle3,angleA=0,angleB=-90\"))\n",
    "\n",
    "ax.annotate('node 3', (g_zac.vs['p'][3],g_zac.vs['z'][3]-.05), xytext=(g_zac.vs['p'][3]+.07,g_zac.vs['z'][3]-.3), \n",
    "            fontsize=14,\n",
    "            arrowprops = dict(  arrowstyle=\"-\",connectionstyle=\"angle3,angleA=0,angleB=-90\"))\n",
    "\n",
    "ax.annotate('node 2', (g_zac.vs['p'][2],g_zac.vs['z'][2]-.05), xytext=(g_zac.vs['p'][2]-.07,g_zac.vs['z'][2]-.3), \n",
    "            fontsize=14,\n",
    "            arrowprops = dict(  arrowstyle=\"-\",connectionstyle=\"angle3,angleA=0,angleB=-90\"))\n",
    "\n",
    "ax.annotate('provincial hubs',(.3,3), fontsize=18)\n",
    "ax.annotate('peripheral non-hubs',(.3,1.8), fontsize=18)\n",
    "ax.annotate('ultra peripheral non-hubs',(0.025,0.0),xytext=(.1,0), fontsize=18,\n",
    "             arrowprops = dict( arrowstyle=\"->\", connectionstyle=\"angle3,angleA=0,angleB=-90\"))\n",
    "\n",
    "plt.xlabel(r'participation coefficient ($p(v)$)',fontsize=16)\n",
    "plt.ylabel(r'normalized within module degree ($z(v)$)',fontsize=16);\n",
    "#plt.savefig('zachary_roles_2.eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at a few other community-based features\n",
    "\n",
    "We already saw the *normalized within-module degree* $z(v)$ and *participation coefficient* $p(v)$.\n",
    "\n",
    "Recall that a high value for $z(v)$ is indicative of a hub. \n",
    "\n",
    "For $p(v)$, a value close to zero indicates homogeneity of communities amongst $v$'s neighbours, while a high value indicates heterogeneity.\n",
    "\n",
    "Below we compute the *community distribution distance* (cdd) and the *community association strength* (cas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## community distribution distance\n",
    "def cdd(G, A):\n",
    "    deg = G.degree()\n",
    "    Vol = sum(deg)\n",
    "    Vol_A = np.zeros(max(A)+1, dtype='int')\n",
    "    for i in range(G.vcount()):\n",
    "        Vol_A[A[i]] += deg[i]\n",
    "    Vol_A = Vol_A/Vol\n",
    "    cdd = []\n",
    "    for i in range(G.vcount()):\n",
    "        deg_A = np.zeros(max(A)+1, dtype='int')\n",
    "        for v in G.neighbors(i):\n",
    "            deg_A[A[v]]+=1\n",
    "        cdd.append(np.sqrt( sum( (deg_A/deg[i]-Vol_A)**2 )))\n",
    "    return cdd\n",
    "\n",
    "## community association strength\n",
    "def cas(G,A):\n",
    "    deg = np.array(G.degree())\n",
    "    deg_int = np.array([sum([A[i] == A[j] for i in G.neighbors(j)]) for j in range(G.vcount())])\n",
    "    Vol = sum(deg)\n",
    "    Vol_A = np.zeros(max(A)+1, dtype='int')\n",
    "    for i in range(G.vcount()):\n",
    "        Vol_A[A[i]] += deg[i]\n",
    "    return deg_int/deg - (np.array([Vol_A[A[i]] for i in range(G.vcount())]) - deg)/Vol\n",
    "\n",
    "g_zac.vs['cdd'] = cdd(g_zac, g_zac.vs['comm'])\n",
    "g_zac.vs['cas'] = cas(g_zac, g_zac.vs['comm'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show the nodes with low *cas* values with white color. We see that those correspond to nodes that are at the boundary between communities.\n",
    "\n",
    "We also compute the Pearson correlation coefficient between the community-based measures we computed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## value with lowest 'cas' are shown in white\n",
    "th = np.quantile(g_zac.vs['cas'], .15)\n",
    "g_zac.vs['color'] = [cls[int(i)] for i in np.array(g_zac.vs['cas'])>th]\n",
    "ig.plot(g_zac, bbox=(0,0,400,300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## correlation between various community-based measures\n",
    "np.corrcoef([g_zac.vs['z'], g_zac.vs['p'], g_zac.vs['cdd'], g_zac.vs['cas']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strong and weak communities\n",
    "\n",
    "Communities can be defined as strong or weak as per (5.1) and (5.2) in the book.\n",
    "\n",
    "For the Zachary graph, we verify if nodes within communities satisfy the strong criterion, then we verify if the two communities satisfy the weak definition.\n",
    "\n",
    "For the strong definition (internal degree larger than external degree for each node), only two nodes do not qualify: nodes 2 and 9.\n",
    "\n",
    "For the weak definition (total community internal degree > total community external degree), both communities satisfy this criterion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## strong criterion\n",
    "for i in range(g_zac.vcount()):\n",
    "    c = g_zac.vs[i]['comm']\n",
    "    n = [g_zac.vs[v]['comm']==c for v in g_zac.neighbors(i)]\n",
    "    if sum(n)<=len(n)-sum(n):\n",
    "        print('node',i,'has internal degree',sum(n),'external degree',len(n)-sum(n))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## weak criterion\n",
    "I = [0,0]\n",
    "E = [0,0]\n",
    "for i in range(g_zac.vcount()):\n",
    "    c = g_zac.vs[i]['comm']\n",
    "    n = [g_zac.vs[v]['comm']==c for v in g_zac.neighbors(i)]\n",
    "    I[c] += sum(n)\n",
    "    E[c] += len(n)-sum(n)\n",
    "print('community 0 internal degree',I[0],'external degree',E[0])\n",
    "print('community 1 internal degree',I[1],'external degree',E[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering and dendrogram\n",
    "\n",
    "Girvan-Newman algorithm is described in section 5.5 of the book. We apply it to the Zachary graph and show the results of this divisive algorithm as a dendrogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Girvan-Newman algorithm\n",
    "gn = g_zac.community_edge_betweenness()\n",
    "#ig.plot(gn,'zachary_dendrogram.eps',bbox=(0,0,400,400))\n",
    "ig.plot(gn,bbox=(0,0,400,400))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a hierarchical clustering. In the next plot, we compute modularity for each possible cut of the dendrogram.\n",
    "\n",
    "We see that we get strong modularity with 2 clusters, but maximal value is obtained with 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute modularity at each possible cut and plot\n",
    "q = []\n",
    "for i in np.arange(g_zac.vcount()):\n",
    "    q.append(g_zac.modularity(gn.as_clustering(n=i+1)))\n",
    "plt.plot(np.arange(1,1+g_zac.vcount()), q, 'o-', color=cls[2])\n",
    "plt.xlabel('number of clusters',fontsize=14)\n",
    "plt.ylabel('modularity',fontsize=14);\n",
    "#plt.savefig('zachary_modularity.eps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the nodes partitioned if we pick only 2 communities? How does this compare to the underlying ground truth?\n",
    "\n",
    "From the plot below, we see that only 1 node is misclassified (node 2 was \"white\" in the plot with ground-truth communities)\n",
    "\n",
    "The modularity of this partition, $q = 0.35996$. \n",
    "\n",
    "We also compare the partition with ground truth via AMI (adjusted mutual information), as defined in section 5.3 of the book; we got a high value AMI = 0.83276 showing  strong concordance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show result with 2 clusters -- \n",
    "g_zac.vs['gn'] = gn.as_clustering(n=2).membership\n",
    "print('AMI:',AMI(g_zac.vs['comm'],g_zac.vs['gn']))  ## adjusted mutual information\n",
    "print('q:',g_zac.modularity(g_zac.vs['gn']))        ## modularity\n",
    "g_zac.vs['label_size'] = 8\n",
    "g_zac.vs['color'] = [cls[i] for i in g_zac.vs['gn']]\n",
    "#ig.plot(g_zac, 'zachary_2.eps', layout=ly_zac, bbox=(0,0,400,300))\n",
    "ig.plot(g_zac, bbox=(0,0,400,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show the same plot as above, but we label the nodes with respect to the 5 communities found by modularity-based algorithms. \n",
    "\n",
    "We color the nodes with respect to the two ground-truth communities.\n",
    "\n",
    "We indeed see that in that case, we get higher modularity, but weaker AMI value.\n",
    "\n",
    "Other than breaking up each community in two, we see that node 2 is mis-labelled (as already noticed), and node 9 is isolated in its own community (community #4). \n",
    "\n",
    "Recall that those were the only two nodes **not** having **strong connectivity** with respect to the ground truth communities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## show result with optimal modularity (5 clusters)\n",
    "g_zac.vs['label'] = gn.as_clustering(n=5).membership\n",
    "print('AMI:',AMI(g_zac.vs['comm'],g_zac.vs['label']))\n",
    "print('q:',g_zac.modularity(g_zac.vs['label']))\n",
    "g_zac.vs['color'] = [cls[i] for i in g_zac.vs['comm']]\n",
    "#ig.plot(g_zac, 'zachary_5.eps', layout=ly_zac, bbox=(0,0,400,300))\n",
    "ig.plot(g_zac, bbox=(0,0,400,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABCD graph with 100 nodes\n",
    "\n",
    "Next we look at a slightly larger graph generated with the ABCD benchmark model, which is described in section 5.3 of the book. This graph has 3 communities. \n",
    "Using hierarchical clustering, we compare modularity and AMI for each possible cut.\n",
    "\n",
    "The ABCD parameters used to generate this graph are: \n",
    "* $\\gamma=3$\n",
    "* degree range [5,15]\n",
    "* $\\tau=2$\n",
    "* community size range [25,50]\n",
    "* $\\xi=.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Python-ABCD\n",
    "n = 100\n",
    "xi = 0.2\n",
    "\n",
    "## degrees\n",
    "gamma = 3.0\n",
    "delta = 5\n",
    "Delta = 15\n",
    "\n",
    "## communities\n",
    "beta = 2.0\n",
    "s = 25\n",
    "S = 50\n",
    "\n",
    "## build\n",
    "random.seed(321)\n",
    "np.random.seed(321)\n",
    "\n",
    "params = ABCDParams(vcount=n, gamma=gamma, min_degree=delta, max_degree=Delta, beta=beta, min_community_size=s, max_community_size=S, xi=xi)\n",
    "g = ABCDGraph(params).build()\n",
    "g_abcd = g.exporter.to_igraph()\n",
    "g_abcd.vs['comm'] = g_abcd.vs['ground_truth_community']\n",
    "\n",
    "## plot\n",
    "g_abcd.vs['size'] = 7\n",
    "g_abcd.es['color'] = cls_edges\n",
    "g_abcd.vs['color'] = [cls[i] for i in g_abcd.vs['comm']]\n",
    "ig.plot(g_abcd, bbox=(0,0,400,300))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Girvan-Newman algorithm\n",
    "\n",
    "We plot the modularity and AMI for each cut from the GN algorithm.\n",
    "\n",
    "In this case, both modularity and AMI are maximized with 3 communities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = []\n",
    "a = []\n",
    "gn = g_abcd.community_edge_betweenness()\n",
    "for i in np.arange(g_abcd.vcount()):\n",
    "    q.append(g_abcd.modularity(gn.as_clustering(n=i+1)))\n",
    "    a.append(AMI(g_abcd.vs['comm'],gn.as_clustering(n=i+1).membership))\n",
    "plt.plot(np.arange(1,1+g_abcd.vcount()),q,'.-',color='black',label='modularity')\n",
    "plt.plot(np.arange(1,1+g_abcd.vcount()),a,'.-',color='grey',label='AMI')\n",
    "plt.xlabel('number of clusters',fontsize=14)\n",
    "plt.ylabel('modularity or AMI',fontsize=14)\n",
    "plt.legend();\n",
    "#plt.savefig('abcd_dendrogram.eps');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with 3 communities, $q=0.480$ and AMI=1, so perfect recovery.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comm = np.arange(1,g_abcd.vcount()+1)\n",
    "D = pd.DataFrame(np.array([n_comm, q, a]).transpose(),columns=['n_comm','q','AMI'])\n",
    "D.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would we get with 4 clusters, for which AMI = 0.95 is also quite high?\n",
    "We see below that we have a few nodes splitted from one community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4 communities\n",
    "g_abcd.vs['gn'] = gn.as_clustering(n=4).membership\n",
    "small_comm = Counter(g_abcd.vs['gn']).most_common()[-1][0] ## smallest community\n",
    "## show the small community with larger nodes and dark edges\n",
    "for v in np.where(np.array(g_abcd.vs['gn'])==small_comm):\n",
    "    g_abcd.vs[v]['size'] = 10\n",
    "for e in g_abcd.es:\n",
    "    if (g_abcd.vs[e.source]['gn'] == small_comm) and (g_abcd.vs[e.target]['gn'] == small_comm):\n",
    "        e['color'] = 'black'\n",
    "g_abcd.vs['color'] = [cls[i] for i in g_abcd.vs['gn']]\n",
    "ly = g_abcd.layout()\n",
    "#ig.plot(g_abcd, 'abcd_4.eps', bbox=(0,0,400,300))\n",
    "ig.plot(g_abcd, layout=ly, bbox=(0,0,400,300))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ABCD graph with outliers\n",
    "\n",
    "Similar to the 100-node ABCD graph, but this time, 10 nodes are \"outliers\" (marked as community 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Python-ABCD\n",
    "n = 100\n",
    "xi = 0.2\n",
    "\n",
    "## degrees\n",
    "gamma = 3.0\n",
    "delta = 5\n",
    "Delta = 15\n",
    "\n",
    "## communities\n",
    "beta = 2.0\n",
    "s = 20\n",
    "S = 40\n",
    "\n",
    "## outliers\n",
    "outliers = 10\n",
    "\n",
    "## build\n",
    "random.seed(4321)\n",
    "np.random.seed(4321)\n",
    "\n",
    "params = ABCDParams(vcount=n, gamma=gamma, min_degree=delta, max_degree=Delta, beta=beta, \n",
    "                    min_community_size=s, max_community_size=S, xi=xi, num_outliers=outliers)\n",
    "g = ABCDGraph(params).build()\n",
    "g_abcd_o = g.exporter.to_igraph()\n",
    "g_abcd_o.vs['comm'] = [x+1 for x in g_abcd_o.vs['ground_truth_community']]\n",
    "g_abcd_o.vs['leiden'] = g_abcd_o.community_leiden(objective_function='modularity').membership\n",
    "\n",
    "## plot\n",
    "g_abcd_o.vs['size'] = 7\n",
    "g_abcd_o.es['color'] = cls_edges\n",
    "g_abcd_o.vs['color'] = [cls[i] for i in g_abcd_o.vs['comm']]\n",
    "ig.plot(g_abcd_o, bbox=(0,0,400,300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## community-based features for outlier and other nodes\n",
    "df = pd.DataFrame(g_abcd_o.vs['comm'], columns=['community'])\n",
    "df['cdd'] = cdd(g_abcd_o, g_abcd_o.vs['leiden'])\n",
    "df['cas'] = cas(g_abcd_o, g_abcd_o.vs['leiden'])\n",
    "df['pc'] = pc(g_abcd_o, g_abcd_o.vs['leiden'])\n",
    "df.groupby(by='community').quantile([.25,.75])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot - node size proportional to participation coefficient\n",
    "g_abcd_o.vs['size'] = [4+1.2/x for x in df['cas']]\n",
    "g_abcd_o.es['color'] = cls_edges\n",
    "g_abcd_o.vs['color'] = [cls[(3-i)%4] for i in g_abcd_o.vs['comm']]\n",
    "ig.plot(g_abcd_o, bbox=(0,0,400,300))\n",
    "#ig.plot(g_abcd_o, 'abcd+o.eps', bbox=(0,0,400,300))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## American College Football Graph\n",
    "\n",
    "This is a nice, small graph for illustrating anomaly detection methods.\n",
    "\n",
    "[Ref]: \"Community structure in social and biological networks\", M. Girvan and M. E. J. Newman PNAS June 11, 2002 99 (12) 7821-7826; https://doi.org/10.1073/pnas.122653799\n",
    "with corrections to the labels as described in: https://arxiv.org/pdf/1009.0638\n",
    "\n",
    "The graph has 115 nodes, 613 edges and after the corrections, there are  12 communities corresponding to Football Conferences.\n",
    "\n",
    "One of these communities (labelled as '5') is in fact a group of **independent** teamswhich we use as surrogate for **outlier** nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(datadir+'Football/football.pkl', 'rb') as fp:\n",
    "    cfg = pickle.load(fp)\n",
    "cfg.vs['community'] = cfg.vs['gt']\n",
    "\n",
    "## plot the College Football Graph\n",
    "## show communities in dfferent colors\n",
    "## show known anomalies as triangles\n",
    "cfg.vs['shape'] = 'circle'\n",
    "cfg.vs['anomaly'] = 0\n",
    "pal = ig.RainbowPalette(n=max(cfg.vs['community'])+1) \n",
    "cfg.vs['color'] = [pal.get(int(i)) for i in cfg.vs['community']]\n",
    "for v in cfg.vs:\n",
    "    if v['community']==5:\n",
    "        v['shape']='triangle'\n",
    "        v['anomaly']=1\n",
    "        v['color']='black'\n",
    "random.seed(321)\n",
    "ly = cfg.layout_fruchterman_reingold()\n",
    "ig.plot(cfg, layout=ly, bbox=(0,0,500,300), vertex_size=8, edge_color='lightgray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## greyscale version (for the book)\n",
    "pal = ig.GradientPalette(\"white\",\"black\",max(cfg.vs['community'])+1)\n",
    "cfg.vs['color'] = [pal.get(int(i)) for i in cfg.vs['community']]\n",
    "for v in cfg.vs:\n",
    "    if v['community']==5:\n",
    "        v['shape']='triangle'\n",
    "        v['anomaly']=1\n",
    "        v['color']='black'\n",
    "ig.plot(cfg, layout=ly, bbox=(0,0,500,300), vertex_size=8, edge_color='lightgray')\n",
    "#ig.plot(cfg, target=\"foot.eps\", layout=ly, bbox=(0,0,500,300), vertex_size=8, edge_color='lightgray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly detection\n",
    "\n",
    "We apply some of the community-based measures we defined earlier to find the *anomalous* nodes, namely looking at:\n",
    "\n",
    "* the participation coefficient (pc) - high values are indicative of anomalous nodes\n",
    "* the community association strength (cas) - low values are indicative of anomalous nodes\n",
    "* the community distribution distance (cdd)- low values are indicative of anomalous nodes\n",
    "\n",
    "The rationale is that an *anomalous* node will be difficult to place in a single cluster.\n",
    "In the plot below, we show the distribution of those scores amongst the anomalous and non-anomalous nodes respecvively.\n",
    "For the communities, we use the results from the ECG algorithm rather than using the ground-truth communities.\n",
    "\n",
    "We also show the ROC curves for each measure and compute the AUC (area under the ROC curve), \n",
    "A ROC curve is a plot of the true positive rate (TPR) against the false positive rate (FPR) at each threshold value.\n",
    "The area unde rthe ROC curve, the AUC, can be interpreted as the probability that a randomly chosen positive case has a higher \n",
    "score than a randomly chosen negative case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "cfg_comm = cfg.community_ecg(final='leiden').membership\n",
    "ROC_cdd = roc_curve(cfg.vs['anomaly'], 1-np.array(cdd(cfg,cfg_comm)))\n",
    "ROC_cas = roc_curve(cfg.vs['anomaly'], 1-np.array(cas(cfg,cfg_comm)))\n",
    "ROC_pc = roc_curve(cfg.vs['anomaly'], np.array(pc(cfg,cfg_comm)))\n",
    "AUC_cdd = roc_auc_score(cfg.vs['anomaly'], 1-np.array(cdd(cfg,cfg_comm)))\n",
    "AUC_cas = roc_auc_score(cfg.vs['anomaly'], 1-np.array(cas(cfg,cfg_comm)))\n",
    "AUC_pc = roc_auc_score(cfg.vs['anomaly'], np.array(pc(cfg,cfg_comm)))\n",
    "\n",
    "plt.plot(ROC_pc[0],ROC_pc[1], label='pc,  AUC='+\"{:.3f}\".format(AUC_pc))\n",
    "plt.plot(ROC_cdd[0],ROC_cdd[1], label='cdd, AUC='+\"{:.3f}\".format(AUC_cdd))\n",
    "plt.plot(ROC_cas[0],ROC_cas[1], label='cas, AUC='+\"{:.3f}\".format(AUC_cas))\n",
    "plt.legend()\n",
    "plt.xlabel('False positive rate (FPR)', fontsize=14)\n",
    "plt.ylabel('True positive rate (TPR)', fontsize=14);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## build dataframe with community-based features\n",
    "n = cfg.vcount()\n",
    "df = pd.DataFrame()\n",
    "df['measure'] = np.concatenate((np.repeat('pc',n), np.repeat('cas',n), np.repeat('cdd',n)))\n",
    "df['value'] = np.concatenate((np.array(pc(cfg,cfg_comm)), np.array(cas(cfg,cfg_comm)), np.array(cdd(cfg,cfg_comm))))\n",
    "df['anomaly'] = np.concatenate((np.array(cfg.vs['anomaly'])==1,np.array(cfg.vs['anomaly'])==1,np.array(cfg.vs['anomaly'])==1))\n",
    "\n",
    "## boxplots\n",
    "flierprops = dict(markerfacecolor='black', markersize=2,  markeredgecolor='black')\n",
    "bp = sns.boxplot(data=df, x='measure', y='value', hue='anomaly', palette=cls[:2], flierprops=flierprops);\n",
    "bp.set_xlabel(\"Measure\", fontsize=14)\n",
    "bp.set_ylabel(\"Value\", fontsize=14)\n",
    "bp.tick_params(labelsize=12)\n",
    "plt.show()\n",
    "#plt.savefig('anomalies_foot.eps');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABCD with varying $\\xi$ -- Illustration\n",
    "\n",
    "The cell below is for illustration purpose only, to show some small ABCD graphs with different $\\xi$ (noise) parameters.\n",
    "\n",
    "* notice the density of edges between communities as $\\xi$ increases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ABCD with varying community strength (xi)\n",
    "n = 100\n",
    "\n",
    "## degrees\n",
    "gamma = 2.5\n",
    "delta = 5\n",
    "Delta = 15\n",
    "zeta = np.log(Delta) / np.log(n)\n",
    "\n",
    "## communities\n",
    "beta = 1.5\n",
    "s = 25\n",
    "S = 50\n",
    "tau = np.log(S) / np.log(n)\n",
    "\n",
    "XIs = [.05,.15,.33,.5]\n",
    "\n",
    "np.random.seed(321)\n",
    "random.seed(321)\n",
    "G = ig.Graph()\n",
    "for i in range(len(XIs)):\n",
    "    xi = XIs[i]\n",
    "    params = ABCDParams(vcount=n, gamma=gamma, min_degree=delta, max_degree=Delta, beta=beta, min_community_size=s, max_community_size=S, xi=xi)\n",
    "    g = ABCDGraph(params).build()\n",
    "    G[i] = g.exporter.to_igraph()\n",
    "    comms = G[i].vs['ground_truth_community'] \n",
    "    G[i].vs['color'] = [cls[j+1] for j in comms]\n",
    "\n",
    "\n",
    "## plot the above graphs\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax = axs[i,j]\n",
    "        ly = G[2*i+j].layout_fruchterman_reingold()\n",
    "        ig.plot(G[2*i+j], bbox=(0,0,300,300), target=ax, layout=ly,\n",
    "                vertex_size=5, edge_color='lightgrey', edge_width=0.5)\n",
    "        ttl = r'$\\xi =$ ' + str(XIs[2*i+j])\n",
    "        ax.set_title(ttl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABCD with varying $\\xi$ -- Experiments\n",
    "\n",
    "Here we show a typical way to compare graph clustering algorithms using benchmark graphs. \n",
    "We pick some model, here ABCD, and we vary the noise parameter $\\xi$. \n",
    "With ABCD, the larger $\\xi$ is, the closer we are to a random Chung-Lu or configuration model graph (i.e. where only the degree distribution matters). For $\\xi=0$, we get pure communities (all edges are internal).\n",
    "\n",
    "For each choice of $\\xi$, we generate 30 graphs, apply several different clustering algorithms,\n",
    "and compute AMI (adjusted mutual information) for each algorithm, comparing with ground-truth communities.\n",
    "\n",
    "The code below is commented out as it can take a while to run; a pickle file with results is included in the Data directory. To re-run from scratch, uncomment the cell below.\n",
    "\n",
    "Parameters for the ABCD benchmark graphs are:\n",
    "\n",
    "* $n=1,000$\n",
    "* $\\gamma=2.5$\n",
    "* $\\tau=1.5$\n",
    "* degree range [10,50]\n",
    "* community size range [50,100]\n",
    "* $0.3 \\le \\xi \\le 0.8$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the results below. \n",
    "We see good results with Leiden and Infomap, and slightly better results with ECG.\n",
    "Label propagation is a fast algortihm, but it does collapse with moderate to high level of noise.\n",
    "\n",
    "From the standard deviation plot, we see high variability around the value(s) for $\\xi$ where the different\n",
    "algorithms start to collapse. We see that this happen later and at a smaller scale with EGC, which is known to have good stability.\n",
    "\n",
    "ECG and Leiden are often good choices for *unweighted* graphs while for *weighted* graphs, Leiden is usually a good option.\n",
    "\n",
    "Such studies are useful to compare algorithms; using benchmarks, we can directly control parameters such as the noise level.\n",
    "\n",
    "Uncomment the cell below to re-run all the experiments (can take 4-5 minutes), otherwise we load the results in the next cell.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "## common ABCD graph parameters\n",
    "\n",
    "n = 1000\n",
    "\n",
    "## degrees\n",
    "gamma = 2.5\n",
    "delta = 10\n",
    "Delta = 50\n",
    "\n",
    "## communities\n",
    "beta = 1.5\n",
    "s = 50\n",
    "S = 100\n",
    "\n",
    "## generate the graphs and run various clustering algorithms\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "REP = 30\n",
    "L = []\n",
    "for xi in np.arange(.3,.801,.02):\n",
    "    print(xi)\n",
    "    for rep in range(REP):\n",
    "        \n",
    "        params = ABCDParams(vcount=n, gamma=gamma, min_degree=delta, max_degree=Delta, beta=beta, min_community_size=s, max_community_size=S, xi=xi)\n",
    "        g = ABCDGraph(params).build()\n",
    "        G = g.exporter.to_igraph()\n",
    "\n",
    "        v = [xi, AMI(G.community_leiden(objective_function='modularity').membership, G.vs['ground_truth_community']),\n",
    "             AMI(G.community_ecg(ens_size=16, final='leiden').membership, G.vs['ground_truth_community']),\n",
    "             AMI(G.community_infomap().membership, G.vs['ground_truth_community']),\n",
    "             AMI(G.community_label_propagation().membership, G.vs['ground_truth_community'])\n",
    "            ]\n",
    "        L.append(v)\n",
    "df = pd.DataFrame(L, columns=['xi', 'leiden','ecg','infomap','lp'])\n",
    "\n",
    "# with open( datadir+\"ABCD/abcd_python_study.pkl\", \"wb\" ) as f:\n",
    "#     pickle.dump(df, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data generated with the code from above cell \n",
    "with open(datadir+\"ABCD/abcd_python_study.pkl\",\"rb\") as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mean\n",
    "lt = ['-','--','-.',':']\n",
    "D = df.groupby('xi', as_index=False).mean()\n",
    "plt.plot(D.xi,D.ecg,lt[0],label='ECG',color=cls[3])\n",
    "plt.plot(D.xi,D.leiden,lt[1],label='Leiden',color=cls[3])\n",
    "plt.plot(D.xi,D.infomap,lt[2],label='Infomap',color=cls[3])\n",
    "plt.plot(D.xi,D.lp,lt[3],label='Label Prop.',color=cls[3])\n",
    "plt.xlabel(r'ABCD noise ($\\xi$)', fontsize=14)\n",
    "plt.ylabel('AMI', fontsize=14)\n",
    "plt.legend();\n",
    "#plt.savefig('abcd_study.eps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Standard deviation\n",
    "D = df.groupby('xi', as_index=False).std()\n",
    "plt.plot(D.xi,D.ecg,lt[0],label='ECG',color=cls[3])\n",
    "plt.plot(D.xi,D.leiden,lt[1],label='Leiden',color=cls[3])\n",
    "plt.plot(D.xi,D.infomap,lt[2],label='Infomap',color=cls[3])\n",
    "plt.plot(D.xi,D.lp,lt[3],label='Label Prop.',color=cls[3])\n",
    "plt.xlabel(r'ABCD noise ($\\xi$)', fontsize=14)\n",
    "plt.ylabel('Standard Deviation (AMI)', fontsize=14)\n",
    "plt.legend();\n",
    "#plt.savefig('abcd_study_stdv.eps')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare stability \n",
    "\n",
    "This study is similar to the previous one, but this time we run each algorithm **twice** on each graph, and we compare the similarity for each such pair of partitions, instead of comparing with the ground truth.\n",
    "\n",
    "Thus we look at the **stability** of the algorithms. \n",
    "\n",
    "Note that an algorithm can be stable, but still be bad (ex: always cluster all nodes in a single community).\n",
    "\n",
    "The code below can take a while to run; a pickle file with results is included in the Data directory. To re-run from scratch, uncomment the cell below.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "## same graph - mean AMI between successive runs\n",
    "## takes about 10 min to run with REP = 30\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "L = []\n",
    "REP = 30\n",
    "for xi in np.arange(.3,.801,.02):\n",
    "    print(xi)\n",
    "    for rep in range(REP):\n",
    "        V = [xi]\n",
    "\n",
    "        params = ABCDParams(vcount=n, gamma=gamma, min_degree=delta, max_degree=Delta, beta=beta, min_community_size=s, max_community_size=S, xi=xi)\n",
    "        g = ABCDGraph(params).build()\n",
    "        G = g.exporter.to_igraph()\n",
    "        \n",
    "        idx = np.random.permutation(G.vcount())\n",
    "        ## same graph - permute vertices\n",
    "        Gp = ig.Graph.Erdos_Renyi(n=G.vcount(),p=0)\n",
    "        for e in G.es():\n",
    "            Gp.add_edge(idx[e.tuple[0]],idx[e.tuple[1]])\n",
    "\n",
    "        x = Gp.community_leiden(objective_function='modularity').membership\n",
    "        V.append(AMI(G.community_leiden(objective_function='modularity').membership, [x[idx[i]] for i in range(len(idx))] ))\n",
    "        \n",
    "        x = Gp.community_ecg(ens_size=16, final='leiden').membership\n",
    "        V.append(AMI(G.community_ecg(ens_size=16, final='leiden').membership, [x[idx[i]] for i in range(len(idx))] ))\n",
    "        \n",
    "        x = Gp.community_infomap().membership\n",
    "        V.append(AMI(G.community_infomap().membership, [x[idx[i]] for i in range(len(idx))] ))\n",
    "\n",
    "        x = Gp.community_label_propagation().membership\n",
    "        V.append(AMI(G.community_label_propagation().membership, [x[idx[i]] for i in range(len(idx))] ))\n",
    "\n",
    "        L.append(V)\n",
    "df = pd.DataFrame(L, columns=['xi', 'leiden','ecg','infomap','lp'])\n",
    "\n",
    "# save results \n",
    "# with open( datadir+\"ABCD/abcd_python_study_stability.pkl\", \"wb\" ) as f:\n",
    "#     pickle.dump(df, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the results below. The behaviour of algorithms can be clustered in two groups:\n",
    "\n",
    "* For Leiden and ECG, stability is excellent and degrades gradually for high noise level.\n",
    "* For Infomap and Label Propagation, stability is also good until the noise value where the results start to degrade, as we saw in the previous study. We see near perfect stability for very high noise values; those are values where the results were very bad in the previous study; this typically happens when the algorithm can't get any good clustering and returns some trivial partition, such as putting all nodes together in the same community, thus a stable but bad result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load results\n",
    "with open(datadir+\"ABCD/abcd_python_study_stability.pkl\",\"rb\") as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mean\n",
    "lt = ['-','--','-.',':']\n",
    "D = df.groupby('xi', as_index=False).mean()\n",
    "plt.plot(D.xi,D.ecg,lt[0],label='ECG',color=cls[3])\n",
    "plt.plot(D.xi,D.leiden,lt[1],label='Leiden',color=cls[3])\n",
    "plt.plot(D.xi,D.infomap,lt[2],label='Infomap',color=cls[3])\n",
    "plt.plot(D.xi,D.lp,lt[3],label='Label Prop.',color=cls[3])\n",
    "plt.xlabel(r'ABCD noise ($\\xi$)', fontsize=14)\n",
    "plt.ylabel('AMI between successive runs', fontsize=14)\n",
    "plt.legend();\n",
    "#plt.savefig('abcd_study_stability.eps')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularity, resolution limit and rings of cliques\n",
    "\n",
    "We illustrate issues with modularity with the famous ring of cliques examples.\n",
    "\n",
    "For example below, we have a ring of 3-cliques connected by a single (inter-clique) edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build a RoC with n cliques of size s\n",
    "def ringOfCliques(n, s):\n",
    "    roc = ig.Graph.Erdos_Renyi(n=n*s,p=0)\n",
    "    ## cliques\n",
    "    for i in range(n):\n",
    "        for j in np.arange(s*i,s*(i+1)):\n",
    "            for k in np.arange(j+1,s*(i+1)):\n",
    "                roc.add_edge(j,k)\n",
    "    ## ring\n",
    "    for i in range(n):\n",
    "        if i>0:\n",
    "            roc.add_edge(s*i-1,s*i)\n",
    "        else:\n",
    "            roc.add_edge(n*s-1,0)\n",
    "    roc.vs['size'] = 8\n",
    "    roc.vs['color'] = cls[2]\n",
    "    roc.es['color'] = cls_edges\n",
    "    return roc\n",
    "\n",
    "## Ex: 10 3-cliques\n",
    "roc = ringOfCliques(10,3)\n",
    "#ig.plot(roc,'ring_3.eps',bbox=(0,0,300,300))     \n",
    "ig.plot(roc,bbox=(0,0,300,300))     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the number of cliques (the natural parts in a partition) with the actual number of communities found via modularity based algorithms Leiden and ECG.\n",
    "\n",
    "With Leiden as is (i.e. resolution parameter 1.0), we see a smaller number of communities than the number of cliques; \n",
    "this is a known problem with modularity: merging cliques in the same community often lead to higher modularity.\n",
    "\n",
    "One way to alleviate this issue is to increase the **resolution parameter**; with resolution parameter 5 instead of the default value of 1, we see that we can recover \n",
    "the cliques in most cases. \n",
    "\n",
    "A concensus algorithm like ECG can help a lot in such cases; below we see that all the cliques are correctly recovered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare number of cliques and number of clusters found\n",
    "L = []\n",
    "s = 3\n",
    "for n in np.arange(3,50,3):\n",
    "    roc = ringOfCliques(n, s)\n",
    "    le = np.max(roc.community_leiden(objective_function='modularity').membership)+1\n",
    "    ler = np.max(roc.community_leiden(objective_function='modularity', resolution=5.0).membership)+1\n",
    "    ec = np.max(roc.community_ecg().membership)+1\n",
    "    L.append([n,ler,le,ec])\n",
    "D = pd.DataFrame(L,columns=['n','Leiden_res','Leiden','ECG'])\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(D['n'],D['Leiden'],':o',color=cls[3],label='Leiden (default)')\n",
    "plt.plot(D['n'],D['Leiden_res'],'--o',color=cls[3],label='Leiden (res.=5)')\n",
    "plt.plot(D['n'],D['ECG'],'-o',color=cls[3],label='ECG')\n",
    "plt.xlabel('number of '+str(s)+'-cliques',fontsize=14)\n",
    "plt.ylabel('number of clusters found',fontsize=14)\n",
    "plt.legend(fontsize=14);\n",
    "#plt.savefig('rings.eps');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at a specific example: 10 cliques of size 3. Below we plot the communities found with Leiden; we clearly see that several **pairs** of communities are grouped into clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Leiden communities with 10 3-cliques\n",
    "roc = ringOfCliques(n=10, s=3)\n",
    "roc.vs['le'] = roc.community_leiden(objective_function='modularity').membership\n",
    "roc.vs['color'] = [cls[x%3] for x in roc.vs['le']]\n",
    "#ig.plot(roc,'ring_3_q.eps', bbox=(0,0,300,300))\n",
    "ig.plot(roc,bbox=(0,0,300,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is ECG solving this problem? It is due to the first step, where we run an ensemble of level-1 Louvain and assign new weights to edges based on the proportion of times those edges are internal to a community.\n",
    "\n",
    "We see below that there are exactly 30 edges with maximal edge weight of 1 (edges within cliques) and 10 edges with default minimal weight of 0.05 (edges between cliques). \n",
    "\n",
    "With those new weights, the last clustering in ECG can easily recover the cliques as communities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ECG weights in this case: all 30 clique edges have max score\n",
    "roc.es['W'] = roc.community_ecg().W\n",
    "Counter(roc.es['W'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ego nets and more\n",
    "\n",
    "Suppose we want to look at the node that are *near* some seed node $v$. \n",
    "\n",
    "One common way to do this is to look at its ego-net, i.e. the subgraph consisting of node $v$ and all other nodes that can be reached from $v$ in $k$ hops or less, where $k$ is small, typically 1 or 2. \n",
    "\n",
    "Such subgraphs can become large quickly as we increase $k$. In the cells below, we look at ego-nets and compare with another approach to extract subgraph(s) around $v$ via clustering.\n",
    "\n",
    "We consider the **airport** graph we already saw several times. We consider a simple, undirected version (no loops, directions or edge weights).\n",
    "\n",
    "We compare ego-nets (1 and 2-hops subgraphs from a given node) with clusters obtained via graph clustering for some vertex $v$ (airport 'MQT') which has degree 11 (you can try other vertices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read edges and build simple undirected graph\n",
    "D = pd.read_csv(datadir+'Airports/connections.csv')\n",
    "g = ig.Graph.TupleList([tuple(x) for x in D.values], directed=True, edge_attrs=['weight'])\n",
    "g = g.as_undirected()\n",
    "g = g.simplify()\n",
    "\n",
    "## read vertex attributes and add to graph\n",
    "A = pd.read_csv(datadir+'Airports/airports_loc.csv')\n",
    "lookup = {k:v for v,k in enumerate(A['airport'])}\n",
    "l = [lookup[x] for x in g.vs()['name']]\n",
    "g.vs()['layout'] = [(A['lon'][i],A['lat'][i]) for i in l]\n",
    "g.vs()['state'] = [A['state'][i] for i in l]\n",
    "g.vs()['city'] = [A['city'][i] for i in l]\n",
    "\n",
    "## add a few more attributes for visualization\n",
    "g.vs()['size'] = 8\n",
    "g.vs()['color'] = cls[1]\n",
    "g.es()['color'] = cls_edges\n",
    "df = A.head()\n",
    "\n",
    "## pick a vertex v ('MQT' airport)\n",
    "v = g.vs.find('MQT').index\n",
    "print(g.vs[v])\n",
    "print('vertex degree:',g.degree()[v])\n",
    "g.vs[v]['color'] = 'black'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show its ego-net for k=1 (vertex v shown in black)\n",
    "sg = g.subgraph([i for i in g.neighborhood(v,order=1)])\n",
    "print(sg.vcount(),'nodes')\n",
    "#ig.plot(sg,'airport_ego_1.eps',bbox=(0,0,300,300))\n",
    "ig.plot(sg,bbox=(0,0,300,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show its 2-hops ego-net ... this is already quite large!\n",
    "sg = g.subgraph([i for i in g.neighborhood(v,order=2)])\n",
    "sg.vs()['core'] = sg.coreness()\n",
    "sg.delete_vertices([v for v in sg.vs if v['core']<2])\n",
    "print(sg.vcount(),'nodes')\n",
    "#ig.plot(sg,'airport_ego_2.eps',bbox=(0,0,300,300))\n",
    "ig.plot(sg,bbox=(0,0,300,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply clustering and show the cluster containing the selected vertex\n",
    "## recall that we ignore edge weights\n",
    "random.seed(12345) ## seed for the clustering algorithm\n",
    "np.random.seed(12345) ## seed for the random permutations used\n",
    "ec = g.community_ecg(ens_size=32)\n",
    "g.es['W'] = ec.W\n",
    "m = ec.membership[v]\n",
    "sg = g.subgraph([i for i in range(g.vcount()) if ec.membership[i]==m])\n",
    "sg.vs()['core'] = sg.coreness()\n",
    "## display the 2-core\n",
    "sg.delete_vertices([v for v in sg.vs if v['core']<2])\n",
    "print(sg.vcount(),'nodes')\n",
    "#ig.plot(sg,'airport_ecg.eps',bbox=(0,0,300,300))\n",
    "ig.plot(sg,bbox=(0,0,300,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above that looking at the cluster with $v$ is smaller than the 2-hops ego-net, and several nodes are tightly connected.\n",
    "\n",
    "Below we go further and look at the ECG edge weights, which we can use to prune the graph above, so we can look at the nodes most tightly connected to node $v$.\n",
    "\n",
    "You can adjust the threshold below to get different zoomings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter edges w.r.t. ECG votes (weights)\n",
    "thresh = .85\n",
    "\n",
    "tmp = sg.subgraph_edges([e for e in sg.es if e['W'] > thresh])\n",
    "n = [i for i in range(tmp.vcount()) if tmp.vs[i]['color']=='black'][0]\n",
    "tmp.vs['cl'] = tmp.connected_components().membership\n",
    "cl = tmp.vs[n]['cl']\n",
    "ssg = tmp.subgraph([i for i in tmp.vs if i['cl']==cl])\n",
    "ssg.vs()['core'] = ssg.coreness()\n",
    "ssg.delete_vertices([v for v in ssg.vs if v['core']<2])\n",
    "print(ssg.vcount(),'nodes')\n",
    "#ig.plot(ssg,'airport_ecg_focus.eps',bbox=(0,0,300,300))\n",
    "ig.plot(ssg, bbox=(0,0,300,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most nodes in this subgraph are from the same state as node $v$ (MI) or nearby state (WI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## states in the above subgraph\n",
    "Counter(ssg.vs['state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures to compare partitions - Figure 5.13\n",
    "\n",
    "* We illustrate the importance of using proper adjusted measures when comparing partitions; this is why we use AMI (adjusted mutual information) or ARI (adjusted Rand index) in our experiments\n",
    "* We generate some ABCD graph and compare ground truth with **random** partitions of different sizes\n",
    "* Scores for random partitions should be close to 0 regardless of the number of parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ABCD example\n",
    "n = 1000\n",
    "xi = .1\n",
    "\n",
    "## degrees\n",
    "gamma = 2.5\n",
    "delta = 5\n",
    "Delta = 50\n",
    "\n",
    "## communities\n",
    "beta = 1.5\n",
    "s = 75\n",
    "S = 150\n",
    "\n",
    "np.random.seed(321)\n",
    "random.seed(321)\n",
    "params = ABCDParams(vcount=n, gamma=gamma, min_degree=delta, max_degree=Delta, beta=beta, min_community_size=s, max_community_size=S, xi=xi)\n",
    "g = ABCDGraph(params).build()\n",
    "G = g.exporter.to_igraph()\n",
    "print('number of communities:',max(G.vs['ground_truth_community'])+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## generate random clusterings and compute various measures w.r.t. ground truth\n",
    "## this can take a few minutes to run\n",
    "from sklearn.metrics import mutual_info_score as MI\n",
    "from sklearn.metrics import rand_score as RI\n",
    "from sklearn.metrics import adjusted_rand_score as ARI\n",
    "from sklearn.metrics import normalized_mutual_info_score as NMI\n",
    "\n",
    "REP = 10 ## we used 100 for the textbook\n",
    "\n",
    "L = []\n",
    "gt = G.vs['ground_truth_community']\n",
    "n = G.vcount()\n",
    "tc = {idx:part for idx,part in enumerate(gt)}\n",
    "ar = np.arange(2,21)\n",
    "for s in ar:\n",
    "    for i in range(REP):\n",
    "        r = np.random.choice(s, size=n)\n",
    "        rd = {idx:part for idx,part in enumerate(r)}        \n",
    "        L.append([s,MI(gt,r),NMI(gt,r),AMI(gt,r),RI(gt,r),ARI(gt,r),G.gam(tc,rd,adjusted=False),G.gam(tc,rd)])\n",
    "df = pd.DataFrame(L,columns=['size','MI','NMI','AMI','RI','ARI','GRI','AGRI'])\n",
    "df_avg = df.groupby(by='size').mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show results for 3 measures:\n",
    "* Mutual information (MI), which has strong bias w.r.t. number of clusters\n",
    "* Normalized MI (NMI) is better\n",
    "* AMI is best, no bias w.r.t. number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mutual information (MI), normalized MI (NMI) and adjusted MI (AMI)\n",
    "plt.plot(ar, df_avg['MI'],':',color='black',label='MI')\n",
    "plt.plot(ar, df_avg['NMI'],'--',color='black',label='NMI')\n",
    "plt.plot(ar, df_avg['AMI'],'-',color='black',label='AMI')\n",
    "plt.xlabel('number of random clusters',fontsize=14)\n",
    "plt.legend();\n",
    "#plt.savefig('MI.eps');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same below for Rand index (RI) and adjusted version. \n",
    "\n",
    "GRI (graph RI) and AGRI (adjusted GRI) are variations of RI specifically for graph data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAND index (RI) and adjusted (ARI)\n",
    "## Also: Graph-aware RAND index (GRI) and adjusted version (AGRI)\n",
    "## those measures are included in \"partition-igraph\"\n",
    "## input are partitions of type 'igraph.clustering.VertexClustering'or a dictionaries of node:community.\n",
    "plt.plot(ar, df_avg['RI'],':',color='black',label='RI')\n",
    "plt.plot(ar, df_avg['GRI'],'--',color='black',label='GRI')\n",
    "plt.plot(ar, df_avg['ARI'],'-',color='black',label='ARI/AGRI')\n",
    "plt.plot(ar, df_avg['AGRI'],'-',color='black')\n",
    "plt.xlabel('number of random clusters',fontsize=14)\n",
    "plt.legend();\n",
    "#plt.savefig('RI.eps');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complexnetworks",
   "language": "python",
   "name": "complexnetworks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
