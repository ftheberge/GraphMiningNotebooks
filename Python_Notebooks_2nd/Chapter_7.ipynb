{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 - Hypergraphs\n",
    "\n",
    "In this notebook, we introduce hypergraphs, a generalization of graphs where we allow for arbitrary sized edges (in practice, we usually consider only edges of size 2 or more). \n",
    "\n",
    "We illustrate a few concepts using hypergraphs including modularity, community detection, simpliciality and transformation into 2-section graphs.\n",
    "\n",
    "(If using HNX version **2.4**, remove all instances of: ```{'with_node_counts': False}```)\n",
    "\n",
    "We also do some visualization with **XGI** (https://xgi.readthedocs.io/en/stable/index.html), which can by pip installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import hypernetx as hnx\n",
    "import hypernetx.algorithms.hypergraph_modularity as hmod \n",
    "import xgi \n",
    "import pickle\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import random\n",
    "import networkx as nx\n",
    "from sklearn.metrics import adjusted_mutual_info_score as AMI\n",
    "from itertools import combinations\n",
    "import seaborn as sns\n",
    "import fastnode2vec as n2v\n",
    "import umap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "## Some Python files:\n",
    "\n",
    "# Functions to compute various simpliciality measures\n",
    "import simpliciality as spl\n",
    "\n",
    "# h-Louvain preliminary version\n",
    "import h_louvain as hl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set this to the data directory\n",
    "datadir='../Datasets/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to compute degree-size correlation\n",
    "def h_deg_size_corr(H):\n",
    "    deg = {v:H.degree(v) for v in H.nodes}\n",
    "    X = []\n",
    "    Y = []\n",
    "    for e in H.edges:\n",
    "        for v in H.edges[e]:\n",
    "            X.append(deg[v])\n",
    "            Y.append(len(H.edges[e]))\n",
    "    return(X, Y, np.corrcoef(X,Y)[1,0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperNetX basics with a toy hypergraph\n",
    "\n",
    "We illustrate a few concepts with a toy hypergraph. \n",
    "\n",
    "First, we build the HNX hypergraph from a list of sets (the hyperedges), and we draw the hypergraph as well as its dual (where the role of nodes and hyperedges are swapped).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build an hypergraph from a list of sets (the hyperedges)\n",
    "E = [{'A','B'},{'A','C'},{'A','B','C'},{'A','D','E','F'},{'D','F'},{'E','F'},{'B'},{'G','B'}]\n",
    "#kwargs = {'layout_kwargs': {'seed': 123}, 'with_node_counts': False}\n",
    "kwargs = {'layout_kwargs': {'seed': 123}}\n",
    "## using enumeration, edges will have integer IDs\n",
    "H = hnx.Hypergraph(dict(enumerate(E)))\n",
    "for e in H.edges:\n",
    "    H.edges[e].weight = 1.0\n",
    "edges_kwargs={'edgecolors':'grey'}\n",
    "with warnings.catch_warnings(): ## matplotlib warning\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    plt.figure(figsize=(8,5))\n",
    "    hnx.draw(H, **kwargs, edges_kwargs=edges_kwargs, edge_label_alpha=1,\n",
    "             node_labels_kwargs={'fontsize': 9},\n",
    "             edge_labels_kwargs={'fontsize': 7}\n",
    "            )\n",
    "#plt.savefig('h_toy_a.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dual hypergraph\n",
    "H_dual = H.dual()\n",
    "#kwargs = {'layout_kwargs': {'seed': 123}, 'with_node_counts': False, 'with_edge_labels':True}\n",
    "kwargs = {'layout_kwargs': {'seed': 123}, 'with_edge_labels':True}\n",
    "edges_kwargs={'edgecolors':'grey'}\n",
    "plt.figure(figsize=(8,5))\n",
    "hnx.draw(H_dual, **kwargs, edges_kwargs=edges_kwargs, edge_label_alpha=1,\n",
    "         node_labels_kwargs={'fontsize': 9},\n",
    "         edge_labels_kwargs={'fontsize': 7}\n",
    "        )\n",
    "#plt.savefig('h_toy_b.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bipartite representation - HNX exports in networkx format\n",
    "B = ig.Graph.from_networkx(H.bipartite())\n",
    "B.vs['label'] = B.vs['_nx_name']\n",
    "ly = B.layout_bipartite(types='bipartite')\n",
    "#ig.plot(B, 'h_toy_c.pdf', bbox=(400,300), vertex_color='white', layout=ly, vertex_label_size=14, edge_color='black')\n",
    "ig.plot(B, bbox=(400,300), vertex_color='white', layout=ly, vertex_label_size=14, edge_color='black')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show the nodes and edges\n",
    "print('shape:', H.shape)\n",
    "print('nodes:', [x for x in H.nodes()])\n",
    "print('edges:', [x for x in H.edges()])\n",
    "print('node degrees:', [(v,H.degree(v)) for v in H.nodes()])\n",
    "print('edge sizes:',[H.size(e) for e in H.edges()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## incidence dictionary\n",
    "H.incidence_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## incidence matrix\n",
    "H.incidence_matrix(index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## incidence matrix (in csr format by default - here showing whole array)\n",
    "M = H.incidence_matrix(index=True)\n",
    "df = pd.DataFrame(M[0].toarray(), \n",
    "                  index=M[1],\n",
    "                  columns=M[2])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2-section graph\n",
    "G = hmod.two_section(H)\n",
    "ig.plot(G, bbox=(400,300),vertex_label=G.vs['name'], \n",
    "        vertex_label_size=12, vertex_color='lightblue',\n",
    "        edge_width=G.es['weight'])\n",
    "#ig.plot(G, target=\"h_toy_d.pdf\", bbox=(400,300),vertex_label=G.vs['name'], vertex_label_size=14, vertex_color='white')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## s-walks and distance-based measures\n",
    "\n",
    "We illustrate a few concepts with the toy hypergraph defined earlier.\n",
    "\n",
    "Let $H=(V,E)$ a hypergraph, and consider its incidence matrix $B$ as defined in section 7.2. \n",
    "Consider also the dual hypergraph $H^*$, where the roles of nodes are hyperedges are swapped, \n",
    "namely the edges in $H$ are the nodes in $H^*$, \n",
    "and there is as edge two vertices in $H^*$ if the corresponding hyperedges in $H$ have a non-empty intersection.\n",
    "\n",
    "### s-walks and distances\n",
    "\n",
    "We define the concept of $s$-walks on a hypergraph as follows. A $s$-**walk** of length $k$ on $H$ is a sequences of edges $e_{i_0}, e_{i_1}, ..., e_{i_k}$ in $E$ such that \n",
    "all $|e_{i_{j-1}} \\cap e_{i_j}| \\ge s$ for $1 \\le j \\le k$ and all $i_{j-1} \\ne i_j$.\n",
    "\n",
    "The $s$-**distance** $d_s(e_i,e_j)$ between edges $e_i$ and $e_j$ is the length of the smallest $s$-walk between those, if it exists (else the distance is usually considered as infinity, and its inverse is set to zero).\n",
    "\n",
    "A subset $E_s \\subset E$ is an $s$-**connected component** if it is a maximal subset with an $s$-walk between all $e_i, e_j \\in E_s$.\n",
    "The $s$-**diameter** for $E_s$ is the maximal shortest path length between all $e_i, e_j \\in E_s$.\n",
    "\n",
    "Other concepts can also be defined using $s$-walks. For example for distinct $e_i, e_j, e_k \\in E$, if there is a $s$-walk $e_i, e_j, e_k$, we say that they form an $s$-**wedge**, and if there is an $s$ walk $e_i, e_j, e_k, e_i$, we can say those form an $s$-**triangle** and from those, we can define the $s$-**clustering coefficients** as in section 1.11.\n",
    "\n",
    "For **nodes**, all definitions above follow by considering the **dual** hypergraph. For example, a $s$-walk is a sequence of adjacent nodes such that each consecutive node pair in the walk share at least $s$ hyperedges; all other concepts defined above follow directly.\n",
    "\n",
    "#### toy example\n",
    "\n",
    "In the toy example above, with $s=2$, the sequence of edges 1-2-0 is an $s$-path since edges 1 and 2 share nodes A and C, and edges 2 and 0 share nodes A and B.\n",
    "\n",
    "In the dual toy hypergraph, again with $s=2$, the sequence of nodes (edges in the dual) D-F-E is a $s$-path since nodes D and F are both incident to edges 3 and 4, and nodes F and E are both incident to edges 3 and 5. \n",
    "Another $s$-path (with $s=2$) is C-A-B.\n",
    "\n",
    "With $s=1$, this corresponds to a walk on the (unweighted 2-section) graph, while for $s \\ge 2$, this concept only applies to hypergraphs.\n",
    "\n",
    "Below, we compute the distances between every pair of nodes (thus, using the $s$-walks on the dual). An infinite distance between a pair of nodes means that there is no $s$-path joining those.\n",
    "\n",
    "We see the correspondence between the $s=1$ and graph cases; moreover in those cases, we have a single connected component since every pairs of nodes is connected by a path.\n",
    "\n",
    "With $s=2$, we see several disconnected node pairs, so in this case, we have several $s$-connected components. From inspection of the table below, we see that nodes {A,B,C} are connected,\n",
    "nodes {D,E,F} are connected; node G is then an isolated node. We verify this claim below (we also do the same with the edges, i.e. using the $s$-walk on $H$ with $s=2$)                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## distances with s=1 and s=2 and on the 2-section graph\n",
    "warnings.filterwarnings('ignore') ## avoid warnings in disconnected case\n",
    "Nodes = ['A','B','C','D','E','F','G']\n",
    "L = []\n",
    "for i in range(len(Nodes)-1):\n",
    "    for j in np.arange(i+1,len(Nodes)):\n",
    "        L.append([Nodes[i],Nodes[j],G.distances(Nodes[i],Nodes[j])[0][0],\n",
    "                  H.distance(Nodes[i],Nodes[j]),H.distance(Nodes[i],Nodes[j],s=2)])\n",
    "df = pd.DataFrame(L, columns=['node1','node2','2-section','s=1','s=2'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## s=2 components\n",
    "Edges = [cc for cc in H.s_connected_components(s=2, return_singletons=True)]\n",
    "Nodes = [cc for cc in H.s_connected_components(s=2, edges=False, return_singletons=True)]\n",
    "print('s=2, connected components for the nodes:',Nodes)\n",
    "print('s=2, connected components for the edges:',Edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line graph\n",
    "\n",
    "Below we illustrate the **line graph** for the toy hypergraph and its dual, with $s=2$.\n",
    "\n",
    "Recall that in a line graph, the nodes are the edges in the original hypergraph, \n",
    "and an edge is draw between those if they share at least $s$ nodes in the original hypergraph.\n",
    "\n",
    "We see the same connectd components as listed above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## linegraph\n",
    "LG = ig.Graph.from_networkx(H.get_linegraph(s=2))\n",
    "ig.plot(LG, bbox=(200,200),vertex_label=LG.vs['_nx_name'], vertex_label_size=9, vertex_color='lightgrey')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dual's linegraph\n",
    "DLG = ig.Graph.from_networkx(H.dual().get_linegraph(s=2))\n",
    "ig.plot(DLG, bbox=(200,200),vertex_label=DLG.vs['_nx_name'], vertex_label_size=9, vertex_color='lightgrey')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Centrality measures\n",
    "\n",
    "For $H=(V,E)$, we define the **$s$-harmonic centrality** for edge $e_i \\in E$ as:\n",
    "$\\frac{1}{|E|-1}\\sum_{e_j \\in E_s; e_i \\ne e_j} \\frac{1}{d_s(e_i,e_j)}$.\n",
    "Recall that for $s$-disconnected edges $e_i, e_j$, we set $\\frac{1}{d_s(e_i,e_j)} = 0$.\n",
    "\n",
    "* n.b.: The HyperNetX implementation uses a different normalization, namely $(|E|-1)(|E|-2)/2$.\n",
    "\n",
    "For nodes, the definition is identical using the dual hypergraph.\n",
    "For our toy example, with $s=2$, nodes {A,B,C} form a connected connected component as we saw earlier, same\n",
    "for nodes {D,E,F}, while node G is an isolated node.\n",
    "\n",
    "Looking at the table of distances we computed earlier, we see that $d_2(A,B)=d_2(A,C)=1$ and $d_2(B,C)$=2,\n",
    "so before normalization, the harmonic centrality for A is 2, and for B and C it is 1.5.\n",
    "Results are comparable for the other connected component, with values of 1.5 for nodes D and E, and 2 for node F.\n",
    "Node G is isolated and thus has zero harmonic centrality.\n",
    "\n",
    "We can also define $s$-**betweenness centrality** as we did for graphs, namely for edge $e_i \\in E$:\n",
    "\n",
    "$\\frac{1}{(|E|-1)(|E|-2)}\\sum_{e_j \\in E-\\{e_i\\}} \\sum_{e_k \\in E-\\{e_i, e_j\\}} \\frac{\\ell(e_j,e_k,e_i)}{\\ell(e_j,e_k)}$\n",
    "\n",
    "where: $\\ell(e_j,e_k)$ is the number of shortest $s$-paths between $e_j$ and $e_k$, \n",
    "and $\\ell(e_j,e_k,e_i)$ is the number of shortest $s$-paths between $e_j$ and $e_k$ that include $e_i$.\n",
    "Again the definition is the same for nodes using the dual hypergraph.\n",
    "\n",
    "For our toy example, with $s=2$, the only nodes that are on shortest $s$-paths between other nodes are nodes A (between B and C)\n",
    "and node F (between D and E), thus the results we see below.\n",
    "\n",
    "Other distance-based centrality measures can be defined for hypergraphs in the same way, using $s$-distances,\n",
    "including the measures we covered in Section 3.3. \n",
    "In the example below, we also show **closeness centrality**; note that by default, the computation is done separately for each $s$-connected component, thus the results below.\n",
    "\n",
    "Computing **eccentricity** (the length of the longest shortest path from a vertex to every other vertex in\n",
    "the s-linegraph) with $s=2$ returns an error since some node are not connected, so we show the results for $s=1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## eccentricity - this yields an error with s > 1\n",
    "hnx.algorithms.s_eccentricity(H, edges=False, s=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## centralities for 's=2'\n",
    "s = 2\n",
    "\n",
    "hc = hnx.algorithms.s_harmonic_centrality(H, edges=False, s=s, normalized=False)\n",
    "bc = hnx.algorithms.s_betweenness_centrality(H, edges=False, s=s, normalized=False)\n",
    "cc = hnx.algorithms.s_closeness_centrality(H, edges=False, s=s)\n",
    "\n",
    "## normalize w.r.t. definition in the book\n",
    "D = pd.DataFrame([[v,hc[v]/(H.nodes.dataframe.shape[0]-1),\n",
    "                   2*bc[v]/((H.nodes.dataframe.shape[0]-1)*(H.nodes.dataframe.shape[0]-2)),\n",
    "                   cc[v]] for v in H.nodes], \n",
    "                   columns=['node','harmonic','betweenness','closeness'])\n",
    "\n",
    "#print(D.sort_values('harmonic', ascending=False).to_latex())\n",
    "D.sort_values('harmonic', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hypergraph modularity (qH) and clustering\n",
    "\n",
    "We compute qH on the toy graph for 4 different partitions, and using different variations for the edge contribution (a.k.a. $\\tau$-modularity).\n",
    "\n",
    "For edges of size $d$ where $c$ is the number of nodes from the part with the most representatives, we consider  variations as follows for edge contribution:\n",
    "\n",
    "* **strict**: edges are considered only if all nodes are from the same part, with unit weight, i.e. $w$ = 1 iff $c == d$ (0 else).\n",
    "* **cubic**: edges are counted only if more that half the nodes are from the same part, with weights proportional to the cube of the number of nodes in the majority, i.e. $w = (c/d)^3$ iff $c>d/2$ (0 else).\n",
    "* **quadratic**: edges are counted only if more that half the nodes are from the same part, with weights proportional to the square of the number of nodes in the majority, i.e. $w = (c/d)^2$ iff $c>d/2$ (0 else).\n",
    "* **linear**: edges are counted only if more that half the nodes are from the same part, with weights proportional to the number of nodes in the majority, i.e. $w = c/d$ iff $c>d/2$ (0 else).\n",
    "* **majority**: edges are counted only if more that half the nodes are from the same part, with unit weights, i.e. $w$ = 1 iff $c>d/2$ (0 else).\n",
    "\n",
    "Some of the above are supplied with the `hmod` module, the **qH2** and **qH3** functions are examples of user-supplied choice.\n",
    "\n",
    "The order above goes from only counting \"pure\" edges as community edges, gradually giving more weight to edges with $c>d/2$, all the way to giving the the same weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## these will be included in the next version of hmod\n",
    "## square modularity weights\n",
    "def qH2(d,c):\n",
    "    return (c/d)**2 if c > d/2 else 0\n",
    "## cubic modularity weights\n",
    "def qH3(d,c):\n",
    "    return (c/d)**3 if c > d/2 else 0\n",
    "\n",
    "## compute hypergraph modularity (qH) for the following partitions:\n",
    "A1 = [{'A','B','C','G'},{'D','E','F'}]            ## good clustering, qH should be positive\n",
    "A2 = [{'B','C'},{'A','D','E','F','G'}]            ## not so good\n",
    "A3 = [{'A','B','C','D','E','F','G'}]              ## this should yield qH == 0\n",
    "A4 = [{'A'},{'B'},{'C'},{'D'},{'E'},{'F'},{'G'}]  ## qH should be negative here\n",
    "\n",
    "## we compute with different choices of functions for the edge contribution\n",
    "\n",
    "print('strict edge contribution:')\n",
    "print('qH(A1):',\"{:.4f}\".format(hmod.modularity(H,A1,hmod.strict)),\n",
    "      'qH(A2):',\"{:.4f}\".format(hmod.modularity(H,A2,hmod.strict)),\n",
    "      'qH(A3):',\"{:.4f}\".format(hmod.modularity(H,A3,hmod.strict)),\n",
    "      'qH(A4):',\"{:.4f}\".format(hmod.modularity(H,A4,hmod.strict)))\n",
    "print('\\ncubic edge contribution:')\n",
    "print('qH(A1):',\"{:.4f}\".format(hmod.modularity(H,A1,qH3)),\n",
    "      'qH(A2):',\"{:.4f}\".format(hmod.modularity(H,A2,qH3)),\n",
    "      'qH(A3):',\"{:.4f}\".format(hmod.modularity(H,A3,qH3)),\n",
    "      'qH(A4):',\"{:.4f}\".format(hmod.modularity(H,A4,qH3)))\n",
    "print('\\nquadratic edge contribution:')\n",
    "print('qH(A1):',\"{:.4f}\".format(hmod.modularity(H,A1,qH2)),\n",
    "      'qH(A2):',\"{:.4f}\".format(hmod.modularity(H,A2,qH2)),\n",
    "      'qH(A3):',\"{:.4f}\".format(hmod.modularity(H,A3,qH2)),\n",
    "      'qH(A4):',\"{:.4f}\".format(hmod.modularity(H,A4,qH2)))\n",
    "print('\\nlinear edge contribution:')\n",
    "print('qH(A1):',\"{:.4f}\".format(hmod.modularity(H,A1,hmod.linear)),\n",
    "      'qH(A2):',\"{:.4f}\".format(hmod.modularity(H,A2,hmod.linear)),\n",
    "      'qH(A3):',\"{:.4f}\".format(hmod.modularity(H,A3,hmod.linear)),\n",
    "      'qH(A4):',\"{:.4f}\".format(hmod.modularity(H,A4,hmod.linear)))\n",
    "print('\\nmajority edge contribution:')\n",
    "print('qH(A1):',\"{:.4f}\".format(hmod.modularity(H,A1,hmod.majority)),\n",
    "      'qH(A2):',\"{:.4f}\".format(hmod.modularity(H,A2,hmod.majority)),\n",
    "      'qH(A3):',\"{:.4f}\".format(hmod.modularity(H,A3,hmod.majority)),\n",
    "      'qH(A4):',\"{:.4f}\".format(hmod.modularity(H,A4,hmod.majority)));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighted 2-section graph\n",
    "\n",
    "We already built the 2-section weighted graph **G** for the above toy hypergraph.\n",
    "\n",
    "Here we run Leiden custering algorithm on this graph, and compare with Kumar's hypergraph clustering algorithm.\n",
    "\n",
    "We run each algorithm multiple times to show the difference in performance. In general, hypergraph-based algorithms are much slower than graph-based algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2-section graph\n",
    "G.vs['label'] = G.vs['name']\n",
    "ig.plot(G, bbox=(0,0,250,250), edge_width = 2*np.array(G.es['weight']), \n",
    "        vertex_color='gainsboro', vertex_label_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## 2-section clustering with Leiden\n",
    "for i in range(100):\n",
    "    G.vs['community'] = G.community_leiden(objective_function='modularity', weights='weight').membership\n",
    "print('clusters:',hmod.dict2part({v['name']:v['community'] for v in G.vs}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Kumar clustering\n",
    "for i in range(100):\n",
    "    cl = hmod.kumar(H)\n",
    "print('clusters:', cl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplicial ratio\n",
    "\n",
    "We use the same toy graph, but we remove the singleton edge {'B'}.\n",
    "\n",
    "First, we see a simplicial ratio slightly above 1, and we also see that the two simplicial pairs between 2-edges and 3-edges are more surprising that the two pairs between 2-edges and 4-edges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## toy example without the singleton edge\n",
    "vertices = [v for v in H.nodes()]\n",
    "edges = [{'A','B'},{'A','C'},{'A','B','C'},{'A','D','E','F'},{'D','F'},{'E','F'},{'G','B'}]\n",
    "\n",
    "## simplicial ratio\n",
    "random.seed(123)\n",
    "spl.get_simplicial_ratio(vertices, edges, samples=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## simplicial matrix\n",
    "random.seed(123)\n",
    "spl.get_simplicial_matrix(vertices, edges, samples=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of simplicial pairs\n",
    "spl.get_simplicial_pairs(vertices, edges, as_matrix=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other simpliciality measures\n",
    "\n",
    "* no 3+ edge has downward closure, so the fraction is 0\n",
    "* edit simpliciality is 7/16, since 9 edges would need to be added to get downward closures\n",
    "* face edit simpliciality: the two values for maximal edges are 3/4 and 3/11 (keeping the maximal face in the counts) or 2/3 and 2/10 otherwise\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Simplicial fraction:',spl.get_simplicial_fraction(vertices, edges))\n",
    "print('Edit simpliciality:',spl.get_edit_simpliciality(vertices, edges))\n",
    "print('Face edit simpliciality:',spl.get_face_edit_simpliciality(vertices, edges, exclude_self=False))\n",
    "print('Face edit simpliciality:',spl.get_face_edit_simpliciality(vertices, edges, exclude_self=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# h-ABCD Examples\n",
    "\n",
    "Julia code to generate h-ABCD benchmarks ca be found here:\n",
    "https://github.com/bkamins/ABCDHypergraphGenerator.jl\n",
    "\n",
    "The first small h-ABCD hypergraph we use next was generated as follows:\n",
    "\n",
    "`julia --project abcdh.jl -n 100 -d 2.5,3,10 -c 1.5,30,40 -x .2 -q 0,.3,.4,.3 -w :strict -s 123 -o toy_100`\n",
    "\n",
    "It has 100 nodes and 3 well-defined communities. We will use this example mainly for visualization.\n",
    "\n",
    "The second one, which is much more noisy, was generated as follows:\n",
    "\n",
    "`julia --project abcdh.jl -n 300 -d 2.5,5,30 -c 1.5,80,120 -x .6 -q 0,0,.1,.9 -w :strict -s 123 -o toy_300`\n",
    "\n",
    "We will use this example to show that optimizing the appropriate hypergraph modularity function can lead to better clustering in some cases.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100-node h-ABCD - visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the edges and build the h-ABCD hypergraph H\n",
    "fp = open(datadir+'ABCD/toy_100_he.txt', 'r')\n",
    "\n",
    "Lines = fp.readlines()\n",
    "Edges = []\n",
    "for line in Lines:\n",
    "    #Edges.append(set([int(x)-1 for x in line.strip().split(',')]))\n",
    "    Edges.append(set([x for x in line.strip().split(',')]))\n",
    "H = hnx.Hypergraph(dict(enumerate(Edges)))\n",
    "print('distribution of edge sizes:',Counter([len(x) for x in Edges]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the ground-truth communities and assign node colors accordingly\n",
    "H_comm = {str(k+1):v for k,v in enumerate(pd.read_csv(datadir+'ABCD/toy_100_assign.txt', header=None)[0].tolist())}\n",
    "cls = ['white','darkgrey','black']\n",
    "node_colors = dict(zip(H.nodes, [cls[H_comm[i]-1] for i in H.nodes]))\n",
    "\n",
    "## build the 2-section graph and plot (with ground-truth community colors)\n",
    "g = hmod.two_section(H)\n",
    "for v in g.vs:\n",
    "    v['color'] = node_colors[v['name']]\n",
    "    v['gt'] = H_comm[v['name']]\n",
    "    \n",
    "random.seed(12345)\n",
    "ly = g.layout_fruchterman_reingold()\n",
    "g.vs['ly'] = [x for x in ly]\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ig.plot(g, target=ax, vertex_size=9, layout=ly, edge_color='darkgrey', edge_width=1)\n",
    "#fig.savefig('habcd_1.pdf');\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rubber band plot\n",
    "H_ly = dict(zip(g.vs['name'], [[x[0],x[1]] for x in g.vs['ly']]))\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "hnx.draw(H, with_node_labels=False, with_edge_labels=False, node_radius=.67,\n",
    "         nodes_kwargs={'facecolors': node_colors, 'edgecolors' : 'black'},\n",
    "         edges_kwargs={'edgecolors': 'darkgrey'},\n",
    "         pos=H_ly)\n",
    "#fig.savefig('habcd_2.pdf');\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot via convex hull with the XGI package\n",
    "H_nc = dict(zip(g.vs['name'], g.vs['color']))\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "XH = xgi.Hypergraph(Edges)\n",
    "xgi.draw(XH, node_fc=H_nc, dyad_color='grey', hull=True, radius=.15, edge_fc_cmap='Greys_r', alpha=.2, pos=H_ly, node_size=8, ax=ax, node_labels=False )\n",
    "#fig.savefig('habcd_3.pdf');\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge composition\n",
    "\n",
    "Recall we call a $d$-edge a **community** edge if $c>d/2$ where $c$ is the number of nodes that belong to the **most represented** community.\n",
    "\n",
    "Below we show the number of edges with all values $d$ and $c$, community edges or not.\n",
    "We see that given the ground-truth communities, most community edges are *pure* in the sense that $c=d$.\n",
    "\n",
    "In real examples, we usually do not know the ground-truth communities, or at least not for every node.\n",
    "We can try some clustering, for example graph clustering on the 2-section graph, or Kumar's algorithm on the hypergraph, to get a sense of edge composition.\n",
    "\n",
    "The result is quite similar to the ground-truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## edge composition - ground truth\n",
    "L = []\n",
    "for e in H.edges:\n",
    "    L.append((Counter([H_comm[i] for i in H.edges[e]]).most_common(1)[0][1],len(H.edges[e])))\n",
    "X = Counter(L).most_common()\n",
    "\n",
    "L = []\n",
    "for x in X:\n",
    "    L.append([x[0][1], x[0][0], x[0][0]>x[0][1]/2, x[1]])\n",
    "D = pd.DataFrame(np.array(L), columns=['d','c','community edge','frequency (ground truth)'])\n",
    "D = D.sort_values(by=['d','c'], ignore_index=True)\n",
    "\n",
    "## edge composition - Leiden on 2-section\n",
    "g.vs['leiden'] = g.community_leiden(objective_function='modularity', weights='weight').membership\n",
    "leiden = dict(zip(g.vs['name'],g.vs['leiden']))\n",
    "L = []\n",
    "for e in H.edges:\n",
    "    L.append((Counter([leiden[i] for i in H.edges[e]]).most_common(1)[0][1],len(H.edges[e])))\n",
    "X = Counter(L).most_common()\n",
    "L = []\n",
    "for x in X:\n",
    "    L.append([x[0][1], x[0][0], x[0][0]>x[0][1]/2, x[1]])\n",
    "D2 = pd.DataFrame(np.array(L), columns=['d','c','community edge','frequency (Leiden)'])\n",
    "D2 = D2.sort_values(by=['d','c'], ignore_index=True)\n",
    "\n",
    "D['frequency (Leiden)'] = D2['frequency (Leiden)']\n",
    "D = D.sort_values('frequency (ground truth)', ascending=False)\n",
    "#print(D[['d','c','frequency (ground truth)','frequency (Leiden)']].to_latex(index=False))\n",
    "D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simpliciality\n",
    "\n",
    "We show some measures of simpliciality, namely the number of simplicial pairs, the simpliciality matrix and the simplicial ratio measure.\n",
    "\n",
    "The simplicial ratio value is around 1.3 (recall it is based on sampling), which indicates that this hypergraph does not exhibit high simpliciality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = [set(H.edges[e]) for e in H.edges]\n",
    "V = list(set([x for y in E for x in y]))\n",
    "spl.get_simplicial_pairs(V, E, as_matrix=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl.get_simplicial_matrix(V, E, samples=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl.get_simplicial_ratio(V, E, samples=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## other measures of simpliciality\n",
    "print('Simplicial fraction:',spl.get_simplicial_fraction(V,E))\n",
    "print('Edit simpliciality:',spl.get_edit_simpliciality(V,E))\n",
    "print('Face edit simpliciality:',spl.get_face_edit_simpliciality(V,E,exclude_self=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 300-node noisy h-ABCD\n",
    "\n",
    "This is a noisier hypergraph with $\\xi=0.6$, edges mostly of size 4 and some edges of size 3.\n",
    "\n",
    "In the experiment below, we run each of the following algorithms 30 times and compare AMI with the ground-truth communities.\n",
    "* Leiden on 2-section (weighted) graph\n",
    "* Kumar's algorithm\n",
    "* h-Louvain\n",
    "\n",
    "We observe that Kumar's algorithm, which does take the hypergraph structure into account, slightly improves on the results with 2-section clustering, \n",
    "while h-Louvain improves it further, albeit with slower run time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the edges and build the h-ABCD hypergraph H\n",
    "fp = open(datadir+'ABCD/toy_300_he.txt', 'r')\n",
    "Lines = fp.readlines()\n",
    "Edges = []\n",
    "for line in Lines:\n",
    "    Edges.append(set([x for x in line.strip().split(',')]))\n",
    "H = hnx.Hypergraph(dict(enumerate(Edges)))\n",
    "\n",
    "## read the ground-truth communities and assign node colors accordingly\n",
    "H_comm = {str(k+1):v for k,v in enumerate(pd.read_csv(datadir+'ABCD/toy_300_assign.txt', header=None)[0].tolist())}\n",
    "\n",
    "## build the 2-section graph\n",
    "g = hmod.two_section(H)\n",
    "for v in g.vs:\n",
    "    v['gt'] = H_comm[v['name']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## reduce the number of repeats (REP) for a faster run (we used REP=30 for the book)\n",
    "REP = 30\n",
    "L = []\n",
    "random.seed(321)\n",
    "np.random.seed(321) \n",
    "\n",
    "for s in range(REP):\n",
    "    g.vs['leiden'] = g.community_leiden(objective_function='modularity',weights='weight').membership\n",
    "    ami_g = AMI(g.vs['gt'], g.vs['leiden'])\n",
    "    H_kumar = hmod.kumar(H)\n",
    "    H_kumar_dict = hmod.part2dict(H_kumar)\n",
    "    ami_k = AMI([H_comm[v] for v in H.nodes], [H_kumar_dict[v] for v in H.nodes])\n",
    "    #H_ls = hmod.part2dict(hmod.last_step(H, H_kumar, hmod.strict))\n",
    "    #ami_ls = AMI([H_comm[v] for v in H.nodes], [H_ls[v] for v in H.nodes])\n",
    "    L.append([ami_g, ami_k])\n",
    "        \n",
    "D = pd.DataFrame(L, columns=['2-section', 'Kumar'])\n",
    "print('mean values:')\n",
    "print(D.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running h-Louvain with Bayesian Optimization\n",
    "\n",
    "This is slower as for each repetition, several attempts are made to find a good set of parameters using Bayesian optimization.\n",
    "Results are saved and can be retieved for plotting. To re-run the experiment, uncomment the cell below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "REP = 30\n",
    "L = []\n",
    "for rep in range(REP):\n",
    "    hBO = hl.hLouvainBO(H,hmod_tau=\"infinity\",random_seed=rep)\n",
    "    hBO.set_params(show_bo_table=False)\n",
    "    result_df = hBO.hLouvain_perform_BO()\n",
    "    dct = hmod.part2dict(result_df['A_lstep'].tolist()[0])\n",
    "    ami = AMI([H_comm[v] for v in H.nodes], [dct[v] for v in H.nodes])\n",
    "    L.append(ami)\n",
    "\n",
    "#with open(datadir+'ABCD/toy_300_h-Louvain.pkl','wb') as fn:\n",
    "#    pickle.dump(L,fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(datadir+'ABCD/toy_300_h-Louvain.pkl','rb') as fn:\n",
    "    L = pickle.load(fn)\n",
    "D['h-Louvain'] = L[:D.shape[0]]\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.boxplot(D, width=.5, color='darkgray', linewidth=1.2)\n",
    "plt.ylabel('AMI', fontsize=14)\n",
    "#plt.savefig('habcd_cluster.eps')   \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## no simplicial pair in this case\n",
    "E = [set(H.edges[e]) for e in H.edges]\n",
    "V = list(set([x for y in E for x in y]))\n",
    "spl.get_simplicial_pairs(V, E, as_matrix=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## other measures\n",
    "print('Simplicial fraction:',spl.get_simplicial_fraction(V,E))\n",
    "print('Edit simpliciality:',spl.get_edit_simpliciality(V,E))\n",
    "print('Face edit simpliciality:',spl.get_face_edit_simpliciality(V,E,exclude_self=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "We fit two embeddings to the h-ABCD graph, namely:\n",
    "* 2-section node2vec\n",
    "* bipartite node2vec (where we ignore the edge embeddings)\n",
    "\n",
    "We fit a classifier where we train on 50% of the points, and test on the rest,\n",
    "after reducing to 16-dim via UMAP.\n",
    "\n",
    "We verify if keeping the hypergraph structure helps, as we do with the bipartite representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2-section\n",
    "graph = n2v.Graph(g.to_tuple_list(), directed=False, weighted=False)\n",
    "nv = n2v.Node2Vec(graph, dim=32, p=1, q=1, walk_length=80, window=5, seed=123)\n",
    "nv.train(epochs=10, verbose=False)\n",
    "X_twosec = np.array([nv.wv[i] for i in range(len(nv.wv))])\n",
    "\n",
    "## 2-section - 2-d visualization\n",
    "U = umap.UMAP().fit_transform(X_twosec)\n",
    "df = pd.DataFrame(U, columns=['X','Y'])\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(df.X, df.Y, c=g.vs['gt'], s=25)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bipartite (edges are in first positions; we ignore the edges)\n",
    "G = ig.Graph.from_networkx(H.bipartite())\n",
    "graph = n2v.Graph(G.to_tuple_list(), directed=False, weighted=False)\n",
    "nv = n2v.Node2Vec(graph, dim=32, p=1, q=1, walk_length=80, window=5, seed=123)\n",
    "nv.train(epochs=10, verbose=False)\n",
    "n_edges = len([e for e in H.edges()])\n",
    "X_bip = np.array([nv.wv[i] for i in range(len(nv.wv))])[n_edges:]\n",
    "\n",
    "## bipartite 2-d viz\n",
    "U = umap.UMAP().fit_transform(X_bip)\n",
    "df = pd.DataFrame(U,columns=['X','Y'])\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(df.X, df.Y, c=g.vs['gt'], s=25)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit a classifier\n",
    "\n",
    "We train on half the data chosen at random, which we repeat several times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## classifier - with 2-section and bipartite embeddings\n",
    "acc = []\n",
    "acc_b = []\n",
    "y = label = g.vs['gt']\n",
    "\n",
    "for seed in np.arange(0,51,10): ## we used 30 repeats in textbook which can take a few minutes\n",
    "    \n",
    "    ## 2-section\n",
    "    X = umap.UMAP(n_components=16, n_jobs=1, random_state=seed).fit_transform(X_twosec)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=seed)\n",
    "    model = RandomForestClassifier(n_estimators=100, bootstrap = True, max_features = 'sqrt', random_state=seed)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    # print(cm)\n",
    "    acc.append(sum(cm.diagonal())/sum(sum(cm)))\n",
    "\n",
    "    ## bipartite - same seed\n",
    "    X = umap.UMAP(n_components=16, n_jobs=1, random_state=seed).fit_transform(X_bip)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=seed)\n",
    "    model = RandomForestClassifier(n_estimators=100, bootstrap = True, max_features = 'sqrt', random_state=seed)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    # print(cm)\n",
    "    acc_b.append(sum(cm.diagonal())/sum(sum(cm)))\n",
    "    \n",
    "print(np.mean(acc), np.mean(acc_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare the results - we see slightly better results with the bipartite representation\n",
    "D = pd.DataFrame(np.array([acc,acc_b]).transpose(),columns=['2-section','bipartite'])\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.boxplot(D, width=.5, color='darkgray', linewidth=1.2);\n",
    "plt.grid()\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "#plt.savefig('habcd_classify.eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game of Thrones scenes hypergraph\n",
    "\n",
    "The original data can be found here: https://github.com/jeffreylancaster/game-of-thrones.\n",
    "\n",
    "A pre-processed version is provided, where we consider a hypergraph from the game of thrones scenes with he following elements:\n",
    "\n",
    "* **Nodes** are named characters in the series\n",
    "* **Hyperedges** are groups of character appearing in the same scene(s)\n",
    "* **Hyperedge weights** are total scene(s) duration in seconds involving each group of characters\n",
    "\n",
    "We kept hyperedges with at least 2 characters and we discarded characters with degree below 5.\n",
    "\n",
    "We saved the following:\n",
    "\n",
    "* *Edges*: list of sets where the nodes are 0-based integers represented as strings: '0', '1', ... 'n-1'\n",
    "* *Names*: dictionary; mapping of nodes to character names\n",
    "* *Weights*: list; hyperedge weights (in same order as Edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the data\n",
    "with open(datadir+\"GoT/GoT.pkl\",\"rb\") as f:\n",
    "    Edges, Names, Weights = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the weighted hypergraph \n",
    "\n",
    "Use the above to build the weighted hypergraph (GoT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nodes are represented as strings from '0' to 'n-1'\n",
    "GoT = hnx.Hypergraph(dict(enumerate(Edges)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add full names of characters and compute node strength (a.k.a. weighted degree)\n",
    "I, _node, _edge = GoT.incidence_matrix(index=True)\n",
    "S = I * [Weights[int(i)] for i in _edge]\n",
    "Strength = {i:j for i,j in zip(_node,S)}\n",
    "for v in GoT.nodes:\n",
    "    GoT.nodes[v].name = Names[v]\n",
    "    GoT.nodes[v].strength = Strength[v]\n",
    "for e in GoT.edges:\n",
    "    GoT.edges[e].weight = Weights[e]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on the GoT hypergraph\n",
    "\n",
    "Simple exploratory data analysis (EDA) on this hypergraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## edge sizes (number of characters per scene)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist([GoT.size(e) for e in GoT.edges], bins=25, color='grey')\n",
    "plt.xlabel(\"Edge size\", fontsize=14)\n",
    "#plt.savefig('got_hist_1.eps')\n",
    "plt.show()\n",
    "\n",
    "## max edge size\n",
    "print('max edge size:', np.max([GoT.size(e) for e in GoT.edges]))\n",
    "print('median edge size:', np.median([GoT.size(e) for e in GoT.edges]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## edge weights (total scene durations for each group of characters appearing together)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist([Weights], bins=25, color='grey')\n",
    "plt.xlabel(\"Edge weight\",fontsize=14)\n",
    "#plt.savefig('got_hist_2.eps')\n",
    "plt.show()\n",
    "\n",
    "## max/median edge weight\n",
    "print('max edge weight:', np.max([Weights]))\n",
    "print('median edge weight:', np.median([Weights]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## node degrees\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(hnx.degree_dist(GoT),bins=20, color='grey')\n",
    "plt.xlabel(\"Node degree\",fontsize=14)\n",
    "#plt.savefig('got_hist_3.eps')\n",
    "plt.show()\n",
    "\n",
    "## max degree\n",
    "print('max node degree:', np.max(hnx.degree_dist(GoT)))\n",
    "print('median node degree:', np.median(hnx.degree_dist(GoT)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## node strength (total scene appearance)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist([GoT.nodes[n].strength for n in GoT.nodes], bins=20, color='grey')\n",
    "plt.xlabel(\"Node strength\",fontsize=14)\n",
    "#plt.savefig('got_hist_4.eps')\n",
    "plt.show()\n",
    "\n",
    "## max strength\n",
    "print('max node strength:', np.max([GoT.nodes[n].strength for n in GoT.nodes]))\n",
    "print('median node strength:', np.median([GoT.nodes[n].strength for n in GoT.nodes]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build a dataframe with node characteristics\n",
    "df = pd.DataFrame()\n",
    "df['name'] = [GoT.nodes[v].name for v in GoT.nodes()]\n",
    "df['degree'] = [GoT.degree(v) for v in GoT.nodes()]\n",
    "df['strength'] = [GoT.nodes[v].strength for v in GoT.nodes()]\n",
    "df.sort_values(by='strength',ascending=False).head(12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Compute s-centrality and betweenness\n",
    "\n",
    "We consider $s=1$ and $s=2$ below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## with s=1\n",
    "bet = hnx.s_betweenness_centrality(GoT, edges=False)\n",
    "har = hnx.s_harmonic_centrality(GoT, edges=False, normalized=False)\n",
    "df['betweenness(s=1)'] = [bet[v] for v in GoT.nodes()]\n",
    "n = GoT.shape[0]\n",
    "df['harmonic(s=1)'] = [har[v]/(n-1) for v in GoT.nodes()]\n",
    "\n",
    "## with s=2\n",
    "bet = hnx.s_betweenness_centrality(GoT, edges=False, s=2)\n",
    "har = hnx.s_harmonic_centrality(GoT, edges=False, normalized=False, s=2)\n",
    "df['betweenness(s=2)'] = [bet[v] for v in GoT.nodes()]\n",
    "df['harmonic(s=2)'] = [har[v]/(n-1) for v in GoT.nodes()]\n",
    "\n",
    "#print(df.sort_values(by=['strength'],ascending=False).head(10)[['name','degree','strength','betweenness(s=1)','harmonic(s=1)']].to_latex(index=False, float_format=\"{:0.5f}\".format))\n",
    "df.sort_values(by=['strength'],ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build 2-section graph and compute a few centrality measures\n",
    "\n",
    "We saw several centrality measures for graphs in chapter 3. \n",
    "\n",
    "Below, we build the 2-section graph for GoT and compute a few of those. \n",
    "\n",
    "**n.b.: Unlike in the first edition of the book, we now ignore edge weights to compare with the hypergraph s-measures.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## build 2-section\n",
    "G = hmod.two_section(GoT)\n",
    "\n",
    "## betweenness\n",
    "n = G.vcount()\n",
    "b = G.betweenness(directed=False)\n",
    "G.vs['bet'] = [2*x/((n-1)*(n-2)) for x in b]\n",
    "for v in G.vs:\n",
    "    GoT.nodes[v['name']].bet = v['bet']\n",
    "df['betweenness'] = [GoT.nodes[v].bet for v in GoT.nodes()]\n",
    "\n",
    "## harmonic\n",
    "G.vs['hc'] = G.harmonic_centrality(normalized=True)\n",
    "for v in G.vs:\n",
    "    GoT.nodes[v['name']].hc = v['hc']\n",
    "df['harmonic'] = [GoT.nodes[v].hc for v in GoT.nodes()]\n",
    "\n",
    "## order w.r.t. harmonic\n",
    "df.sort_values(by='harmonic',ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## high correlation between centrality measures\n",
    "corr = df[['betweenness(s=1)','betweenness(s=2)','betweenness','harmonic(s=1)','harmonic(s=2)','harmonic']].corr()\n",
    "#print(corr[['harmonic','betweenness']].to_latex(index=True,  float_format=\"{:0.3f}\".format))\n",
    "print(corr[['harmonic','betweenness']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypergraph modularity and clustering\n",
    "\n",
    "We use $\\tau=3$ for the hypergraph ($\\tau$) modularity weights below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### visualize the 2-section graph\n",
    "print('nodes:',G.vcount(),'edges:',G.ecount())\n",
    "G.vs['size'] = 14\n",
    "G.vs['color'] = 'lightgrey'\n",
    "G.vs['label'] = [int(x) for x in G.vs['name']] ## use int(name) as label\n",
    "G.vs['character'] = [GoT.nodes[n].name for n in G.vs['name']]\n",
    "G.vs['label_size'] = 6\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "ly_fr = G.layout_fruchterman_reingold()\n",
    "ig.plot(G, layout=ly_fr, bbox=(0,0,600,400), edge_color='lightgrey')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we see a well-separated small clique; it is the Braavosi theater troup\n",
    "print([GoT.nodes[str(x)].name for x in np.arange(166,173)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Compute modularity (with qH3 function) on several random partition with K parts for a range of K's\n",
    "## This should be close to 0 and can be negative.\n",
    "h = []\n",
    "for K in np.arange(2,21,2):\n",
    "    for rep in range(1): ## 10 for the textbook\n",
    "        V = list(GoT.nodes)\n",
    "        np.random.seed(K*rep)\n",
    "        p = np.random.choice(K, size=len(V))\n",
    "        RandPart = hmod.dict2part({V[i]:p[i] for i in range(len(V))})\n",
    "        ## drop empty sets if any\n",
    "        RandPart = [x for x in RandPart if len(x)>0]\n",
    "        ## compute qH\n",
    "        h.append(hmod.modularity(GoT, RandPart, qH3))\n",
    "print('range for qH:',min(h),'to',max(h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "sns.boxplot(h, showfliers=False, width=.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-section graph clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Cluster the 2-section graph (with Leiden) and compute qH\n",
    "## We now see qH >> 0\n",
    "qH_best = -1\n",
    "for i in range(100):\n",
    "    G.vs['_leiden'] = G.community_leiden(objective_function='modularity', weights='weight', resolution=1.0).membership\n",
    "    ML = hmod.dict2part({v['name']:v['_leiden'] for v in G.vs})\n",
    "    qH = hmod.modularity(GoT, ML, qH3)\n",
    "    if qH > qH_best:\n",
    "        qH_best = qH\n",
    "        G.vs['leiden'] = G.vs['_leiden']\n",
    "print('qH:',\"{:.4f}\".format(qH_best))\n",
    "for v in G.vs:\n",
    "    GoT.nodes[v['name']].leiden = v['leiden']\n",
    "df['leiden_cluster'] = [GoT.nodes[v].leiden for v in GoT.nodes()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot 2-section w.r.t. the resulting clusters\n",
    "cl = G.vs['leiden']\n",
    "\n",
    "## pick greyscale or color plot:\n",
    "pal = ig.GradientPalette(\"white\",\"black\",max(cl)+2)\n",
    "pal = ig.ClusterColoringPalette(max(cl)+2)\n",
    "G.vs['color'] = [pal[x] for x in cl]\n",
    "\n",
    "## show labels or not\n",
    "G.vs['label_size'] = 0\n",
    "\n",
    "ig.plot(G, layout = ly_fr, bbox=(0,0,600,400), edge_color='gainsboro', vertex_size=8)\n",
    "#ig.plot(G, target='GoT_clusters.eps', layout = ly_fr, bbox=(0,0,600,400), edge_color='grey')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### edge composition after clustering\n",
    "\n",
    "We see that the most frequent edges are small \"pure\" edges, but there ar also several edges with all but one node from the same community.\n",
    "\n",
    "This suggests an intermediate value for $\\tau$, such as $\\tau$=2 or 3, for the exponent in the modularity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_dict = dict(zip(G.vs['name'],G.vs['leiden']))\n",
    "L = []\n",
    "for e in GoT.edges:\n",
    "    L.append(tuple(x[1] for x in Counter([comm_dict[i] for i in GoT.edges[e]]).most_common()))\n",
    "X = Counter(L).most_common()\n",
    "L = []\n",
    "for x in X:\n",
    "    L.append([len(x[0]), sum(x[0]), x[0][0], x[1], x[0][0]>sum(x[0])/2])\n",
    "df_cd = pd.DataFrame(np.array(L), columns=['n_comm','d','c','frequency','community edge'],)\n",
    "df_cd['cum_freq'] = df_cd.cumsum().frequency / GoT.shape[1]\n",
    "df_cd.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kumar's algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ku = hmod.kumar(GoT, verbose=False)\n",
    "print('qH:',\"{:.4f}\".format(hmod.modularity(GoT, Ku, qH3)))\n",
    "dct = hmod.part2dict(Ku)\n",
    "G.vs['kumar'] = [dct[i] for i in G.vs['name']]\n",
    "df['kumar'] = [dct[v] for v in G.vs['name']]\n",
    "print('AMI vs 2-section partitions:',AMI(G.vs['leiden'],G.vs['kumar']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h-Louvain\n",
    "\n",
    "Slower but often yields higher h-modularity than other algorithms\n",
    "\n",
    "Uncomment the code below to run - this can take a few minutes.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "hBO = hl.hLouvainBO(GoT, hmod_tau=3, random_seed=321)\n",
    "hBO.set_params(show_bo_table=True, init_points=3, n_iter=2) ## show results from each step of Bayesian optimization\n",
    "result_df = hBO.hLouvain_perform_BO()\n",
    "dct = hmod.part2dict(result_df['A_lstep'].tolist()[0])\n",
    "G.vs['h-louvain'] = [dct[i] for i in G.vs['name']]\n",
    "df['h_louvain'] = [dct[v] for v in G.vs['name']]\n",
    "print('qH:',\"{:.4f}\".format(hmod.modularity(GoT, result_df['A_lstep'].tolist()[0], qH3)))\n",
    "print('AMI with 2-section partitions:',AMI(G.vs['leiden'],G.vs['h-louvain']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at one of the lead characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ex: high strength nodes in same cluster with Daenerys Targaryen\n",
    "dt = df[df['name']=='Daenerys Targaryen']['leiden_cluster'].iloc[0]\n",
    "df[df['leiden_cluster']==dt].sort_values(by='strength',ascending=False).head(9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the simplicial ratio and other simpliciality measures\n",
    "\n",
    "We see a simpliciality ratio well above 1, suggesting more simplicial pairs than would happen at random.\n",
    "\n",
    "For the other measures, the simplicial fraction (0.07) and more so the edit simpliciality (7e-5) are small,\n",
    "which is to be expected as there are several large edges in this dataset.\n",
    "The face edit sompliciality is a bit higher at 0.26.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute the simplicial ratio measure\n",
    "E = [set(GoT.edges[e]) for e in GoT.edges]\n",
    "V = list(set([x for y in E for x in y]))\n",
    "\n",
    "## build list of edges incident to each node\n",
    "edge_dict = spl.get_edge_sets(V, E)\n",
    "\n",
    "## mapping between node index and character name\n",
    "node_dict = dict(zip([GoT.nodes[v].name for v in GoT.nodes], list(GoT.nodes)))\n",
    "\n",
    "## simplicial ratio\n",
    "spl.get_simplicial_ratio(V, E, samples=100)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "print('Simplicial fraction:',spl.get_simplicial_fraction(V,E))\n",
    "print('Edit simpliciality:',spl.get_edit_simpliciality(V,E))\n",
    "print('Face edit simpliciality:',spl.get_face_edit_simpliciality(V, E, exclude_self=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the individual simpliciality ratio for each GoT character\n",
    "\n",
    "We look at the ego-nets for some nodes high/low simpliciality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the individual simpliciality ratio for each character and rank\n",
    "sm = []\n",
    "np.random.seed(123)\n",
    "for name in df.name:\n",
    "    E = edge_dict[node_dict[name]]\n",
    "    V = list(set([x for y in E for x in y]))\n",
    "    sm.append(spl.get_simplicial_ratio(V, E, samples=100))\n",
    "df['simpliciality'] = sm\n",
    "df.sort_values(by='simpliciality', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick high/low simpliciality nodes with low degree for viz below\n",
    "hs = 'Bowen Marsh'\n",
    "ls = 'Ros'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## high simpliciality\n",
    "kwargs = {'layout_kwargs': {'seed': 123}, 'with_edge_labels':False, 'with_node_labels':False}\n",
    "edges_kwargs={'edgecolors':'grey'}\n",
    "SE = [e for e in edge_dict[node_dict[hs]]]\n",
    "HG = hnx.Hypergraph(SE)\n",
    "nc = ['grey']*len(list(HG.nodes))\n",
    "idx = np.where(np.array(HG.nodes)==node_dict[hs])[0][0]\n",
    "nc[idx] = 'black'\n",
    "nr = dict(zip(HG.nodes,[1]*len(list(HG.nodes))))\n",
    "nr[node_dict[hs]] = 2\n",
    "nodes_kwargs={'facecolors':nc}\n",
    "print('looking at node:',hs)\n",
    "plt.subplots(figsize=(7,7))\n",
    "hnx.draw(HG, **kwargs, edges_kwargs=edges_kwargs, nodes_kwargs=nodes_kwargs,  node_radius=nr)\n",
    "#plt.savefig('bowen.eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convex hull view\n",
    "XH = xgi.Hypergraph([list(HG.edges[e]) for e in HG.edges])\n",
    "xgi.draw(XH, node_fc='black', hull=True, node_size=[nr[i] for i in XH.nodes])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## low simpliciality\n",
    "kwargs = {'layout_kwargs': {'seed': 123}, 'with_edge_labels':False, 'with_node_labels':False}\n",
    "edges_kwargs={'edgecolors':'grey'}\n",
    "SE = [e for e in edge_dict[node_dict[ls]]]\n",
    "HG = hnx.Hypergraph(SE)\n",
    "nc = ['grey']*len(list(HG.nodes))\n",
    "idx = np.where(np.array(HG.nodes)==node_dict[ls])[0][0]\n",
    "nc[idx] = 'black'\n",
    "nr = dict(zip(HG.nodes,[1]*len(list(HG.nodes))))\n",
    "nr[node_dict[ls]] = 2\n",
    "nodes_kwargs={'facecolors':nc}\n",
    "print('looking at node:',ls)\n",
    "plt.subplots(figsize=(7,7))\n",
    "hnx.draw(HG, **kwargs, edges_kwargs=edges_kwargs, nodes_kwargs=nodes_kwargs, node_radius=nr)\n",
    "#plt.savefig('ros.eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##convex hull view\n",
    "XH = xgi.Hypergraph([list(HG.edges[e]) for e in HG.edges])\n",
    "xgi.draw(XH, node_fc='black', hull=True, node_size=[nr[i] for i in XH.nodes]);\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3-d view per edge size\n",
    "_, ax = plt.subplots(figsize=(10, 10), subplot_kw={\"projection\": \"3d\"})\n",
    "xgi.draw_multilayer(XH, ax=ax, node_fc='black',hull=True, node_size=[nr[i] for i in XH.nodes], sep=1, h_angle=25)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  degree - size correlation\n",
    "\n",
    "We see positive, but very small correlation in this case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x, _y, corr = h_deg_size_corr(GoT)\n",
    "print('correlation:',corr)\n",
    "_df = pd.DataFrame(np.array([_x, _y]).T, columns=['degree','edge size'])\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.boxplot(data=_df, x='edge size', y='degree', showfliers=False, width=.5, color='lightblue')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## grouping node sizes in 3 tiers: up to 8, 9-16 and 17+\n",
    "_df['edge size range'] = [(x-1)//8 for x in _df['edge size']]\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.boxplot(data=_df, x='edge size range', y='degree', showfliers=False, width=.5, color='lightblue')\n",
    "plt.xticks([0,1,2],['2-8','9-16','17-24'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rich club coefficients - via sampling for computing the denominator\n",
    "\n",
    "* first, compute number of edges with all nodes having degree >= k for each k: $\\phi(k)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## degrees in GoT graph\n",
    "threshold = np.quantile([GoT.degree(v) for v in GoT.nodes],.95)\n",
    "d = np.sort(list(set([GoT.degree(v) for v in GoT.nodes if GoT.degree(v)<threshold])))\n",
    "L = []\n",
    "for e in GoT.edges:\n",
    "    L.append(np.min([GoT.degree(v) for v in GoT.edges[e]]))\n",
    "## compute phi's\n",
    "phi = []\n",
    "L = np.array(L)\n",
    "for k in d:\n",
    "    phi.append(sum(L>=k))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* now generate random bipartites graphs and compute all $\\hat{\\phi}_k$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of repeats\n",
    "REP = 100\n",
    "\n",
    "## repeat each node w.r.t. its degree\n",
    "V = []\n",
    "for v in GoT.nodes:\n",
    "    V.extend(list(np.repeat(v,GoT.degree(v))))\n",
    "\n",
    "## edge sizes\n",
    "S = [len(GoT.edges[e]) for e in GoT.edges()]\n",
    "\n",
    "## initialize\n",
    "np.random.seed(321)\n",
    "phi_hat = np.zeros(len(phi))\n",
    "\n",
    "for rep in range(REP):\n",
    "    ## randomize   \n",
    "    V = np.random.permutation(V)\n",
    "    ## generate the edges\n",
    "    ctr = 0\n",
    "    E = []\n",
    "    for s in S:\n",
    "        E.append(list(V[ctr:(ctr+s)]))\n",
    "        ctr += s\n",
    "    ## min degree seen for each edge\n",
    "    L = []\n",
    "    for e in E:\n",
    "        L.append(np.min([GoT.degree(v) for v in e]))\n",
    "    L = np.array(L)\n",
    "    ## compute one instance of phi_hat and add to the sum\n",
    "    ph = []\n",
    "    for k in d:\n",
    "        ph.append(sum(L>=k))   \n",
    "    phi_hat = phi_hat + np.array(ph)\n",
    "\n",
    "## average the final phi_hat vector\n",
    "phi_hat = phi_hat / REP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## no strong rich-club phenomenon here\n",
    "plt.figure(figsize=(6,6))\n",
    "rc = [a/b for a,b in zip(phi,phi_hat)]\n",
    "plt.semilogx(d, rc, \".\", c=\"black\")\n",
    "plt.xlabel(r\"degree $\\ell$\", fontsize=12)\n",
    "plt.ylabel(r\"rich club coefficient $\\rho(\\ell)$\")\n",
    "# plt.savefig('rich_club_got.eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (k,t)-hypercoreness\n",
    "\n",
    "Maximal generalized subhypergraph where nodes have degree $k$ or more, and each edge contains at least proportion t of its original nodes.\n",
    "\n",
    "We compute the size of this maximal hypercore for values of $5 \\le k \\le 50$ and $.6 \\le t \\le 1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From paper - faster\n",
    "def hypercore(HG, k, t=1, verbatim=False):\n",
    "    E = [set(HG.edges[e]) for e in HG.edges]\n",
    "    D = [max(2,t*len(e)) for e in E]\n",
    "    V = set([v for v in HG.nodes()])\n",
    "    deg = Counter([v for e in E for v in e])\n",
    "    if verbatim:\n",
    "        print(len(V))\n",
    "    R = set([v for v in V if deg[v] < k])\n",
    "    while len(R)>0:\n",
    "        Rp = set()\n",
    "        for i in range(len(E)):\n",
    "            e = E[i]\n",
    "            if len(e)>0:\n",
    "                if len(R.intersection(e))>0:\n",
    "                    E[i] = E[i].difference(R)\n",
    "                    deg = Counter([v for e in E for v in e])\n",
    "                    if (len(E[i])<D[i]):\n",
    "                        a = set([v for v in E[i] if deg[v]==k])\n",
    "                        Rp = Rp.union(a)\n",
    "                        E[i] = set()\n",
    "\n",
    "        V = V.difference(R)\n",
    "        R = Rp\n",
    "        if verbatim:\n",
    "            print(len(V))\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# T = [.6,.7,.8,.9,1] ## un-comment to try severar values for t\n",
    "T = [.9]\n",
    "## compute for range of values for k and t and store\n",
    "L = []\n",
    "for k in np.arange(5,51):\n",
    "    for t in T:\n",
    "        L.append([k,t, len(hypercore(GoT,k,t))])\n",
    "D = pd.DataFrame(L, columns=['k','t','Size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the resulting values\n",
    "plt.figure(figsize=(6,6))\n",
    "for t in T:\n",
    "    plt.plot(D[D.t==t].k, D[D.t==t].Size, '.-', label=t)\n",
    "plt.xlabel('value of k', fontsize=14)\n",
    "plt.ylabel('(k,t)-hypercore size', fontsize=14)\n",
    "plt.legend(title='value of t', fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### looking at a specific (k,t)-hypercore\n",
    "\n",
    "$k=18$ and $t=0.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## map to 2-section and visualize\n",
    "V = hypercore(GoT,18,.9, verbatim=False)\n",
    "E = [e for e in GoT.edges if len(V.intersection(set(GoT.edges[e]))) / len(GoT.edges[e]) >= .9]\n",
    "H = GoT.restrict_to_edges(E)\n",
    "H = H.restrict_to_nodes(V) ## ADDED\n",
    "G = hmod.two_section(H)\n",
    "G.vs['size'] = 0\n",
    "G.vs['color'] = 'white'\n",
    "#G.vs['label'] = G.vs['name']a\n",
    "G.vs['label'] = [GoT.nodes[n].name for n in G.vs['name']]\n",
    "G.vs['label_size'] = 12\n",
    "random.seed(321)\n",
    "G.vs['layout'] = G.layout_fruchterman_reingold()\n",
    "ig.plot(G, layout=G.vs['layout'], bbox=(500,500), margin=50, edge_color='lightgrey')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## same hypergraph, different view with XGI\n",
    "pos = dict(zip(G.vs['label'],[[v[0],-v[1]] for v in G.vs['layout']]))\n",
    "E = []\n",
    "for e in H.edges:\n",
    "    E.append([Names[x] for x in H.edges[e]])\n",
    "XH = xgi.Hypergraph(E)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "xgi.draw(XH, pos=pos, dyad_color='grey', hull=True, radius=.25, edge_fc_cmap='Greys_r', alpha=.005, node_size=0, ax=ax, node_labels=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contact hypergraphs\n",
    "\n",
    "We consider two datasets where hyperedges are built when individuals come into close physical contact over some time ingtervals. The datasets are available from the XGI package directly, see: https://xgi.readthedocs.io/en/stable/xgi-data.html.\n",
    "For both datasets, we keep a single instance for every edge. \n",
    "The data is in directory ```../Datasets/Contacts```.\n",
    "Some questions at the end of Chapter 7 refer to those datasets.\n",
    "\n",
    "### Primary school dataset\n",
    "\n",
    "* 12,704 hyperedges of size 2 to 5 built from 242 nodes.\n",
    "* the nodes are children belonging to one of 10 classes, and the teachers \n",
    "* file ```hyperedges-contact-primary.txt``` contains the edges (1 per line, csv), the nodes are 1-based\n",
    "* file ```labels-contact-primary.txt``` contains the node labels, 1 to 11 (in numerical order of the nodes)\n",
    "\n",
    "References in: https://zenodo.org/records/10155810\n",
    "\n",
    "\n",
    "### High school dataset\n",
    "\n",
    "* 7,818 hyperedges of size 2 to 5 built from 327 nodes.\n",
    "* the nodes are students belonging to one of 9 classes\n",
    "* file ```hyperedges-contact-highschool.txt``` contains the edges (1 per line, csv), the nodes are 1-based\n",
    "* file ```labels-contact-highschool.txt``` contains the node labels, 1 to 9 (in numerical order of the nodes)\n",
    "\n",
    "References in: https://zenodo.org/records/10155802\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the edges and ground-truth communities and build hypergraph H and 2-section graph G\n",
    "\n",
    "## pick one of the two datasets\n",
    "#dataset = 'primary'\n",
    "dataset = 'highschool'\n",
    "\n",
    "\n",
    "## read edge list, build H\n",
    "fp = open(datadir+'Contacts/hyperedges-contact-'+dataset+'.txt', 'r')\n",
    "Lines = fp.readlines()\n",
    "E = []\n",
    "for line in Lines:\n",
    "    E.append(set([x for x in line.strip().split(',')]))\n",
    "H = hnx.Hypergraph(dict(enumerate(E)))\n",
    "print('number of nodes:',len(H.nodes),'  number of edges:',len(H.edges))\n",
    "\n",
    "## build 2-section graph\n",
    "G = hmod.two_section(H)\n",
    "\n",
    "## read ground-truth communities and store in a dictionary\n",
    "fn = datadir+'Contacts/labels-contact-'+dataset+'.txt'\n",
    "gt = pd.read_csv(fn, header=None)[0].tolist()\n",
    "Communities = {str(k+1):v for k,v in enumerate(gt)}\n",
    "\n",
    "## plot the 2-section graph\n",
    "pal = ig.RainbowPalette(n=max(gt)+1)\n",
    "G.vs['color'] = [pal[Communities[v['name']]] for v in G.vs]\n",
    "ig.plot(G, bbox=(400,400), vertex_size=5, edge_color='lightgrey')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motifs example \n",
    "\n",
    "Using HNX and XGI draw function to get patterns from **Figure 7.1** in the book and count motifs reported in **Table 7.2**.\n",
    "\n",
    "Given:\n",
    "* E2: number of edges of size 2\n",
    "* G(E2): graph built only with E2\n",
    "* E3: edges of size 3\n",
    "\n",
    "Compute:\n",
    "* H1: number of subgraphs of 4-nodes in G(E2) with 5 edges + 6 times the number of 4-cliques in G(E2)\n",
    "* H3: count pairs of edges in E3 with intersection of size 2\n",
    "* H2: for each (i,j,k) in E3, count common neighbours in G(E2) for (i,j), (i,k) and (j,k) \n",
    "\n",
    "Random hypergraphs:\n",
    "* probability for 2-edges: p2 = c/(n-1)\n",
    "* probability for 3-edges to maintain expected 2-section graph degree:  p3 = (8-c)/((n-1)*(n-2)) \n",
    "* probability for 3-edges to maintain expected H-degree: p3 = (8-c)/((n-1)*(n/2-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## H1 pattern\n",
    "ly = {'A':(0,1),'B':(1,1),'C':(0,0),'D':(1,0)}\n",
    "E = [{'A'},{'B'},{'C'},{'D'}]\n",
    "HG = hnx.Hypergraph(dict(enumerate(E)))\n",
    "g = nx.Graph()\n",
    "g.add_edge('B','A')\n",
    "g.add_edge('C','A')\n",
    "g.add_edge('B','C')\n",
    "g.add_edge('B','D')\n",
    "g.add_edge('C','D')\n",
    "plt.figure(figsize=(3,3))\n",
    "hnx.draw(HG, pos=ly, with_edge_labels=False, with_node_labels=False,  \n",
    "         edges_kwargs={'linewidths': 0, 'edgecolors': 'grey'},\n",
    "         node_radius=3.0, with_additional_edges=g\n",
    "        )\n",
    "#plt.savefig('H1.eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## H2 pattern\n",
    "E = [{'A','B','C'},{'D'}]\n",
    "HG = hnx.Hypergraph(dict(enumerate(E)))\n",
    "g = nx.Graph()\n",
    "g.add_edge('B','D')\n",
    "g.add_edge('C','D')\n",
    "plt.figure(figsize=(3,3))\n",
    "hnx.draw(HG, pos=ly, with_edge_labels=False, with_node_labels=False,  \n",
    "         edges_kwargs={'linewidths': [1.5,0], 'edgecolors': 'grey'},\n",
    "         node_radius=3.0, with_additional_edges=g\n",
    "        )\n",
    "#plt.savefig('H2.eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## H3 pattern\n",
    "E = [{'A','B','C'},{'B','C','D'}]\n",
    "HG = hnx.Hypergraph(dict(enumerate(E)))\n",
    "plt.figure(figsize=(3,3))\n",
    "hnx.draw(HG, pos=ly, with_edge_labels=False, with_node_labels=False,  \n",
    "         edges_kwargs={'linewidths': 1.5, 'edgecolors': 'grey'},\n",
    "         node_radius=3.0\n",
    "        )\n",
    "#plt.savefig('H3.eps');\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## This takes a while to run -- see some results in next cell\n",
    "choice = '2-section'\n",
    "np.random.seed(123)\n",
    "\n",
    "n = 500\n",
    "V = [str(i) for i in range(n)]\n",
    "\n",
    "L = []\n",
    "REP = 16\n",
    "\n",
    "for c in np.arange(0,9):\n",
    "    p2 = c/(n-1)\n",
    "    if choice == '2-section':\n",
    "        p3 = (8-c)/((n-1)*(n-2))    ## to maintain expected 2-section graph degree\n",
    "    else:\n",
    "        p3 = (8-c)/((n-1)*(n/2-1))  ## to maintain expected H-degree\n",
    "    print('running c =',c)\n",
    "    for rep in range(REP):\n",
    "        E2 = []\n",
    "        E3 = []\n",
    "        \n",
    "        ## generate 2-edges\n",
    "        r = np.random.random(int(n*(n-1)/2))\n",
    "        v = combinations(V,2)\n",
    "        for i,j in enumerate(v):\n",
    "            if r[i] < p2:\n",
    "                E2.append(j)\n",
    "        ## generate 3-edges\n",
    "        r = np.random.random(int(n*(n-1)*(n-2)/6))\n",
    "        v = combinations(V,3)\n",
    "        for i,j in enumerate(v):\n",
    "            if r[i] < p3:\n",
    "                E3.append(j)\n",
    "\n",
    "        dg = 2*len(E2)+3*len(E3)\n",
    "        HG = hnx.Hypergraph(dict(enumerate(E2+E3)))\n",
    "        g = hmod.two_section(HG)\n",
    "        sd = g.ecount()\n",
    "\n",
    "        ## count motifs in graph G with 2-edges only\n",
    "        G = ig.Graph.TupleList(E2)\n",
    "        M = G.motifs_randesu(size=4)\n",
    "        H1 = M[9] + 6*M[10] ## exactly as H1 + 6 times 4-clique\n",
    "\n",
    "        ## H2: for each 3-edge, for each pair within, count common neighbor(s) in G\n",
    "        H2 = 0\n",
    "        for e in E3:\n",
    "            if len(set(G.vs['name']).intersection(set(e)))==3:\n",
    "                s1 = set(G.neighbors(G.vs.find(name=e[0])))\n",
    "                s2 = set(G.neighbors(G.vs.find(name=e[1])))\n",
    "                s3 = set(G.neighbors(G.vs.find(name=e[2])))\n",
    "                H2 += len(s1.intersection(s2))+len(s1.intersection(s3))+len(s3.intersection(s2))\n",
    "\n",
    "        ## H3: count pairs of 3-edges with intersection of size 2\n",
    "        H3 = 0\n",
    "        e = [set(i) for i in E3]\n",
    "        l = len(e)\n",
    "        for i in np.arange(0,l-1):\n",
    "            for j in np.arange(i+1,l):\n",
    "                if len(e[i].intersection(e[j]))==2:\n",
    "                    H3+=1\n",
    "        L.append([c,H1,H2,H3,dg/n,2*sd/n])\n",
    "        \n",
    "D = pd.DataFrame(L,columns=['c','H1','H2','H3','H deg','2-sec deg'])\n",
    "D.groupby(by='c').mean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### RESULTS - seed = 123 ###\n",
    "\n",
    "## fixing expected 2-section degree:\n",
    "\n",
    "     H1\t     H2\t     H3\t     H-deg\t   2-sec-deg\n",
    "c\t\t\t\t\t\n",
    "0\t0.0000\t0.0000\t16.3750\t4.086375\t8.10725\n",
    "1\t0.0000\t2.7500\t11.0000\t4.412250\t7.78375\n",
    "2\t0.0000\t10.0000\t8.4375\t4.993125\t7.89800\n",
    "3\t0.0625\t22.1250\t6.1250\t5.460875\t7.88925\n",
    "4\t0.3750\t31.2500\t3.3125\t5.961625\t7.93325\n",
    "5\t2.0000\t38.2500\t2.1875\t6.529000\t8.03175\n",
    "6\t3.3750\t38.8750\t0.9375\t7.056000\t8.04925\n",
    "7\t10.0625\t23.5625\t0.1875\t7.427000\t7.91650\n",
    "8\t15.3125\t0.0000\t0.0000\t7.932250\t7.93225\n",
    "\n",
    "# fixing expected H-degree\n",
    "\n",
    "     H1\t     H2\t     H3\t     H-deg\t   2-sec-deg\n",
    "c\t\t\t\t\t\n",
    "0\t0.0000\t0.0000\t66.0000\t8.074125\t15.88700\n",
    "1\t0.0000\t5.0625\t50.0000\t7.867875\t14.52700\n",
    "2\t0.0000\t18.0000\t36.8125\t7.998000\t13.77350\n",
    "3\t0.0625\t44.2500\t23.1250\t7.909250\t12.69200\n",
    "4\t0.3750\t61.3125\t14.2500\t7.947625\t11.83250\n",
    "5\t2.0000\t75.1875\t9.6250\t8.009125\t10.93325\n",
    "6\t3.3750\t73.6250\t3.2500\t8.048625\t10.00225\n",
    "7\t10.0625\t46.1250\t1.0625\t7.935125\t8.91300\n",
    "8\t15.3125\t0.0000\t0.0000\t7.932250\t7.93225\n",
    "\n",
    "\n",
    "### RESULTS - no seed (1st edition) ###\n",
    "\n",
    "## fixing expected 2-section degree:\n",
    "\n",
    "     H1\t     H2\t     H3\t     H-deg\t   2-sec-deg\n",
    "c\t\t\t\t\t\n",
    "0\t0.0000\t0.0000\t15.4375\t4.037625\t8.01375\n",
    "1\t0.0000\t2.1250\t11.5000\t4.509500\t7.96650\n",
    "2\t0.0000\t11.9375\t7.1875\t5.029125\t7.97825\n",
    "3\t0.1250\t21.6875\t6.8125\t5.549375\t8.03600\n",
    "4\t0.4375\t31.4375\t3.5625\t5.967875\t7.87300\n",
    "5\t1.5000\t37.9375\t1.7500\t6.485625\t7.90475\n",
    "6\t3.5000\t35.5625\t1.4375\t6.963125\t7.92875\n",
    "7\t9.0000\t24.0000\t0.3750\t7.509375\t7.99325\n",
    "8\t15.0625\t0.0000\t0.0000\t8.012750\t8.01275\n",
    "\n",
    "# fixing expected H-degree\n",
    "\n",
    "     H1\t     H2\t     H3\t     H-deg\t   2-sec-deg\n",
    "c\t\t\t\t\t\n",
    "0\t0.0000\t0.0000\t64.7500\t8.028750\t15.80150\n",
    "1\t0.0000\t3.9375\t47.6875\t8.091500\t14.94500\n",
    "2\t0.0625\t20.0625\t33.9375\t7.939125\t13.68550\n",
    "3\t0.0625\t43.5000\t23.6875\t7.979750\t12.81150\n",
    "4\t0.5000\t63.4375\t15.3750\t7.978500\t11.87000\n",
    "5\t1.3125\t72.5625\t9.1250\t7.951875\t10.84900\n",
    "6\t3.1250\t75.1250\t4.0000\t8.006500\t9.97400\n",
    "7\t8.3125\t52.0000\t0.6875\t8.040250\t9.04775\n",
    "8\t15.0625\t0.0000\t0.0000\t7.973000\t7.97300\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complexnetworks",
   "language": "python",
   "name": "complexnetworks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
