{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "- install node2vec code and add executable to `node2vec` directory in top level of the project (code: https://snap.stanford.edu/node2vec)\n",
    "- compile GED code (graph embedding divergence), \n",
    "  the base implementation of the framework in C (the code is included, and can also be found at      https://github.com/ftheberge/Comparing_Graph_Embeddings) \n",
    "- new package to install:\n",
    "```\n",
    "using PyCall\n",
    "run(`$(PyCall.python) -m pip install graphrole`)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the data directory\n",
    "datadir=\"../Datasets/\"\n",
    "\n",
    "## location of the GED code\n",
    "GED=\"../GED/GED\"\n",
    "\n",
    "## common colors\n",
    "cls = [\"red\",\"green\",\"blue\"];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyCall\n",
    "using PyPlot\n",
    "using LightGraphs\n",
    "using GraphPlot\n",
    "using Statistics\n",
    "using Random\n",
    "using LinearAlgebra\n",
    "using DataFrames\n",
    "using CSV\n",
    "using DecisionTree\n",
    "using FreqTables\n",
    "using StatsBase\n",
    "using Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = pyimport(\"igraph\")\n",
    "umap = pyimport(\"umap\")\n",
    "partition_igraph = pyimport(\"partition_igraph\")\n",
    "CHS = pyimport(\"sklearn.metrics\").calinski_harabasz_score\n",
    "AMI = pyimport(\"sklearn.metrics\").adjusted_mutual_info_score\n",
    "np = pyimport(\"numpy\")\n",
    "DBSCAN = pyimport(\"sklearn.cluster\").dbscan\n",
    "LogisticRegression = pyimport(\"sklearn.linear_model\").LogisticRegression\n",
    "roc_auc_score = pyimport(\"sklearn.metrics\").roc_auc_score\n",
    "roc_curve = pyimport(\"sklearn.metrics\").roc_curve\n",
    "npseed = pyimport(\"numpy.random\").seed\n",
    "npchoice = pyimport(\"numpy.random\").choice\n",
    "pickle_load = pyimport(\"pickle\").load\n",
    "RecursiveFeatureExtractor = pyimport(\"graphrole\").RecursiveFeatureExtractor\n",
    "RoleExtractor = pyimport(\"graphrole\").RoleExtractor;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function ig2lg(ig_g)\n",
    "    lg_g = SimpleGraph(ig_g.vcount())\n",
    "    for e in ig_g.es()\n",
    "        add_edge!(lg_g, e.source + 1, e.target + 1)\n",
    "    end\n",
    "    return lg_g\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function binary_operator(u, v, op=:had)\n",
    "    op == :had && return u .* v\n",
    "    op == :l1 && return abs.(u .- v)\n",
    "    op == :l2 && return (u .- v) .^ 2\n",
    "    op == :avg && return (u .+ v) ./ 2.0\n",
    "    throw(ArgumentError(\"unknown op\"))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Hope(g, sim, dim; beta=0.01, alpha=0.5)\n",
    "    dim = dim*2\n",
    "    A = g.get_adjacency().data\n",
    "    n = g.vcount()\n",
    "    ## Katz\n",
    "    if sim == :katz\n",
    "        M_g = I - beta * A\n",
    "        M_l = beta * A\n",
    "    end\n",
    "    ## Adamic-Adar\n",
    "    if sim == :aa\n",
    "        M_g = I\n",
    "        D = diagm((x -> x > 1 ? 1/log(x) : 0.0).(g.degree()))\n",
    "        M_l = A*D*A\n",
    "        M_l[diagind(M_l)] .= 0.0\n",
    "    end\n",
    "    ## Common neighbors\n",
    "    if sim == :cn\n",
    "        M_g = I\n",
    "        M_l = A*A\n",
    "    end\n",
    "    ## personalized page rank\n",
    "    if sim == :ppr\n",
    "        P = mapslices(A, dims=1) do x\n",
    "            s = sum(x)\n",
    "            iszero(s) ? fill(1/n, n) : x / s\n",
    "        end\n",
    "        M_g = I-alpha*P\n",
    "        M_l = (1-alpha)*I\n",
    "    end\n",
    "    S = M_g \\ M_l\n",
    "    k = div(dim, 2)\n",
    "    u, s, vt = svd(S)\n",
    "    X1 = u[:, 1:k] * diagm(sqrt.(s[1:k]))\n",
    "    ## undirected graphs have identical source and target embeddings\n",
    "    if !g.is_directed()\n",
    "        return X1\n",
    "    else\n",
    "        X2 = vtu[:, 1:k] * diagm(sqrt.(s[1:k]))\n",
    "        return [X1 X2]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save to disk to compute divergence\n",
    "function saveEmbedding(X, g, fn=\"_embed\")\n",
    "    names = g.vs.get_attribute_values(\"name\")\n",
    "    open(fn, \"w\") do f\n",
    "        println(f, size(X,1), \" \", size(X, 2))\n",
    "        for i in axes(X, 1)\n",
    "            print(f, names[i], \" \")\n",
    "            for j in axes(X, 2)\n",
    "                print(f, X[i, j])\n",
    "                 j < size(X, 2) && print(f, \" \")\n",
    "            end\n",
    "            println(f)\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computing JS divergence with GED code given edgelist, communities and embedding\n",
    "function JS(edge_file, comm_file, embed_file, entropy=false)\n",
    "    if entropy\n",
    "        x = `$GED -E -g $edge_file -c $comm_file -e $embed_file`\n",
    "    else\n",
    "        x = `$GED -g $edge_file -c $comm_file -e $embed_file`\n",
    "    end\n",
    "    io = IOBuffer()\n",
    "    run(pipeline(x, stdout=io), wait=true)\n",
    "    s = split(String(take!(io)), \" \")\n",
    "    return parse(Float64, s[2])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Laplacian eigenmaps embedding\n",
    "function LE(g, dim=2)\n",
    "    L_sym = g.laplacian(normalized=true)\n",
    "    w, v = eigen(L_sym)\n",
    "    return real.(v[:, 2:dim+1])\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read embedding from file in node2vec format\n",
    "## Map to layout format\n",
    "## for visualization, we use UMAP if dim > 2\n",
    "function embed2layout(fn=\"_embed\"; graph)\n",
    "    D = CSV.read(fn, DataFrame, header=false, datarow=2)\n",
    "    if eltype(D[!, end]) === Missing\n",
    "        D = D[!, 1:end-1]\n",
    "    end\n",
    "    \n",
    "    df1 = DataFrame(Column1=parse.(Int, graph.vs.get_attribute_values(\"name\")))\n",
    "    df1.id = axes(df1, 1)\n",
    "    df2 = leftjoin(df1, D, on=:Column1)\n",
    "    disallowmissing!(sort!(df2, :id))\n",
    "    \n",
    "    Y = Matrix(select(df2, Not(1:2)))\n",
    "    if size(Y,2) >  2\n",
    "        Y = umap.UMAP().fit_transform(Y)\n",
    "    end\n",
    "    return Y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function readEmbedding(fn=\"_embed\"; N2K=nothing)\n",
    "    D = CSV.read(fn, DataFrame, header=false, datarow=2)\n",
    "    if eltype(D[!, end]) === Missing\n",
    "        D = D[!, 1:end-1]\n",
    "    end\n",
    "    Y = Matrix(select(D, Not(1)))\n",
    "\n",
    "    if N2K !== nothing\n",
    "        x = [N2K[i] for i in D[:, 1]]\n",
    "        Y = Y[sortperm(x), :]\n",
    "    end\n",
    "    return Y\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare datasets\n",
    "\n",
    "* g: small ABCD graph (100 nodes), mainly for visualization and quick exampes\n",
    "* G: larger ABCD graph (1000 nodes), for experiments\n",
    "* z: zachary graph, for visualzation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Small ABCD graph "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## ABCD graph -- small enough for viz\n",
    "## We used the following parameters:\n",
    "n = \"100\"                     # number of vertices in graph\n",
    "t1 = \"3\"                      # power-law exponent for degree distribution\n",
    "d_min = \"5\"                   # minimum degree\n",
    "d_max = \"15\"                  # maximum degree\n",
    "d_max_iter = \"1000\"           # maximum number of iterations for sampling degrees\n",
    "t2 = \"2\"                      # power-law exponent for cluster size distribution\n",
    "c_min = \"25\"                  # minimum cluster size\n",
    "c_max = \"50\"                  # maximum cluster size\n",
    "c_max_iter = \"1000\"           # maximum number of iterations for sampling cluster sizes\n",
    "xi = \"0.2\"                    # fraction of edges to fall in background graph\n",
    "isCL = \"false\"                # if \"false\" use configuration model, if \"true\" use Chung-Lu\n",
    "degreefile = \"degrees.dat\"               # name of file that contains vertex degrees\n",
    "communitysizesfile = \"comm_sizes.dat\"    # name of file that contains community sizes\n",
    "communityfile = \"abcd_100_comm.dat\" # name of file that contains assignments of vertices to communities\n",
    "networkfile = \"abcd_100.dat\"        # name of file that contains edges of the generated graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read graph and communities\n",
    "g = ig.Graph.Read_Ncol(datadir * \"ABCD/abcd_100.dat\", directed=false)\n",
    "c = np.loadtxt(datadir * \"ABCD/abcd_100_comms.dat\", dtype=\"uint16\", usecols=(1))\n",
    "g.vs.set_attribute_values(\"comm\", [c[parse(Int, x.attributes()[\"name\"])] for x in g.vs])\n",
    "\n",
    "## print a few stats\n",
    "println(g.vcount(),\" vertices, \",\n",
    "        g.ecount(),\" edges; \",\n",
    "        \"avg degreee: \", mean(g.degree()),\n",
    "        \", communities: \",maximum(g.vs.get_attribute_values(\"comm\")))\n",
    "\n",
    "## ground truth\n",
    "gt = Dict(enumerate(g.vs.get_attribute_values(\"comm\")))\n",
    "## map between name to key\n",
    "n2k = Dict((v, k) for (k, v) in enumerate(g.vs.get_attribute_values(\"name\")))\n",
    "\n",
    "v_color = cls[g.vs.get_attribute_values(\"comm\")]\n",
    "g_lg = ig2lg(g)\n",
    "Random.seed!(2)\n",
    "gplot(g_lg,\n",
    "      NODESIZE=0.03, nodefillc=v_color,\n",
    "      EDGELINEWIDTH=0.2, edgestrokec=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Larger ABCD graph"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## ABCD graph -- larger for experiments\n",
    "## We used the following parameters:\n",
    "n = \"1000\"                     # number of vertices in graph\n",
    "t1 = \"3\"                       # power-law exponent for degree distribution\n",
    "d_min = \"10\"                   # minimum degree\n",
    "d_max = \"100\"                  # maximum degree\n",
    "d_max_iter = \"1000\"            # maximum number of iterations for sampling degrees\n",
    "t2 = \"2\"                       # power-law exponent for cluster size distribution\n",
    "c_min = \"50\"                   # minimum cluster size\n",
    "c_max = \"150\"                  # maximum cluster size\n",
    "c_max_iter = \"1000\"            # maximum number of iterations for sampling cluster sizes\n",
    "xi = \"0.6\"                     # fraction of edges to fall in background graph\n",
    "isCL = \"false\"                 # if \"false\" use configuration model, if \"true\" use Chung-Lu\n",
    "degreefile = \"degrees.dat\"               # name of file that contains vertex degrees\n",
    "communitysizesfile = \"comm_sizes.dat\"    # name of file that contains community sizes\n",
    "communityfile = \"abcd_1000_comm.dat\"       # name of file that contains assignments of vertices to communities\n",
    "networkfile = \"abcd_1000.dat\"              # name of file that contains edges of the generated graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read graph and communities\n",
    "G = ig.Graph.Read_Ncol(datadir * \"ABCD/abcd_1000.dat\", directed=false)\n",
    "C = np.loadtxt(datadir * \"ABCD/abcd_1000_comms.dat\", dtype=\"uint16\", usecols=(1))\n",
    "G.vs.set_attribute_values(\"comm\", [C[parse(Int, x.attributes()[\"name\"])] for x in G.vs])\n",
    "\n",
    "## print a few stats\n",
    "println(G.vcount(),\" vertices, \",\n",
    "        G.ecount(),\" edges; \",\n",
    "        \"avg degreee: \", mean(G.degree()),\n",
    "        \", communities: \",maximum(G.vs.get_attribute_values(\"comm\")))\n",
    "\n",
    "## ground truth\n",
    "GT = Dict(enumerate(G.vs.get_attribute_values(\"comm\")))\n",
    "## map between name to key\n",
    "N2K = Dict((v, k) for (k, v) in enumerate(G.vs.get_attribute_values(\"name\")))\n",
    "\n",
    "G_LG = ig2lg(G)\n",
    "Random.seed!(2)\n",
    "gplot(G_LG,\n",
    "      NODESIZE=0.01, nodefillc=\"black\",\n",
    "      EDGELINEWIDTH=0.1, edgestrokec=\"gray\") ## communities are far from obvious in 2d layout!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Zachary (karate) graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = ig.Graph.Famous(\"zachary\")\n",
    "c = [0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1] .+ 1\n",
    "z.vs.set_attribute_values(\"name\", string.(0:z.vcount()-1))\n",
    "z.vs.set_attribute_values(\"comm\", [c[parse(Int, x.attributes()[\"name\"])+1] for x in z.vs])\n",
    "\n",
    "v_color = cls[z.vs.get_attribute_values(\"comm\")]\n",
    "z_lg = ig2lg(z)\n",
    "Random.seed!(2)\n",
    "gplot(z_lg,\n",
    "      NODESIZE=0.03, nodefillc=v_color,\n",
    "      EDGELINEWIDTH=0.2, edgestrokec=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show various 2d layouts using small Zachary graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gplot(z_lg, layout=random_layout,\n",
    "      NODESIZE=0.03, nodefillc=v_color,\n",
    "      EDGELINEWIDTH=0.2, edgestrokec=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gplot(z_lg, layout=circular_layout,\n",
    "      NODESIZE=0.03, nodefillc=v_color,\n",
    "      EDGELINEWIDTH=0.2, edgestrokec=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gplot(z_lg, layout=spectral_layout,\n",
    "      NODESIZE=0.03, nodefillc=v_color,\n",
    "      EDGELINEWIDTH=0.2, edgestrokec=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform several embeddings -- Zachary graph\n",
    "* node2vec from source code\n",
    "* HOPE with different similarities\n",
    "* Laplacian Eigenmaps\n",
    "* visualize some good and bad results\n",
    "\n",
    "We use the framework to compute the \"graph embedding divergence\" (GED.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L=DataFrame()\n",
    "DIM=[5,10,15]\n",
    "\n",
    "best_jsd = 1.1\n",
    "worst_jsd = -0.1\n",
    "\n",
    "## Hope\n",
    "for dim in DIM, sim in (:katz,:ppr, :cn, :aa)\n",
    "    X = Hope(z,sim,dim)\n",
    "    saveEmbedding(X,z)\n",
    "    jsd = JS(datadir * \"Zachary/zachary.edgelist\", datadir * \"Zachary/zachary.ecg\", \"_embed\")\n",
    "    \n",
    "    if jsd < best_jsd\n",
    "        cp(\"_embed\", \"_embed_best\", force=true)\n",
    "        global best_jsd = jsd\n",
    "    end\n",
    "    if jsd > worst_jsd\n",
    "        cp(\"_embed\", \"_embed_worst\", force=true)\n",
    "        global worst_jsd = jsd\n",
    "    end\n",
    "\n",
    "    push!(L, (dim=dim, alg=\"hope\", param=sim, jsd=jsd))\n",
    "end\n",
    "\n",
    "## LE\n",
    "for dim in DIM\n",
    "    X = LE(z,dim)\n",
    "    saveEmbedding(X,z)\n",
    "    jsd = JS(datadir * \"Zachary/zachary.edgelist\", datadir * \"Zachary/zachary.ecg\", \"_embed\")\n",
    "\n",
    "    if jsd < best_jsd\n",
    "        cp(\"_embed\", \"_embed_best\", force=true)\n",
    "        global best_jsd = jsd\n",
    "    end\n",
    "    if jsd > worst_jsd\n",
    "        cp(\"_embed\", \"_embed_worst\", force=true)\n",
    "        global worst_jsd = jsd\n",
    "    end\n",
    "\n",
    "    push!(L, (dim=dim, alg=\"le\", param=nothing,jsd=jsd), cols=:union)\n",
    "end\n",
    "\n",
    "## node2vec is in my path\n",
    "for dim in DIM, (p,q) in [(1,0.1),(0.1,1),(1,1)]\n",
    "        x = `../node2vec/node2vec -i:$datadir/Zachary/zachary.edgelist -o:_embed -d:$dim -p:$p -q:$q -l:15`\n",
    "        run(x, wait=true)\n",
    "        jsd = JS(datadir * \"Zachary/zachary.edgelist\" ,datadir * \"Zachary/zachary.ecg\", \"_embed\")\n",
    "\n",
    "        if jsd < best_jsd\n",
    "            cp(\"_embed\", \"_embed_best\", force=true)\n",
    "            global best_jsd = jsd\n",
    "        end\n",
    "        if jsd > worst_jsd\n",
    "            cp(\"_embed\", \"_embed_worst\", force=true)\n",
    "            global worst_jsd = jsd\n",
    "        end\n",
    "\n",
    "        push!(L, (dim=dim, alg=\"n2v\", param=(p,q),jsd=jsd), cols=:union)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort!(L, :jsd);\n",
    "first(L, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot top result\n",
    "l = embed2layout(\"_embed_best\", graph=z)\n",
    "gplot(z_lg, l[:, 1], l[:, 2],\n",
    "      NODESIZE=0.03, nodefillc=v_color,\n",
    "      EDGELINEWIDTH=0.2, edgestrokec=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last(L, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot bottom result\n",
    "\n",
    "l = embed2layout(\"_embed_worst\", graph=z)\n",
    "gplot(z_lg, l[:, 1], l[:, 2],\n",
    "      NODESIZE=0.03, nodefillc=v_color,\n",
    "      EDGELINEWIDTH=0.2, edgestrokec=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform several embeddings -- small ABCD  graph\n",
    "* node2vec from source code\n",
    "* HOPE different similarities\n",
    "* Laplacian Eigenmaps\n",
    "* visualize some good and bad results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = DataFrame()\n",
    "DIM = [2,4,8,16,24,32]\n",
    "\n",
    "best_jsd = 1.1\n",
    "worst_jsd = -0.1\n",
    "\n",
    "## Hope\n",
    "for dim in DIM, sim in (:katz,:ppr, :cn, :aa)\n",
    "    X = Hope(g,sim,dim)\n",
    "    saveEmbedding(X,g)\n",
    "    jsd = JS(datadir * \"ABCD/abcd_100.dat\", datadir * \"ABCD/abcd_100.ecg\", \"_embed\")\n",
    "    \n",
    "    if jsd < best_jsd\n",
    "        cp(\"_embed\", \"_embed_best\", force=true)\n",
    "        global best_jsd = jsd\n",
    "    end\n",
    "    if jsd > worst_jsd\n",
    "        cp(\"_embed\", \"_embed_worst\", force=true)\n",
    "        global worst_jsd = jsd\n",
    "    end\n",
    "\n",
    "    push!(L, (dim=dim, alg=\"hope\", param=sim, jsd=jsd))\n",
    "end\n",
    "\n",
    "## LE\n",
    "for dim in DIM\n",
    "    X = LE(g,dim)\n",
    "    saveEmbedding(X,g)\n",
    "    jsd = JS(datadir * \"ABCD/abcd_100.dat\", datadir * \"ABCD/abcd_100.ecg\", \"_embed\")\n",
    "    \n",
    "    if jsd < best_jsd\n",
    "        cp(\"_embed\", \"_embed_best\", force=true)\n",
    "        global best_jsd = jsd\n",
    "    end\n",
    "    if jsd > worst_jsd\n",
    "        cp(\"_embed\", \"_embed_worst\", force=true)\n",
    "        global worst_jsd = jsd\n",
    "    end\n",
    "\n",
    "    push!(L, (dim=dim, alg=\"le\", param=nothing,jsd=jsd), cols=:union)\n",
    "end\n",
    "\n",
    "## node2vec is in my path\n",
    "for dim in DIM, (p,q) in [(1,0.1),(0.1,1),(1,1)]\n",
    "        x = `../node2vec/node2vec -i:$datadir/ABCD/abcd_100.dat -o:_embed -d:$dim -p:$p -q:$q -l:15 -l:15`\n",
    "        run(x, wait=true)\n",
    "        jsd = JS(datadir * \"ABCD/abcd_100.dat\" ,datadir * \"ABCD/abcd_100.ecg\", \"_embed\")\n",
    "    \n",
    "        if jsd < best_jsd\n",
    "            cp(\"_embed\", \"_embed_best\", force=true)\n",
    "            global best_jsd = jsd\n",
    "        end\n",
    "        if jsd > worst_jsd\n",
    "        cp(\"_embed\", \"_embed_worst\", force=true)\n",
    "            global worst_jsd = jsd\n",
    "        end\n",
    "\n",
    "        push!(L, (dim=dim, alg=\"n2v\", param=(p,q),jsd=jsd), cols=:union)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort!(L, :jsd)\n",
    "first(L, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## re-run and plot top result\n",
    "l = embed2layout(\"_embed_best\", graph=g)\n",
    "v_color = cls[g.vs.get_attribute_values(\"comm\")]\n",
    "gplot(g_lg, l[:, 1], l[:, 2],\n",
    "      NODESIZE=0.03, nodefillc=v_color,\n",
    "      EDGELINEWIDTH=0.2, edgestrokec=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last(L, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot bottom result\n",
    "l = embed2layout(\"_embed_worst\", graph=g)\n",
    " gplot(g_lg, l[:, 1], l[:, 2],\n",
    "       NODESIZE=0.03, nodefillc=v_color,\n",
    "       EDGELINEWIDTH=0.2, edgestrokec=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large ABCD graph -- find a good embedding with the framework\n",
    "* we only look as 16 configurations with HOPE for now (for speed)\n",
    "* we'll consider more in the large classification experiment later\n",
    "\n",
    "The good embedding was generated using the code provided in the Python notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on larger ABCD graph\n",
    "\n",
    "* we use a good embedding saved from above cell\n",
    "* we use a random forest model on embedded space\n",
    "* we split the data as train and test\n",
    "* the goal is to recover the communities for each node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## used saved \"best\" embedding from above\n",
    "X = readEmbedding(datadir * \"ABCD/abcd_1000_embed_best\")\n",
    "y = string.(G.vs.get_attribute_values(\"comm\"))\n",
    "## train/test split\n",
    "Random.seed!(1234)\n",
    "to_train = falses(length(y))\n",
    "to_train[1:round(Int, length(y)*0.25)] .= true\n",
    "shuffle!(to_train)\n",
    "\n",
    "X_train = X[to_train, :]\n",
    "X_test = X[.!to_train, :]\n",
    "y_train = y[to_train]\n",
    "y_test = y[.!to_train]\n",
    "\n",
    "model = build_forest(y_train, X_train, size(X, 2) รท 2, 100, 0.5)\n",
    "\n",
    "y_pred = apply_forest(model, X_test)\n",
    "\n",
    "cm = freqtable(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"accuracy: $(sum(diag(cm))/length(y_test))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare with random classifier -- assuming we know only the number of classes (12)\n",
    "describe([sum(diag(freqtable(rand(1:12, length(y_test)), y_test)))/length(y_test) for i in 1:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare with random classifier -- using class proportions in training data\n",
    "describe([sum(diag(freqtable(rand(y_train, length(y_test)), y_test)))/length(y_test) for i in 1:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "* we run graph clustering (Louvain, ECG)\n",
    "* we compare with vector space embedding using same embedding\n",
    "* we use k-means (various k) and DBSCAN\n",
    "* recall there are 12 ground truth community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## again we use 'good' embedding from before\n",
    "X = readEmbedding(datadir * \"ABCD/abcd_1000_embed_best\")\n",
    "\n",
    "L = DataFrame()\n",
    "K = [6,9,12,15,24] ## for k-means (real number of clusters is 12)\n",
    "REP = 30\n",
    "\n",
    "for i in 1:REP\n",
    "    ## kmeans\n",
    "    for k in K\n",
    "        labels = kmeans(X', k).assignments\n",
    "        scr = CHS(X, labels)\n",
    "        ami = AMI(G.vs.get_attribute_values(\"comm\"), labels)\n",
    "        push!(L, (method=\"km\", param=k, scr=scr, ami=ami))\n",
    "    end\n",
    "    ## ECG\n",
    "    ec = G.community_ecg().membership\n",
    "    scr = G.modularity(ec)\n",
    "    ami = AMI(G.vs.get_attribute_values(\"comm\"),ec)\n",
    "    push!(L, (method=\"ecg\", param=nothing, scr=scr, ami=ami), cols=:union)\n",
    "    ## Louvain -- permute as this is not done in igraph\n",
    "    p = randperm(G.vcount()) .- 1\n",
    "    GG = G.permute_vertices(Any[x for x in p])\n",
    "    l = GG.community_multilevel().membership\n",
    "    ll = similar(l)\n",
    "    for i in 1:length(l)\n",
    "        ll[i] = l[p[i] + 1]\n",
    "    end\n",
    "    scr = G.modularity(ll)\n",
    "    ami = AMI(G.vs.get_attribute_values(\"comm\"),ll)\n",
    "    push!(L, (method=\"ml\", param=nothing, scr=scr, ami=ami), cols=:union)\n",
    "end\n",
    "\n",
    "combine(groupby(L, :method), x -> last(sort(x, :scr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gL = groupby(L, [:method, :param], sort=true)\n",
    "\n",
    "boxplot([sdf.ami for sdf in gL])\n",
    "xticks(1:7, [isnothing(p) ? m : \"$m($p)\" for (m,p) in keys(gL)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DBSCAN -- we tried different epsilon and dim\n",
    "## test via calinski_harabasz_score (CHS) or silhouette_score or davies_bouldin_score\n",
    "## best result obtained empirically with min_samples = 8\n",
    "\n",
    "top = 0\n",
    "for dim in [4,8,16,24,32,40,48,64], ms in [8]\n",
    "    U = umap.UMAP(n_components=24).fit_transform(X)\n",
    "    for e in 0.4:0.0025:0.5\n",
    "        cl = DBSCAN(U, eps=e, min_samples=ms)\n",
    "        labels = cl[2]\n",
    "        s = CHS(U,labels) ## score\n",
    "        if s > top\n",
    "            global top = s\n",
    "            global e_top = e\n",
    "            global d_top = dim\n",
    "            global m_top = ms\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "U = umap.UMAP(n_components=d_top).fit_transform(X)\n",
    "cl = DBSCAN(U, eps=e_top, min_samples=m_top)\n",
    "labels = cl[2]\n",
    "\n",
    "b = labels .> -1\n",
    "println(\"AMI without outliers: \", AMI(G.vs.get_attribute_values(\"comm\")[b], labels[b]))\n",
    "println(\"AMI with outliers: \", AMI(G.vs.get_attribute_values(\"comm\"), labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link prediction\n",
    "\n",
    "* we drop 10% edges and re-compute the embedding\n",
    "* we train a logistic regression model\n",
    "* we apply final model to test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First try with noisy graph\n",
    "\n",
    "Recall that xi=0.6, the proportion of noise edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick 10% edges at random, save new graph as Gp\n",
    "Random.seed!(12345)\n",
    "test_size = round(Int, 0.1*G.ecount())\n",
    "\n",
    "npseed(123456)\n",
    "test_eid = npchoice(G.ecount(),size=test_size,replace=false)\n",
    "\n",
    "#test_eid = sample(0:G.ecount()-1, test_size, replace=false)\n",
    "Gp = G.copy()\n",
    "Gp.delete_edges(test_eid)\n",
    "\n",
    "## compute embedding on Gp \n",
    "X = Hope(Gp, :ppr, 48);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train model with Hadamard binary operator (other choices are 'l1', 'l2 and 'avg')\n",
    "op = :had\n",
    "\n",
    "## Build training data, first the edges\n",
    "F = [binary_operator(X[e.tuple[1]+1, :],X[e.tuple[2]+1, :],op) for e in Gp.es]\n",
    "sz = length(F)\n",
    "f = [fill(1, sz); fill(0, sz)]\n",
    "\n",
    "## then for equal number of non-edges (we over-sample to drop edges and collisions from the list)\n",
    "e = [minmax(rand(0:Gp.vcount()-1), rand(0:Gp.vcount()-1)) for i in 1:2*sz]\n",
    "unique!(e) # drop collisions\n",
    "filter!(((a, b),) -> Gp.get_eid(a, b, directed=false, error=false) == -1, e)\n",
    "non_edges = e[1:sz]\n",
    "## features for node pairs without edges\n",
    "for (a, b) in non_edges\n",
    "    push!(F, binary_operator(X[a+1, :],X[b+1, :], op))\n",
    "end\n",
    "\n",
    "## train model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(F, f)\n",
    "\n",
    "## prepare test set, first with all dropped edges from G \n",
    "X_test = [(e = G.es[i+1]; binary_operator(X[e.tuple[1]+1, :],X[e.tuple[2]+1, :],op)) for i in test_eid]\n",
    "szt = length(X_test)\n",
    "y_test = [fill(1, szt); fill(0, szt)]\n",
    "\n",
    "## then for equal number of non-edges (we over-sample to drop edges and collisions from the list)\n",
    "e2 = [minmax(rand(0:G.vcount()-1), rand(0:G.vcount()-1)) for i in 1:2*szt]\n",
    "unique!(e2) # drop collisions\n",
    "filter!(((a, b),) -> G.get_eid(a, b, directed=false, error=false) == -1, e2)\n",
    "non_edges2 = e2[1:szt]\n",
    "for (a, b) in non_edges2\n",
    "    push!(X_test, binary_operator(X[a+1, :],X[b+1, :], op))\n",
    "end\n",
    "\n",
    "## apply the model to test data\n",
    "println(\"Accuracy of logistic regression classifier with $op on test set: \", logreg.score(X_test, y_test))\n",
    "print(\"AUC: \",roc_auc_score(y_test, logreg.predict_proba(X_test)[:,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link prediction with less noisy graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read graph and communities - graph with xi=0.2\n",
    "G2 = ig.Graph.Read_Ncol(datadir * \"ABCD/abcd_1000_xi2.dat\", directed=false)\n",
    "C = np.loadtxt(datadir * \"ABCD/abcd_1000_xi2_comms.dat\", dtype=\"uint16\", usecols=(1))\n",
    "G.vs.set_attribute_values(\"comm\", [C[parse(Int, x.attributes()[\"name\"])] for x in G.vs])\n",
    "\n",
    "## pick 10% edges at random, save new graph as Gp\n",
    "Random.seed!(12345)\n",
    "test_size = round(Int, 0.2*G2.ecount())\n",
    "\n",
    "npseed(123456)\n",
    "test_eid = npchoice(G2.ecount(),size=test_size,replace=false)\n",
    "\n",
    "#test_eid = sample(0:G.ecount()-1, test_size, replace=false)\n",
    "Gp = G2.copy()\n",
    "Gp.delete_edges(test_eid)\n",
    "\n",
    "## compute embedding on Gp \n",
    "X = Hope(Gp, :ppr, 48);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train model with Hadamard binary operator (other choices are 'l1', 'l2 and 'avg')\n",
    "op = :had\n",
    "\n",
    "## Build training data, first the edges\n",
    "F = [binary_operator(X[e.tuple[1]+1, :],X[e.tuple[2]+1, :],op) for e in Gp.es]\n",
    "sz = length(F)\n",
    "f = [fill(1, sz); fill(0, sz)]\n",
    "\n",
    "## then for equal number of non-edges (we over-sample to drop edges and collisions from the list)\n",
    "e = [minmax(rand(0:Gp.vcount()-1), rand(0:Gp.vcount()-1)) for i in 1:2*sz]\n",
    "unique!(e) # drop collisions\n",
    "filter!(((a, b),) -> Gp.get_eid(a, b, directed=false, error=false) == -1, e)\n",
    "non_edges = e[1:sz]\n",
    "## features for node pairs without edges\n",
    "for (a, b) in non_edges\n",
    "    push!(F, binary_operator(X[a+1, :],X[b+1, :], op))\n",
    "end\n",
    "\n",
    "## train model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(F, f)\n",
    "\n",
    "## prepare test set, first with all dropped edges from G \n",
    "X_test = [(e = G2.es[i+1]; binary_operator(X[e.tuple[1]+1, :],X[e.tuple[2]+1, :],op)) for i in test_eid]\n",
    "szt = length(X_test)\n",
    "y_test = [fill(1, szt); fill(0, szt)]\n",
    "\n",
    "## then for equal number of non-edges (we over-sample to drop edges and collisions from the list)\n",
    "e2 = [minmax(rand(0:G2.vcount()-1), rand(0:G2.vcount()-1)) for i in 1:2*szt]\n",
    "unique!(e2) # drop collisions\n",
    "filter!(((a, b),) -> G2.get_eid(a, b, directed=false, error=false) == -1, e2)\n",
    "non_edges2 = e2[1:szt]\n",
    "for (a, b) in non_edges2\n",
    "    push!(X_test, binary_operator(X[a+1, :],X[b+1, :], op))\n",
    "end\n",
    "\n",
    "## apply the model to test data\n",
    "println(\"Accuracy of logistic regression classifier with $op on test set: \", logreg.score(X_test, y_test))\n",
    "print(\"AUC: \",roc_auc_score(y_test, logreg.predict_proba(X_test)[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_roc_auc = round(10000*roc_auc_score(y_test, logreg.predict_proba(X_test)[:,2]))/100\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,2])\n",
    "plot(fpr, tpr, color=\"gray\",label=\"Logistic Regression (AUC = $logit_roc_auc%)\")\n",
    "plot([0, 1], [0, 1],\"k--\")\n",
    "xlim([0.0, 1.0])\n",
    "ylim([0.0, 1.05])\n",
    "xlabel(\"False Positive Rate\")\n",
    "ylabel(\"True Positive Rate\")\n",
    "title(\"\")\n",
    "legend(loc=\"lower right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger study -- use accuracy for picking embedding\n",
    "\n",
    "- we training-validation-test split\n",
    "- this can be long to run -- a pickle file with the results is included in data directory\n",
    "- to re-run from scratch please check Python notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load L and train/val/test ids\n",
    "fh = py\"open\"(datadir * \"ABCD/abcd_1000_embeddings.pkl\", \"rb\")\n",
    "id_train,id_val,id_trainval,id_test,L = pickle_load(fh)\n",
    "fh.close()\n",
    "y_all = G.vs.get_attribute_values(\"comm\")\n",
    "y_train = [y_all[i+1] for i in id_train]\n",
    "y_trainval = [y_all[i+1] for i in id_trainval]\n",
    "y_val = [y_all[i+1] for i in id_val]\n",
    "y_test = [y_all[i+1] for i in id_test];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = identity.(DataFrame(L,[:dim,:algo,:param,:div,:acc]))\n",
    "print(\"Kendall's correlation: \", corkendall(R.div, R.acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort by Divergence on validation set\n",
    "sort!(R, :div)\n",
    "R.rank_div = axes(R, 1)\n",
    "first(R, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort by Accuracy on validation set\n",
    "sort!(R, :acc, rev=true)\n",
    "R.rank_acc = axes(R, 1)\n",
    "first(R, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## quite a range of accuracy on the validation set!\n",
    "last(R, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Apply to test set. \n",
    "\n",
    "This takes several minutes to run so a pickle file is provided.\n",
    "\n",
    "Source codes for generating the data are provided in Python notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load L and train/val/test ids\n",
    "fh = py\"open\"(datadir * \"ABCD/abcd_1000_embeddings_test.pkl\", \"rb\")\n",
    "R.test = pickle_load(fh)\n",
    "fh.close()\n",
    "println(\"mean accuracy over all models on test set:\",mean(R.test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort by Accuracy on validation set\n",
    "sort!(R, :test, rev=true)\n",
    "R.rank_test = axes(R, 1)\n",
    "first(R, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## top results on test set w.r.t. divergence on validation set\n",
    "sort!(R, :div)\n",
    "top_div = R.test[1:10]\n",
    "\n",
    "## top results on test set w.r.t. accuracy on validation set\n",
    "sort!(R, :acc, rev=true)\n",
    "top_acc = R.test[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot([top_acc, top_div])\n",
    "xticks(1:2, [\"Top-10 validation set accuracy\",\"Top-10 divergence score\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(R.rank_acc,R.test,\".\",color=\"black\")\n",
    "xlabel(\"Rank\",fontsize=14)\n",
    "ylabel(\"Test set accuracy\",fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(R.rank_div,R.test,\".\",color=\"black\")\n",
    "xlabel(\"Rank\",fontsize=14)\n",
    "ylabel(\"Test set accuracy\",fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((cor(R.rank_acc,R.test),\n",
    "       cor(R.rank_div,R.test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sample_random_classifier()\n",
    "    y_pred = sample(1:12, Weights(proptable(y_trainval)), length(y_test))\n",
    "    return sum(diag(proptable(y_test, y_pred)))\n",
    "end\n",
    "println(\"Random classifier accuracy on test set: \")\n",
    "describe([sample_random_classifier() for i in 1:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReFex: illustrate roles on Zachary graph\n",
    "\n",
    "We use the 'graphrole' package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features\n",
    "feature_extractor = RecursiveFeatureExtractor(z, max_generations=5)\n",
    "features = feature_extractor.extract_features()\n",
    "println(\"Features extracted from $(feature_extractor.generation_count) recursive generations:\")\n",
    "features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign node roles in a dictionary\n",
    "role_extractor = RoleExtractor(n_roles=3)\n",
    "role_extractor.extract_role_factors(features)\n",
    "node_roles = role_extractor.roles\n",
    "role_extractor.role_percentage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_roles = sort(unique(values(node_roles)))\n",
    "cls = [\"red\",\"blue\",\"green\"]\n",
    "# map roles to colors\n",
    "role_colors = Dict(unique_roles .=> cls)\n",
    "v_color = [role_colors[node_roles[i]] for i in 0:z.vcount()-1]\n",
    "Random.seed!(2)\n",
    "gplot(z_lg,\n",
    "      NODESIZE=0.03, nodefillc=v_color,\n",
    "      EDGELINEWIDTH=0.2, edgestrokec=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset -- American College Football Graph\n",
    "\n",
    "[REF]: \"Community structure in social and biological networks\", M. Girvan and M. E. J. Newman\n",
    "PNAS June 11, 2002 99 (12) 7821-7826; https://doi.org/10.1073/pnas.122653799\n",
    "\n",
    "\n",
    "Teams are part of 12 conferences (the 'communities'):\n",
    "*   0 = Atlantic Coast\n",
    "*   1 = Big East\n",
    "*   2 = Big Ten\n",
    "*   3 = Big Twelve\n",
    "*   4 = Conference USA\n",
    "*   5 = Independents\n",
    "*   6 = Mid-American\n",
    "*   7 = Mountain West\n",
    "*   8 = Pacific Ten\n",
    "*   9 = Southeastern\n",
    "*  10 = Sun Belt\n",
    "*  11 = Western Athletic\n",
    "\n",
    "14 teams out of 115 appear as anomalies as can be seen in Figure 5 of [REF], namely:\n",
    "- 5 teams in #5 conference (Independent) play teams in other conferences (green triangles)\n",
    "- 7 teams in #10 conference (Sun Belt) are broken in 2 clumps (pink triangles) \n",
    "- 2 teams from #11 conference play mainly with #10 conference (red triangles)\n",
    "\n",
    "Here, we try to recover those anomalous teams by running several embeddings (we use node2vec):\n",
    "\n",
    "- for each embedding:\n",
    " - compute divergence using our framework\n",
    " - also compute entropy of b-vector for each node (probability distribution of edges w.r.t. every community in the geometric Chung-Lu model)\n",
    "- plot entropy vs divergence\n",
    "- for some good/bad embedding, boxplot entropy of anomalous vs other nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_color = [\"red\", \"red\", \"green\", \"green\", \"blue\", \"blue\",\n",
    "              \"black\", \"black\", \"yellow\", \"yellow\", \"orange\", \"orange\"]\n",
    "comm_label = repeat([\"\", \"x\"], 6)\n",
    "\n",
    "## read graph and communities\n",
    "g = ig.Graph.Read_Ncol(datadir * \"Football/football.edgelist\", directed=false)\n",
    "c = parse.(Int, readlines(datadir * \"Football/football.community\"))\n",
    "g.vs.set_attribute_values(\"community\", [c[1 + parse(Int, x.attributes()[\"name\"])] for x in g.vs])\n",
    "\n",
    "anomaly = Int[c in [5, 10] || n in [\"28\", \"58\"] for\n",
    "              (c, n) in zip(g.vs.get_attribute_values(\"community\"), g.vs.get_attribute_values(\"name\"))]\n",
    "\n",
    "# communities are marked with color and an optional \"x\" mark on the node\n",
    "# if the mark is \"A\" and node is a bit larger it is an anomaly\n",
    "Random.seed!(1234)\n",
    "lab = comm_label[g.vs.get_attribute_values(\"community\") .+ 1]\n",
    "lab[Bool.(anomaly)] .= \"A\"\n",
    "gplot(ig2lg(g),\n",
    "      NODESIZE=0.03 .+ anomaly ./75, nodefillc=comm_color[g.vs.get_attribute_values(\"community\") .+ 1],\n",
    "      nodelabel=lab,\n",
    "      nodelabelc=\"white\",\n",
    "      EDGELINEWIDTH=0.2, edgestrokec=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_jsd = 1.1\n",
    "worst_jsd = -0.1\n",
    "## node2vec with varying parameters (60 embeddings)\n",
    "L = DataFrame()\n",
    "for dim in 2:2:24\n",
    "    for (p,q) in [(1.0, 0.5),(0.5, 1.0),(1.0, 0.01),(0.01, 1.0),(1.0, 1.0)]\n",
    "        x = `../node2vec/node2vec -i:$datadir/Football/football.edgelist -o:_embed -d:$dim -p:$p -q:$q`\n",
    "        run(x, wait=true)\n",
    "        jsd = JS(datadir * \"Football/football.edgelist\" ,datadir * \"Football/football.ecg\", \"_embed\", true)\n",
    "        if jsd < best_jsd\n",
    "            cp(\"_entropy\", \"_entropy_best\", force=true)\n",
    "            global best_jsd = jsd\n",
    "        end\n",
    "        if jsd > worst_jsd\n",
    "            cp(\"_entropy\", \"_entropy_worst\", force=true)\n",
    "            global worst_jsd = jsd\n",
    "        end\n",
    "        ent = parse.(Float64, getindex.(split.(readlines(\"_entropy\"), ','), 2))\n",
    "        roc = roc_auc_score(anomaly, ent)\n",
    "        push!(L, (dim=dim, algo=\"n2v\", param=(p,q),jsd=jsd, auc=roc))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort!(L, :jsd)\n",
    "first(L, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last(L, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## auc vs divergence (jsd)\n",
    "plot(L.jsd,L.auc,\"o\",color=\"black\")\n",
    "xlabel(\"JS Divergence\",fontsize=14)\n",
    "ylabel(\"AUC\",fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Entropy scores - some good embedding\n",
    "ent = parse.(Float64, getindex.(split.(readlines(\"_entropy_best\"), ','), 2))\n",
    "boxplot([ent[anomaly .== 0], ent[anomaly .== 1]],\n",
    "        labels=[\"Regular\", \"Anomalous\"], sym=\".\",whis=(0,100), widths=.5)\n",
    "title(\"Low divergence embedding\",fontsize=14)\n",
    "ylabel(\"Entropy\",fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Entropy scores - not so good embedding\n",
    "ent = parse.(Float64, getindex.(split.(readlines(\"_entropy_worst\"), ','), 2))\n",
    "boxplot([ent[anomaly .== 0], ent[anomaly .== 1]],\n",
    "             labels=[\"Regular\", \"Anomalous\"], sym=\".\",whis=(0,100), widths=.5)\n",
    "title(\"Low divergence embedding\",fontsize=14)\n",
    "ylabel(\"Entropy\",fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC using average rank with several top embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 7\n",
    "ranking = fill(0.0, g.vcount())\n",
    "for i in 1:k\n",
    "    dim = L.dim[i]\n",
    "    p,q = L.param[i]\n",
    "    x = `../node2vec/node2vec -i:$datadir/Football/football.edgelist -o:_embed -d:$dim -p:$p -q:$q`\n",
    "    run(x, wait=true)\n",
    "    jsd = JS(datadir * \"Football/football.edgelist\" ,datadir * \"Football/football.ecg\", \"_embed\", true)\n",
    "    ent = parse.(Float64, getindex.(split.(readlines(\"_entropy\"), ','), 2))\n",
    "    global ranking .+= ordinalrank(ent)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"AUC: \",roc_auc_score(anomaly, ranking))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
