{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 - Hypergraphs\n",
    "\n",
    "In this notebook, we introduce hypergraphs, a generalization of graphs where we allow for arbitrary sized edges (in practice, we consider edges of size 2 or more). We illustrate a few concepts using hypergraphs including modularity, community detection and transformation into 2-section graphs.\n",
    "\n",
    "**This notebook requires version 1.2 or newer of the HyperNetX package** (https://github.com/pnnl/HyperNetX).\n",
    "\n",
    "### New required package (HNX version 1.2 or newer):\n",
    "\n",
    "* pip install hypernetx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set this to the data directory\n",
    "datadir='../Datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "import partition_igraph\n",
    "import hypernetx as hnx\n",
    "import hypernetx.algorithms.hypergraph_modularity as hmod ## new as of version 1.2\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import itertools\n",
    "from scipy.special import comb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy hypergraph example with HNX\n",
    "\n",
    "We illustrate a few concepts with a small toy hypergraph. \n",
    "First, we build the hnx hypergraph from a list of sets (the edges), and we draw the hypergraph as well as its dual (where the role of nodes and edges are swapped).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build an hypergraph from a list of sets (the hyperedges)\n",
    "## using 'enumerate', edges will have integer IDs\n",
    "E = [{'A','B'},{'A','C'},{'A','B','C'},{'A','D','E','F'},{'D','F'},{'E','F'}]\n",
    "HG = hnx.Hypergraph(dict(enumerate(E)))\n",
    "hnx.draw(HG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dual hypergraph\n",
    "HD = HG.dual()\n",
    "hnx.draw(HD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-computing\n",
    "\n",
    "HNX hypergraphs have node and edge weights set to 1 by default if no other values are supplied.\n",
    "The hypergraph modularity code requires a few other quantities that we pre-compute for efficiency: node strength (sum of weight of incident edges; this is the same as degree if all edge weights are equal to 1) and d-weights (sum of weights of edges of size d for each value d appearing in the hypergraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute node strength (add unit weight if none) and a few other quantities useful to quickly compute modularity\n",
    "HG = hmod.precompute_attributes(HG)\n",
    "\n",
    "## show the nodes (here strength = degree since all weights are 1 by default)\n",
    "HG.nodes.elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show the edges (unit weights were added by default)\n",
    "HG.edges.elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## d-weights distribution; here there are edges of size 2, 3 and 4 only.\n",
    "HG.d_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hypergraph modularity qH\n",
    "\n",
    "We compute qH on the toy graph for 4 different partitions, and using 3 different variations for the edge contribution.\n",
    "\n",
    "For edges of size $d$ where $c$ is the number of nodes from the part with the most representatives, we consider three variations as follows for edge contribution:\n",
    "\n",
    "* **strict**: edges are considered only if all nodes are from the same part, with unit weight, i.e. $w$ = 1 iff $c == d$ (0 else).\n",
    "* **majority**: edges are counted only if more that half the nodes are from the same part, with unit weights, i.e. $w$ = 1 iff $c>d/2$ (0 else).\n",
    "* **linear**: edges are counted only if more that half the nodes are from the same part, with weights proportional to the number of nodes in the majority, i.e. $w = c/d$ iff $c>d/2$ (0 else).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute hypergraph modularity (qH) for the following partitions:\n",
    "A1 = [{'A','B','C'},{'D','E','F'}]           ## good clustering, qH should be positive\n",
    "A2 = [{'B','C'},{'A','D','E','F'}]           ## not so good\n",
    "A3 = [{'A','B','C','D','E','F'}]             ## this should yield qH == 0\n",
    "A4 = [{'A'},{'B'},{'C'},{'D'},{'E'},{'F'}]   ## qH should be negative here\n",
    "\n",
    "## we compute with 3 different choices of functions for the edge contribution: linear (default), strict and majority\n",
    "\n",
    "print('linear edge contribution:')\n",
    "print('qH(A1):',hmod.modularity(HG,A1),\n",
    "      'qH(A2):',hmod.modularity(HG,A2),\n",
    "      'qH(A3):',hmod.modularity(HG,A3),\n",
    "      'qH(A4):',hmod.modularity(HG,A4))\n",
    "print('strict edge contribution:')\n",
    "print('qH(A1):',hmod.modularity(HG,A1,hmod.strict),\n",
    "      'qH(A2):',hmod.modularity(HG,A2,hmod.strict),\n",
    "      'qH(A3):',hmod.modularity(HG,A3,hmod.strict),\n",
    "      'qH(A4):',hmod.modularity(HG,A4,hmod.strict))\n",
    "print('majority edge contribution:')\n",
    "print('qH(A1):',hmod.modularity(HG,A1,hmod.majority),\n",
    "      'qH(A2):',hmod.modularity(HG,A2,hmod.majority),\n",
    "      'qH(A3):',hmod.modularity(HG,A3,hmod.majority),\n",
    "      'qH(A4):',hmod.modularity(HG,A4,hmod.majority))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-section graph\n",
    "\n",
    "We build the 2-section graph for the toy hypergraph, and run graph lcustering (ECG) on this graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2-section graph\n",
    "G = hmod.two_section(HG)\n",
    "G.vs['label'] = G.vs['name']\n",
    "ig.plot(G,bbox=(0,0,250,250))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2-section clustering with ECG\n",
    "G.vs['community'] = G.community_ecg().membership\n",
    "hmod.dict2part({v['name']:v['community'] for v in G.vs})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game of Thrones scenes hypergraph\n",
    "\n",
    "The original data can be found here: https://github.com/jeffreylancaster/game-of-thrones.\n",
    "A pre-processed version is provided, where we consider an hypergraph from the game of thrones scenes with he following elements:\n",
    "\n",
    "* **Nodes** are named characters in the series\n",
    "* **Hyperedges** are groups of character appearing in the same scene(s)\n",
    "* **Hyperedge weights** are total scene(s) duration in seconds involving each group of characters\n",
    "\n",
    "We kept hyperedges with at least 2 characters and we discarded characters with degree below 5.\n",
    "\n",
    "We saved the following:\n",
    "\n",
    "* *Edges*: list of sets where the nodes are 0-based integers represented as strings: '0', '1', ... 'n-1'\n",
    "* *Names*: dictionary; mapping of nodes to character names\n",
    "* *Weights*: list; hyperedge weights (in same order as Edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the data\n",
    "with open(datadir+\"GoT/GoT.pkl\",\"rb\") as f:\n",
    "    Edges, Names, Weights = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build weighted hypergraph \n",
    "\n",
    "Use the above to build the weighted hypergraph (GoT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nodes are represented as strings from '0' to 'n-1'\n",
    "GoT = hnx.Hypergraph(dict(enumerate(Edges)))\n",
    "\n",
    "## add edge weights\n",
    "for e in GoT.edges:\n",
    "    GoT.edges[e].weight = Weights[e]\n",
    "\n",
    "## add full names of characters\n",
    "for v in GoT.nodes:\n",
    "    GoT.nodes[v].name = Names[v]\n",
    "\n",
    "## pre-compute required quantities for modularity and clustering\n",
    "GoT = hmod.precompute_attributes(GoT)\n",
    "\n",
    "print(GoT.number_of_nodes(),'nodes and',GoT.number_of_edges(),'edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example of a node (indices are strings)\n",
    "GoT.nodes['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example of an edge (indices are integers)\n",
    "GoT.edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to get the nodes for a given edge\n",
    "GoT.edges[0].elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## or just the keys\n",
    "GoT.edges[0].elements.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on GoT hypergraph\n",
    "\n",
    "Simple exploratory data analysis (EDA) on this hypergraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## edge sizes (number of characters per scene)\n",
    "plt.hist([GoT.edges[e].size() for e in GoT.edges], bins=25, color='grey')\n",
    "plt.xlabel(\"Edge size\",fontsize=14);\n",
    "#plt.savefig('got_hist_1.eps');\n",
    "## max edge size\n",
    "print('max = ',max([GoT.edges[e].size() for e in GoT.edges]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## edge weights (total scene durations for each group of characters appearing together)\n",
    "plt.hist([GoT.edges[e].weight for e in GoT.edges], bins=25, color='grey')\n",
    "plt.xlabel(\"Edge weight\",fontsize=14);\n",
    "#plt.savefig('got_hist_2.eps');\n",
    "## max edge weight\n",
    "print('max = ',max([GoT.edges[e].weight for e in GoT.edges]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## node degrees\n",
    "plt.hist(hnx.degree_dist(GoT),bins=20, color='grey')\n",
    "plt.xlabel(\"Node degree\",fontsize=14);\n",
    "#plt.savefig('got_hist_3.eps');\n",
    "## max degree\n",
    "print('max = ',max(hnx.degree_dist(GoT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## node strength (total appearance)\n",
    "plt.hist([GoT.nodes[n].strength for n in GoT.nodes], bins=20, color='grey')\n",
    "plt.xlabel(\"Node strength\",fontsize=14);\n",
    "#plt.savefig('got_hist_4.eps');\n",
    "## max strength\n",
    "print('max = ',max([GoT.nodes[n].strength for n in GoT.nodes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build a dataframe with node characteristics\n",
    "dg = [GoT.degree(v) for v in GoT.nodes()]\n",
    "st = [GoT.nodes[v].strength for v in GoT.nodes()]\n",
    "nm = [GoT.nodes[v].name for v in GoT.nodes()]\n",
    "D = pd.DataFrame(np.array([nm,dg,st]).transpose(),columns=['name','degree','strength'])\n",
    "D['degree'] = pd.to_numeric(D['degree'])\n",
    "D['strength'] = pd.to_numeric(D['strength'])\n",
    "\n",
    "## sort w.r.t. strength\n",
    "D.sort_values(by='strength',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort w.r.t. degree\n",
    "D.sort_values(by='degree',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## we see clear correlation between degree and strength\n",
    "plt.plot(D['degree'],D['strength'],'.')\n",
    "plt.xlabel('degree',fontsize=14)\n",
    "plt.ylabel('strength',fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build 2-section graph and compute a few centrality measures\n",
    "\n",
    "We saw several centrality measures for graphs in chapter 3. Below, we build the 2-section graph for GoT and compute a few of those.\n",
    "\n",
    "Node ordering should be preserved and we verify that it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build 2-section\n",
    "G = hmod.two_section(GoT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sanity check -- node ordering is the same in GoT and G\n",
    "\n",
    "## ordering of nodes in GoT\n",
    "ord_GoT = list(GoT.nodes.elements.keys())\n",
    "\n",
    "## ordering of nodes in G\n",
    "ord_G = [v['name'] for v in G.vs]\n",
    "\n",
    "ord_GoT == ord_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = G.betweenness(directed=False,weights='weight')\n",
    "n = G.vcount()\n",
    "D['betweenness'] = [2*x/((n-1)*(n-2)) for x in b]\n",
    "D['pagerank'] = G.pagerank(directed=False,weights='weight')\n",
    "\n",
    "## order w.r.t. betweenness\n",
    "D.sort_values(by='betweenness',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## order w.r.t. pagerank\n",
    "D.sort_values(by='pagerank',ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypergraph modularity and clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize the 2-section graph\n",
    "print('nodes:',G.vcount(),'edges:',G.ecount())\n",
    "G.vs['size'] = 10\n",
    "G.vs['color'] = 'lightgrey'\n",
    "G.vs['label'] = [int(x) for x in G.vs['name']] ## use int(name) as label\n",
    "G.vs['character'] = [GoT.nodes[n].name for n in G.vs['name']]\n",
    "G.vs['label_size'] = 5\n",
    "ly = G.layout_fruchterman_reingold()\n",
    "ig.plot(G, layout = ly, bbox=(0,0,600,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we see a well-separated small clique; it is the Braavosi theater troup\n",
    "print([GoT.nodes[str(x)].name for x in np.arange(166,173)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute modularity (qH) on several random partition with K parts for a range of K's\n",
    "## This should be close to 0 and can be negative.\n",
    "h = []\n",
    "for K in np.arange(2,21):\n",
    "    for rep in range(10):\n",
    "        V = list(GoT.nodes)\n",
    "        p = np.random.choice(K, size=len(V))\n",
    "        RandPart = hmod.dict2part({V[i]:p[i] for i in range(len(V))})\n",
    "        ## drop empty sets if any\n",
    "        RandPart = [x for x in RandPart if len(x)>0]\n",
    "        ## compute qH\n",
    "        h.append(hmod.modularity(GoT, RandPart))\n",
    "print('range for qH:',min(h),'to',max(h))\n",
    "plt.boxplot(h, showfliers=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cluster the 2-section graph (with Louvain) and compute qH\n",
    "## We now see qH >> 0\n",
    "G.vs['louvain'] = G.community_multilevel(weights='weight').membership\n",
    "D['cluster'] = G.vs['louvain']\n",
    "ML = hmod.dict2part({v['name']:v['louvain'] for v in G.vs})\n",
    "## Compute qH\n",
    "print(hmod.modularity(GoT, ML))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## plot 2-section w.r.t. the resulting clusters\n",
    "cl = G.vs['louvain']\n",
    "\n",
    "## pick greyscale or color plot:\n",
    "#pal = ig.GradientPalette(\"white\",\"black\",max(cl)+2)\n",
    "pal = ig.ClusterColoringPalette(max(cl)+1)\n",
    "\n",
    "G.vs['color'] = [pal[x] for x in cl]\n",
    "G.vs['label_size'] = 5\n",
    "ig.plot(G, layout = ly, bbox=(0,0,500,400))\n",
    "#ig.plot(G, target='GoT_clusters.eps', layout = ly, bbox=(0,0,400,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ex: high strength nodes in same cluster with Daenerys Targaryen\n",
    "dt = int(D[D['name']=='Daenerys Targaryen']['cluster'])\n",
    "D[D['cluster']==dt].sort_values(by='strength',ascending=False).head(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra material\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with simple random hypergraphs with communities\n",
    "\n",
    "Note: qH-based heuristics are still very experimental; we only provide this for illustration in **Section 7.4** of the book. Experiment results are stored in files taus_xx.pkl with xx in {00, 05, 10, 15}.\n",
    "\n",
    "For each experiment, we have results for:\n",
    "\n",
    "* 16 hypergraphs each with 1000 nodes, 1400 edges of size 2 to 8 (200 each)\n",
    "* 10 communities with 0%, 5%, 10% or 15% of \"noise\" edges ($\\mu$)\n",
    "* community edge homogeneity ($\\tau$) from 0.5 to 1\n",
    "* communities obtained via 3 algorithms:\n",
    " * qG-based Louvain on the 2-section graph\n",
    " * qH-based heuristic clustering algorithm on the hypergraph\n",
    " * qH+: same but using true homogeneity ($\\tau$)\n",
    "\n",
    "Recall that AMI = adjusted mutual information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load results (here mu = .05) and plot\n",
    "with open( datadir+\"Hypergraph/taus_05.pkl\", \"rb\" ) as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "R = pd.DataFrame(results,columns=['tau','Graph','Hypergraph','Hypergraph+']).groupby(by='tau').mean()\n",
    "t = [x for x in np.arange(.501,1,.025)]\n",
    "\n",
    "## color or greyscale\n",
    "pal = ig.GradientPalette(\"grey\",\"black\",3)\n",
    "#pal = ig.GradientPalette(\"red\",\"blue\",3)\n",
    "\n",
    "## plot\n",
    "plt.plot(t,R['Graph'],'o-',label='qG-based',color=pal[0])\n",
    "plt.plot(t,R['Hypergraph'],'o-',label='qH-based',color=pal[1])\n",
    "plt.plot(t,R['Hypergraph+'],'o-',label='qH-based (tuned)',color=pal[2])\n",
    "plt.xlabel(r'homogeneity ($\\tau$)',fontsize=14)\n",
    "plt.ylabel('AMI',fontsize=14)\n",
    "plt.legend();\n",
    "#plt.savefig('taus_05.eps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community hypergraphs\n",
    "\n",
    "We provide hyperedge list and communities for 3 random hypergraph with communities, namely:\n",
    "\n",
    "* edges_65, comm_65: hypergraph with $\\tau_e = \\lceil(d*0.65)\\rceil$ for all community edges of size $d$\n",
    "* edges_85, comm_85: hypergraph with $\\tau_e = \\lceil(d*0.85)\\rceil$ for all community edges of size $d$\n",
    "* edges_65_unif, comm_65_unif: hypergraph with $\\tau_e$ chosen uniformly from $\\{\\lceil(d*0.65)\\rceil,...,d\\}$ for all community edges of size $d$\n",
    "\n",
    "All have 1000 nodes, 1400 edges of size 2 to 8 (200 each) 10 communities and noise parameter $\\mu=0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the edge lists and communities\n",
    "with open(datadir+\"Hypergraph/hypergraphs.pkl\",\"rb\") as f:\n",
    "    (edges_65, comm_65, edges_85, comm_85, edges_65_unif, comm_65_unif) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the experiment below, we estimate the homogeneity parameter $\\tau$ via clustering on the 2-section graph and comare with the results we get using the true communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick one of the three hypergraphs\n",
    "comm = comm_65\n",
    "L = edges_65\n",
    "\n",
    "## build hypergraph\n",
    "HG = hnx.Hypergraph(dict(enumerate(L)))\n",
    "\n",
    "## compute P(homogeneity > $\\tau$) using the true communities\n",
    "x = []\n",
    "for e in L:\n",
    "    x.append(max([len(e.intersection(k)) for k in comm])/len(e))\n",
    "y = []\n",
    "for t in np.arange(.501,1,.025):\n",
    "    y.append(sum([i>t for i in x])/len(x))\n",
    "plt.plot(np.arange(.501,1,.025),y,'.-',color='grey',label='true communities')\n",
    "\n",
    "## same but using the communities obtained via Louvain algorithm on the 2-section graph\n",
    "G = hmod.two_section(HG)\n",
    "G.vs['louvain'] = G.community_multilevel(weights='weight').membership\n",
    "ML = hmod.dict2part({v['name']:v['louvain'] for v in G.vs})\n",
    "x = []\n",
    "for e in L:\n",
    "    x.append(max([len(e.intersection(k)) for k in ML])/len(e))\n",
    "y = []\n",
    "for t in np.arange(.501,1,.025):\n",
    "    y.append(sum([i>t for i in x])/len(x))\n",
    "plt.plot(np.arange(.501,1,.025),y,'.-',color='black',label='Louvain')\n",
    "\n",
    "## add grid and legend\n",
    "plt.grid()\n",
    "#plt.title(r'Estimating $\\tau$ from data',fontsize=14)\n",
    "plt.ylabel(r'Pr(homogeneity > $\\tau$)',fontsize=14)\n",
    "plt.xlabel(r'$\\tau$',fontsize=14)\n",
    "plt.legend()\n",
    "plt.ylim(0,1);\n",
    "#plt.savefig('tau_65.eps');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## distribution of edge homogeneity with all tau = 0.65\n",
    "## results vary in view of various edge sizes, nd some \"noise\" edges.\n",
    "x = []\n",
    "for e in edges_65:\n",
    "    x.append(max([len(e.intersection(k)) for k in comm_65])/len(e))\n",
    "plt.hist(x,bins='rice',color='grey');\n",
    "#plt.savefig('hist_65.eps');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## distribution of edge homogeneity with tau varying from 0.65 to 1\n",
    "## we see many more pure community edges in this case, as expected\n",
    "x = []\n",
    "for e in edges_65_unif:\n",
    "    x.append(max([len(e.intersection(k)) for k in comm_65_unif])/len(e))\n",
    "plt.hist(x, bins='rice',color='grey');\n",
    "#plt.savefig('hist_65_unif.eps');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motifs example \n",
    "\n",
    "Using HNX draw function to get patterns from Figure 7.1 in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## H1 pattern\n",
    "E = [{'A','B'},{'A','C'},{'A','D'},{'B','D'},{'C','D'}]\n",
    "HG = hnx.Hypergraph(dict(enumerate(E)))\n",
    "hnx.draw(HG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## H2 pattern\n",
    "E = [{'A','B','C'},{'A','D'},{'C','D'}]\n",
    "HG = hnx.Hypergraph(dict(enumerate(E)))\n",
    "hnx.draw(HG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## H3 pattern\n",
    "E = [{'A','B','C'},{'B','C','D'}]\n",
    "HG = hnx.Hypergraph(dict(enumerate(E)))\n",
    "hnx.draw(HG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Counting those patterns -- Table 7.2 in the book"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## This takes a while to run -- see some results in next cell\n",
    "from itertools import combinations\n",
    "\n",
    "## given:\n",
    "## E2: edges of size 2\n",
    "## G(E2): graph built only with E2\n",
    "## E3: edges of size 3\n",
    "## compute:\n",
    "## H1: number of subgraphs of 4-nodes in G(E2) with 5 edges + 6 times 4-cliques in G(E2)\n",
    "## H3: count pairs of edges in E3 with intersection of size 2\n",
    "## H2: for each (i,j,k) in E3, count common neighbours in G(E2) for (i,j), (i,k) and (j,k) \n",
    "\n",
    "n = 500\n",
    "V = [str(i) for i in range(n)]\n",
    "\n",
    "L = []\n",
    "REP = 16\n",
    "\n",
    "for c in np.arange(0,9):\n",
    "    p2 = c/(n-1)\n",
    "    p3 = (8-c)/((n-1)*(n-2))    ## maintain expected 2-section graph degree\n",
    "    #p3 = (8-c)/((n-1)*(n/2-1)) ## maintain expected H-degree\n",
    "    print(c)\n",
    "    for rep in range(REP):\n",
    "        E2 = []\n",
    "        E3 = []\n",
    "        \n",
    "        ## generate 2-edges\n",
    "        r = np.random.random(int(n*(n-1)/2))\n",
    "        v = combinations(V,2)\n",
    "        for i,j in enumerate(v):\n",
    "            if r[i] < p2:\n",
    "                E2.append(j)\n",
    "        ## generate 3-edges\n",
    "        r = np.random.random(int(n*(n-1)*(n-2)/6))\n",
    "        v = combinations(V,3)\n",
    "        for i,j in enumerate(v):\n",
    "            if r[i] < p3:\n",
    "                E3.append(j)\n",
    "\n",
    "        dg = 2*len(E2)+3*len(E3)\n",
    "        HG = hnx.Hypergraph(dict(enumerate(E2+E3)))\n",
    "        g = hmod.two_section(HG)\n",
    "        sd = g.ecount()\n",
    "\n",
    "        ## count motifs in graph G with 2-edges only\n",
    "        G = ig.Graph.TupleList(E2)\n",
    "        M = G.motifs_randesu(size=4)\n",
    "        H1 = M[9] + 6*M[10] ## exactly as H1 + 6 times 4-clique\n",
    "\n",
    "        ## H2: for each 3-edge, for each pair within, count common neighbor(s) in G\n",
    "        H2 = 0\n",
    "        for e in E3:\n",
    "            if len(set(G.vs['name']).intersection(set(e)))==3:\n",
    "                s1 = set(G.neighbors(G.vs.find(name=e[0])))\n",
    "                s2 = set(G.neighbors(G.vs.find(name=e[1])))\n",
    "                s3 = set(G.neighbors(G.vs.find(name=e[2])))\n",
    "                H2 += len(s1.intersection(s2))+len(s1.intersection(s3))+len(s3.intersection(s2))\n",
    "\n",
    "        ## H3: count pairs of 3-edges with intersection of size 2\n",
    "        H3 = 0\n",
    "        e = [set(i) for i in E3]\n",
    "        l = len(e)\n",
    "        for i in np.arange(0,l-1):\n",
    "            for j in np.arange(i+1,l):\n",
    "                if len(e[i].intersection(e[j]))==2:\n",
    "                    H3+=1\n",
    "        L.append([c,H1,H2,H3,dg/n,2*sd/n])\n",
    "        \n",
    "D = pd.DataFrame(L,columns=['c','H1','H2','H3','H deg','2-sec deg'])\n",
    "D.groupby(by='c').mean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### RESULTS ###\n",
    "\n",
    "## fixing expected 2-section degree:\n",
    "\n",
    "     H1\t     H2\t     H3\t     H-deg\t   2-sec-deg\n",
    "c\t\t\t\t\t\n",
    "0\t0.0000\t0.0000\t15.4375\t4.037625\t8.01375\n",
    "1\t0.0000\t2.1250\t11.5000\t4.509500\t7.96650\n",
    "2\t0.0000\t11.9375\t7.1875\t5.029125\t7.97825\n",
    "3\t0.1250\t21.6875\t6.8125\t5.549375\t8.03600\n",
    "4\t0.4375\t31.4375\t3.5625\t5.967875\t7.87300\n",
    "5\t1.5000\t37.9375\t1.7500\t6.485625\t7.90475\n",
    "6\t3.5000\t35.5625\t1.4375\t6.963125\t7.92875\n",
    "7\t9.0000\t24.0000\t0.3750\t7.509375\t7.99325\n",
    "8\t15.0625\t0.0000\t0.0000\t8.012750\t8.01275\n",
    "\n",
    "# fixing expected H-degree\n",
    "\n",
    "     H1\t     H2\t     H3\t     H-deg\t   2-sec-deg\n",
    "c\t\t\t\t\t\n",
    "0\t0.0000\t0.0000\t64.7500\t8.028750\t15.80150\n",
    "1\t0.0000\t3.9375\t47.6875\t8.091500\t14.94500\n",
    "2\t0.0625\t20.0625\t33.9375\t7.939125\t13.68550\n",
    "3\t0.0625\t43.5000\t23.6875\t7.979750\t12.81150\n",
    "4\t0.5000\t63.4375\t15.3750\t7.978500\t11.87000\n",
    "5\t1.3125\t72.5625\t9.1250\t7.951875\t10.84900\n",
    "6\t3.1250\t75.1250\t4.0000\t8.006500\t9.97400\n",
    "7\t8.3125\t52.0000\t0.6875\t8.040250\t9.04775\n",
    "8\t15.0625\t0.0000\t0.0000\t7.973000\t7.97300\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphmining",
   "language": "python",
   "name": "graphmining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
