{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9 - Embedding Graphs\n",
    "\n",
    "In this notebook, we experiment with the *graph2vec* algorithm to embed whole graphs. We illustrate this over two datasets from ther National Cancer Institute (NCI); each dataset contains labelled graph representations of chemical compounds; each compound is labelled as active (label = 1) or inactive (label = 0).\n",
    "\n",
    "We perform the following experiments:\n",
    "\n",
    "* supervised learning (binary classification)\n",
    "* graph-based feature extraction and feature importance\n",
    "* unsupervised learning (clustering)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "* download graph2vec code from (we used the May 2020 version): https://github.com/benedekrozemberczki/graph2vec/tree/be7fc2ac44706f9664b6636cea5df477e8a6bb06/src\n",
    "* the NCI1 and NCI109 datasets\n",
    "    \n",
    "Also set the path(s) in the cell below. \n",
    "\n",
    "NB: On Windows, there could be issues with the graph2vec code. One way to fix this is to change “/“ to “\\\\\\\\” on lines 63 and 97 of graph2vec.py  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick one of the two supplied datasets\n",
    "\n",
    "datadir = '../Datasets/NCI1/'\n",
    "#datadir = '../Datasets/NCI109/'\n",
    "\n",
    "## location of graph2vec python code\n",
    "g2v = '~/Tools/graph2vec/'\n",
    "\n",
    "## random state for reproducibility with train/test splits\n",
    "RS = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import umap, umap.plot\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier as rfc \n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets for graph2vec\n",
    "\n",
    "* save each graph in a json file, where the filenames are 0-based integers\n",
    "* each file contains a list of edges and dictionary of node features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read data from NCI folder\n",
    "edges = datadir+'DS_A.txt'\n",
    "graph_id = datadir+'DS_graph_indicator.txt'\n",
    "graph_label = datadir+'DS_graph_labels.txt'\n",
    "node_label = datadir+'DS_node_labels.txt'\n",
    "\n",
    "## read edges, build overall graph\n",
    "X = np.array(pd.read_csv(edges,header=None))\n",
    "E = [list(x) for x in X]\n",
    "G = ig.Graph.TupleList(E, directed=True)\n",
    "\n",
    "## read subgraph membership (1-based)\n",
    "m = [int(x) for x in np.array(pd.read_csv(graph_id,header=None))]\n",
    "\n",
    "## some vertices do not appear in any edges\n",
    "## we add those as isolated nodes for easier processing.\n",
    "## collect all vertex names \n",
    "vertices = set(G.vs['name'])\n",
    "## add difference\n",
    "v = set(np.arange(1,len(m)+1))\n",
    "diff = v.difference(vertices)\n",
    "G.add_vertices(list(diff))\n",
    "\n",
    "## parameters for plotting\n",
    "G.vs['size'] = 10\n",
    "G.vs['color'] = 'darkgrey'\n",
    "G.es['color'] = 'grey'\n",
    "G.es['arrow_size'] = .33\n",
    "G.vs['label_size'] = 8\n",
    "\n",
    "## mapping nodes in order of name\n",
    "idx = list(np.argsort(G.vs['name']))\n",
    "\n",
    "## assign subgraph\n",
    "for i in range(len(m)):\n",
    "    G.vs[idx[i]]['graph'] = m[i]\n",
    "\n",
    "## verify with graph label list -- should output 'True'\n",
    "l = [G.vs[i]['graph'] for i in idx]\n",
    "l == m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read node labels\n",
    "l = [int(x) for x in np.array(pd.read_csv(node_label,header=None))]\n",
    "for i in range(len(l)):\n",
    "    G.vs[idx[i]]['label'] = l[i]\n",
    "G = G.as_undirected()\n",
    "\n",
    "## read graph labels\n",
    "gl = [int(x) for x in np.array(pd.read_csv(graph_label,header=None))]\n",
    "\n",
    "## build the subgraphs and save json files in the data directory\n",
    "## this takes a few minutes (and some amount of disk space!)\n",
    "## we also count nodes and edges for EDA\n",
    "vc = []\n",
    "ec = []\n",
    "\n",
    "for gp in np.arange(1,np.max(G.vs['graph'])+1):\n",
    "    v = [v for v in G.vs if v['graph']==gp]\n",
    "    sg = G.subgraph(v)\n",
    "    vc.append(sg.vcount())\n",
    "    ec.append(sg.ecount())\n",
    "    sg_edges = [list(e.tuple) for e in sg.es]\n",
    "    sg_features = {str(v.index):str(v['label']) for v in sg.vs}\n",
    "    sg_json = {\"edges\":sg_edges,\"features\":sg_features}\n",
    "    fn = datadir+str(gp-1)+'.json'\n",
    "    with open(fn,'w') as fp:\n",
    "        json.dump(sg_json,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot number of nodes/edges for graphs with label 0 and 1 resp.\n",
    "a = [vc[i] for i in range(len(vc)) if gl[i]==0]\n",
    "b = [vc[i] for i in range(len(vc)) if gl[i]==1]\n",
    "c = [ec[i] for i in range(len(ec)) if gl[i]==0]\n",
    "d = [ec[i] for i in range(len(ec)) if gl[i]==1]\n",
    "\n",
    "plt.subplots(1,2,figsize=(9,4))\n",
    "plt.subplot(121)\n",
    "plt.boxplot([a,b],labels=['0','1'],widths=.6, \n",
    "            flierprops = dict(marker='.', markerfacecolor='black', markersize=3,linestyle='none'),\n",
    "            medianprops = dict(linestyle='-', linewidth=1.5, color='black'))\n",
    "plt.ylabel('Count per graph',fontsize=14);\n",
    "plt.xlabel('Label',fontsize=14)\n",
    "plt.title('Number of nodes',fontsize=14);\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.boxplot([c,d],labels=['0','1'],widths=.6, \n",
    "            flierprops = dict(marker='.', markerfacecolor='black', markersize=3,linestyle='none'),\n",
    "            medianprops = dict(linestyle='-', linewidth=1.5, color='black'))\n",
    "#plt.ylabel('Count per graph',fontsize=11)\n",
    "plt.xlabel('Label',fontsize=14);\n",
    "plt.title('Number of edges',fontsize=14);\n",
    "#plt.savefig('nci_counts.eps');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run graph2vec with 64 dimensions\n",
    "\n",
    "You may get the following warning:\n",
    "\n",
    "```\n",
    "graph2vec.py:104: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
    "  out.append([int(identifier)] + list(model.docvecs[\"g_\"+identifier]))\n",
    "```\n",
    "\n",
    "Changing docvec to dv on line 104 solves this issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run graph2vec with magic '%run'\n",
    "cmd = g2v+'graph2vec.py --input-path '+datadir+' --output-path '+datadir+\\\n",
    "'NCI64.csv --dimensions 64 --workers 1' \n",
    "%run $cmd\n",
    "\n",
    "## another option: run with os.system:\n",
    "#cmd = 'python '+g2v+'graph2vec.py --input-path '+datadir+' --output-path '+datadir+\\\n",
    "#'NCI64.csv --dimensions 64 --workers 1' \n",
    "#x = os.system(cmd+' >/dev/null 2>&1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification in embedded space\n",
    "\n",
    "* we use 80% for training, 20% for testing\n",
    "* we use a random forest classifiers with 100 trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## divide data into training and testing\n",
    "D = pd.read_csv(datadir+'NCI64.csv')\n",
    "D = np.array(D.drop(columns=['type']))\n",
    "X_train, X_test, y_train, y_test = train_test_split(D, gl, test_size=0.2, random_state=RS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random forest classifier -- accuracy\n",
    "rfc_mdl = rfc(n_estimators=100, criterion='entropy')\n",
    "rfc_mdl.fit(X_train,y_train)\n",
    "y_pred = rfc_mdl.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run graph2vec -- 1024 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run graph2vec with magic '%run'\n",
    "cmd = g2v+'graph2vec.py --input-path '+datadir+' --output-path '+datadir+\\\n",
    "'NCI1024.csv --dimensions 1024 --workers 1' \n",
    "%run $cmd\n",
    "\n",
    "## another option: run with os.system:\n",
    "#cmd = 'python '+g2v+'graph2vec.py --input-path '+datadir+' --output-path '+datadir+\\\n",
    "#'NCI1024.csv --dimensions 1024 --workers 1' \n",
    "#x = os.system(cmd+' >/dev/null 2>&1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize embeddings \n",
    "\n",
    "* we color w.r.t. graph label\n",
    "* we see a few outliers (what are those?), but in general, no clear class separation (but this is only 2D!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pd.read_csv(datadir+'NCI1024.csv')\n",
    "D = np.array(D.drop(columns=['type']))\n",
    "U = umap.UMAP(metric='cosine').fit(D)\n",
    "umap.plot.points(U,labels=np.array(gl),theme='fire',width=800,height=600);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification in embedded space\n",
    "\n",
    "* we use 80% for training, 20% for testing\n",
    "* we use random forest classifiers\n",
    "* we also try mapping from 1024 to 64 dimensions\n",
    "\n",
    "Possible project: \n",
    "* divide training set into training and validation \n",
    "* use this to select best model by trying several hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random forest classifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(D, gl, test_size=0.2, random_state=RS)\n",
    "rfc_mdl = rfc(n_estimators=100, criterion='entropy')\n",
    "rfc_mdl.fit(X_train,y_train)\n",
    "y_pred = rfc_mdl.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try in lower dimension -- same random_seed to get same split\n",
    "U = umap.UMAP(n_components=64, metric='cosine').fit(D).embedding_\n",
    "X_train, X_test, y_train, y_test = train_test_split(U, gl, test_size=0.2, random_state=RS)\n",
    "rfc_mdl = rfc(n_estimators=100, criterion='entropy')\n",
    "rfc_mdl.fit(X_train,y_train)\n",
    "y_pred = rfc_mdl.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROC curve\n",
    "y_probs = rfc_mdl.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_probs)\n",
    "auc = metrics.roc_auc_score(y_test, y_probs)\n",
    "plt.plot(fpr,tpr,label=\"ROC, auc=\"+str('%.3f' % auc),color='black')\n",
    "plt.plot([0,1],[0,1],'--',label='Random',color='black')\n",
    "plt.legend(loc=4,fontsize=14)\n",
    "plt.xlabel('False Positive Rate',fontsize=14)\n",
    "plt.ylabel('True Positive Rate',fontsize=14);\n",
    "#plt.savefig('nci_64d_roc.eps');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bootstrap confidence interval for the AUC\n",
    "def bootstrap(y_test,y_probs,n_boot=1000):\n",
    "    y_pred = np.array(y_probs)\n",
    "    y_true = np.array(y_test)\n",
    "    scores = []\n",
    "    rng = np.random.RandomState(RS)\n",
    "    for i in range(n_boot):\n",
    "        indices = rng.randint(0, len(y_pred), len(y_pred))\n",
    "        sc = roc_auc_score(y_true[indices], y_pred[indices])\n",
    "        scores.append(sc)\n",
    "    sorted_scores = np.array(scores)\n",
    "    sorted_scores.sort()\n",
    "    conf_lo = sorted_scores[int(0.05 * len(sorted_scores))]\n",
    "    conf_up = sorted_scores[int(0.95 * len(sorted_scores))]\n",
    "    return (conf_lo, conf_up)\n",
    "bootstrap(y_test,y_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subgraph features\n",
    "\n",
    "- we compute several statistics on the graphs\n",
    " - number of nodes, edges, density \n",
    " - degree distribution \n",
    " - number of components, size of the giant component\n",
    " - transitivity\n",
    " - degree assortativity\n",
    " - coreness distribution\n",
    " - node labels distribution\n",
    " \n",
    "- we compare those for the two classes of graphs\n",
    "- we re-try binary classification when adding those features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example of a graph with label 0\n",
    "gp = 800\n",
    "v = [v for v in G.vs if v['graph']==gp]\n",
    "sg = G.subgraph(v)\n",
    "print('label:',gl[gp-1])\n",
    "ig.plot(sg,bbox=(0,0,300,300))\n",
    "#ig.plot(sg,target='nci_0.eps',bbox=(0,0,300,300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## example of a graph with label 1\n",
    "gp = 2300\n",
    "v = [v for v in G.vs if v['graph']==gp]\n",
    "sg = G.subgraph(v)\n",
    "print('label:',gl[gp-1])\n",
    "ig.plot(sg,bbox=(0,0,300,300))\n",
    "#ig.plot(sg,target='nci_1.eps',bbox=(0,0,300,300))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build graph-based features for each compound (graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degrees(sg,md):\n",
    "    ctr = Counter(sg.degree())\n",
    "    return [ctr[i] for i in range(md+1)]\n",
    "\n",
    "def core(sg,mc):\n",
    "    ctr = Counter(sg.coreness())\n",
    "    return [ctr[i] for i in range(mc+1)]\n",
    "\n",
    "def labels(sg,ml):\n",
    "    ctr = Counter(sg.vs['label'])\n",
    "    return [ctr[i+1] for i in range(ml)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute features for each graph\n",
    "## this takes a few minutes as there are many graphs\n",
    "\n",
    "L = []\n",
    "md = np.max(G.degree())\n",
    "mc = np.max(G.coreness())\n",
    "ml = np.max(G.vs['label'])\n",
    "\n",
    "for gp in np.arange(1,np.max(G.vs['graph'])+1):\n",
    "    v = [v for v in G.vs if v['graph']==gp]\n",
    "    sg = G.subgraph(v)\n",
    "    ## node and edge counts, density\n",
    "    x = [sg.vcount(),sg.ecount(),sg.ecount()/sg.vcount()]\n",
    "    ## number of components, relative size of giant component\n",
    "    x.extend([np.max(sg.connected_components().membership)+1,sg.connected_components().giant().vcount()/sg.vcount()])\n",
    "    ## transitivity, assortativity\n",
    "    x.extend([sg.transitivity_undirected(),sg.transitivity_avglocal_undirected()])\n",
    "    ## assortativity\n",
    "    a = sg.assortativity_degree()\n",
    "    if np.isnan(a):\n",
    "        a=0\n",
    "    x.extend([a])\n",
    "    ## degree distribution\n",
    "    x.extend(degrees(sg,md))\n",
    "    ## coreness distribution\n",
    "    x.extend(core(sg,mc))\n",
    "    ## node labels distribution\n",
    "    x.extend(labels(sg,ml))\n",
    "    L.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## store all features in a dataframe\n",
    "col = ['nodes','edges','density','components','giant','trans','local trans','assort']\n",
    "col.extend(['deg'+str(i) for i in np.arange(md+1)])\n",
    "col.extend(['core'+str(i) for i in np.arange(mc+1)])\n",
    "col.extend(['label'+str(i+1) for i in np.arange(ml)])\n",
    "F = pd.DataFrame(L,columns=col)\n",
    "F['label'] = gl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EDA of the graphs\n",
    "print(F.shape[0],'graphs')\n",
    "print(sum(F['label']),'have label 1')\n",
    "print('avg nodes:',np.mean(F['nodes']))\n",
    "print('avg edges:',np.mean(F['edges']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add those features are re-do binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## binary classification with all features\n",
    "Fp = pd.concat([pd.DataFrame(U,columns=['emb_'+str(i) for i in np.arange(0,U.shape[1])]),F],axis=1)\n",
    "## divide data into training and testing\n",
    "A = np.array(Fp.drop(columns=['label']))\n",
    "X_train, X_test, y_train, y_test = train_test_split(A, gl, test_size=0.2, random_state=RS)\n",
    "features = list(Fp.columns) ## names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random forest classifier\n",
    "rfc_mdl = rfc(n_estimators=100, criterion='entropy')\n",
    "rfc_mdl.fit(X_train,y_train)\n",
    "y_pred = rfc_mdl.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROC curve\n",
    "y_probs = rfc_mdl.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_probs)\n",
    "auc = metrics.roc_auc_score(y_test, y_probs)\n",
    "plt.plot(fpr,tpr,label=\"ROC, auc=\"+str('%.3f' % auc),color='black')\n",
    "plt.plot([0,1],[0,1],'--',label='Random',color='black')\n",
    "plt.legend(loc=4,fontsize=14)\n",
    "plt.xlabel('False Positive Rate',fontsize=14)\n",
    "plt.ylabel('True Positive Rate',fontsize=14);\n",
    "#plt.savefig('nci_120d_roc.eps');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bootstrap confidence interval for AUC\n",
    "bootstrap(y_test,y_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature importance\n",
    "\n",
    "We can extract feature importance from random forest models\n",
    "\n",
    "For a few top features, we plot the distribution for graphs with label 0 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## top features\n",
    "x = np.argsort(rfc_mdl.feature_importances_)\n",
    "top = [x[i] for i in np.arange(len(x)-1,-1,-1)]\n",
    "[features[i] for i in top[:15]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## manually pick a few features to show\n",
    "f = ['deg3','emb_1']\n",
    "\n",
    "plt.subplots(1,2,figsize=(9,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "a = Fp[Fp['label']==0][f[0]]\n",
    "b = Fp[Fp['label']==1][f[0]]\n",
    "plt.boxplot([a,b],labels=['0','1'],widths=.6, \n",
    "            flierprops = dict(marker='.', markerfacecolor='black', markersize=3,linestyle='none'),\n",
    "            medianprops = dict(linestyle='-', linewidth=1.5, color='black'))\n",
    "plt.ylabel(f[0],fontsize=11)\n",
    "plt.xlabel('Label',fontsize=11);\n",
    "\n",
    "plt.subplot(122)\n",
    "a = Fp[Fp['label']==0][f[1]]\n",
    "b = Fp[Fp['label']==1][f[1]]\n",
    "plt.boxplot([a,b],labels=['0','1'],widths=.6, \n",
    "            flierprops = dict(marker='.', markerfacecolor='black', markersize=3,linestyle='none'),\n",
    "            medianprops = dict(linestyle='-', linewidth=1.5, color='black'))\n",
    "plt.ylabel(f[1],fontsize=11)\n",
    "plt.xlabel('Label',fontsize=11);\n",
    "#plt.savefig('nci_features.eps');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning\n",
    "\n",
    "We perform simple k-means clustering (fixing k to 10) and explore the content of the different clusters.\n",
    "\n",
    "We plot the proportion of graphs with label == 1 vs the size of the cluster.\n",
    "\n",
    "We see several small clusters with a large proportion of graphs with label == 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCL = 10 ## number of clusters\n",
    "from sklearn.cluster import KMeans\n",
    "cl = KMeans(n_clusters=NCL, n_init=10).fit(U).labels_\n",
    "K = pd.DataFrame(np.array([gl,cl,np.repeat(1,len(gl))]).transpose(),columns=['label=1','cluster','total'])\n",
    "C = K.groupby(by='cluster').sum()\n",
    "C['ratio'] = C['label=1']/C['total']\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(C['total'],C['ratio'],'o',color='black')\n",
    "plt.xlabel('cluster size', fontsize=14)\n",
    "plt.ylabel('proportion with label 1', fontsize=14);\n",
    "#plt.savefig('kmeans_1.eps');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## same data in a table\n",
    "df = C.sort_values(by='ratio',ascending=False).round(decimals=3)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Um = umap.UMAP(metric='cosine').fit(D)\n",
    "umap.plot.points(Um, labels=cl, width=800, height=600, theme='fire');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we typically find a small 'pure' cluster of size 10\n",
    "## look at this small cluster:\n",
    "CL = df.index[df['total'] == 10][0]\n",
    "df = F.loc[[i for i in range(len(cl)) if cl[i]==CL]][['nodes','edges','assort','deg1','deg2','deg3','label']]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot some of those -- should be similar\n",
    "gp = df.index[0] \n",
    "v = [v for v in G.vs if v['graph']==gp+1]\n",
    "sg = G.subgraph(v)\n",
    "print('label:',gl[gp])\n",
    "ig.plot(sg,bbox=(0,0,300,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## another one -- \n",
    "gp = df.index[1] \n",
    "v = [v for v in G.vs if v['graph']==gp+1]\n",
    "sg = G.subgraph(v)\n",
    "print('label:',gl[gp])\n",
    "ig.plot(sg,bbox=(0,0,300,300))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphmining",
   "language": "python",
   "name": "graphmining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
