{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 - Commmunity Detection\n",
    "\n",
    "In this notebook, we explore several algorithms to find communities in graphs.\n",
    "\n",
    "In some cells, we use the ABCD benchmark to generate synthetic graphs with communities. \n",
    "ABCD is written in Julia.\n",
    "\n",
    "### Installing Julia and ABCD\n",
    "\n",
    "We use the command line interface option to run ABCD below. \n",
    "The following steps are required:\n",
    "\n",
    "* install Julia (we used version 1.4.2) from https://julialang.org/downloads/\n",
    "* download ABCD from https://github.com/bkamins/ABCDGraphGenerator.jl\n",
    "* adjust the 'abcd_path' in the next cell to the location of the 'utils' subdirectory of ABCD\n",
    "* run 'julia abcd_path/install.jl' to install the required packages\n",
    "\n",
    "Also set the path(s) in the cell below. For Windows, you may need to use \"\\\\\" or \"\\\\\\\\\" as delimiters, for example 'C:\\ABCD\\utils\\\\\\\\'\n",
    "\n",
    "### Directories\n",
    "\n",
    "* Set the directories accordingly in the next cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set those accordingly\n",
    "datadir = '../Datasets/'\n",
    "abcd_path = '~/ABCD/utils/'\n",
    "julia = 'julia' ## you may need the full path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import Counter\n",
    "import os\n",
    "import umap\n",
    "import pickle\n",
    "import partition_igraph\n",
    "import subprocess\n",
    "from sklearn.metrics import adjusted_mutual_info_score as AMI\n",
    "\n",
    "## we used those for the book, but you can change to other colors\n",
    "cls_edges = 'gainsboro'\n",
    "cls = ['silver','dimgray','black']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zachary (karate) graph\n",
    "\n",
    "A small graph with 34 nodes and two \"ground-truth\" communities.\n",
    "Modularity-based algorithms will typically find 4 or 5 communities.\n",
    "In the next cells, we look at this small graph from several different angles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = ig.Graph.Famous('zachary')\n",
    "z.vs['size'] = 12\n",
    "z.vs['name'] = [str(i) for i in range(z.vcount())]\n",
    "z.vs['label'] = [str(i) for i in range(z.vcount())]\n",
    "z.vs['label_size'] = 8\n",
    "z.es['color'] = cls_edges\n",
    "z.vs['comm'] = [0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "z.vs['color'] = [cls[i] for i in z.vs['comm']]\n",
    "#ig.plot(z, 'zachary_gt.eps', bbox=(0,0,300,200))\n",
    "ig.plot(z, bbox=(0,0,350,250))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Roles\n",
    " \n",
    "We compute z(v) (normalized within module degree) and p(v) (participation coefficients) as defined in section 5.2 of the book for the Zachary graph. We identify 3 types of nodes, as described in the book.\n",
    "\n",
    "* provincial hubs\n",
    "* peripheral nodes (non-hubs)\n",
    "* ultra peripheral nodes (non-hubs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute internal degrees\n",
    "in_deg_0 = z.subgraph_edges([e for e in z.es if z.vs['comm'][e.tuple[0]]==0 and z.vs['comm'][e.tuple[1]]==0],\n",
    "                            delete_vertices=False).degree()\n",
    "in_deg_1 = z.subgraph_edges([e for e in z.es if z.vs['comm'][e.tuple[0]]==1 and z.vs['comm'][e.tuple[1]]==1],\n",
    "                            delete_vertices=False).degree()\n",
    "\n",
    "## compute z (normalized within-module degree)\n",
    "z.vs['in_deg'] = [in_deg_0[i] + in_deg_1[i] for i in range(z.vcount())]\n",
    "mu = [np.mean([x for x in in_deg_0 if x>0]),np.mean([x for x in in_deg_1 if x>0])]\n",
    "sig = [np.std([x for x in in_deg_0 if x>0],ddof=1),np.std([x for x in in_deg_1 if x>0],ddof=1)]\n",
    "z.vs['z'] = [(v['in_deg']-mu[v['comm']])/sig[v['comm']] for v in z.vs]\n",
    "\n",
    "## computing p (participation coefficient)\n",
    "z.vs['deg'] = z.degree()\n",
    "z.vs['out_deg'] = [v['deg'] - v['in_deg'] for v in z.vs]\n",
    "z.vs['p'] = [1-(v['in_deg']/v['deg'])**2-(v['out_deg']/v['deg'])**2 for v in z.vs]\n",
    "D = pd.DataFrame(np.array([z.vs['z'],z.vs['p']]).transpose(),columns=['z','p']).sort_values(by='z',ascending=False)\n",
    "D.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we plot the Zachary graph w.r.t. z where z>2.5 are hubs, which we show as square nodes.\n",
    "The largest values are for node 0 (instructor), node 33 (president) and node 32.\n",
    "Nodes 0 and 33 are the key nodes for the division of the group into two factions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zachary graph w.r.t. roles\n",
    "z.vs['color'] = 'black'\n",
    "z.vs['shape'] = 'circle'\n",
    "for v in z.vs:\n",
    "    if v['z']<2.5: ## non-hub\n",
    "        if v['p'] < .62 and v['p'] >= .05: ## peripheral\n",
    "            v['color'] = 'dimgrey'\n",
    "        if v['p'] < .05: ## ultra-peripheral\n",
    "            v['color'] = 'gainsboro'\n",
    "    if v['z']>=2.5 and v['p'] < .3: ## hubs (all provincial here)            \n",
    "        v['color'] = 'silver'\n",
    "        v['shape'] = 'square'\n",
    "#ig.plot(z, 'zachary_roles_1.eps', bbox=(0,0,350,250))\n",
    "ig.plot(z, bbox=(0,0,350,250))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below is to generate Figure 5.3(b) in the book, again comparing node roles in the Zachary graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Figure 5.3(b) -- comparing the roles\n",
    "fig, ax = plt.subplots(figsize=(12,9))\n",
    "ax.scatter(z.vs['p'],z.vs['z'],marker='o',s=75, color='k')\n",
    "\n",
    "plt.plot([0, .5], [2.5, 2.5], color='k', linestyle='-', linewidth=2)\n",
    "plt.plot([.05, .05], [-.5, 2.4], color='k', linestyle='-', linewidth=2)\n",
    "\n",
    "ax.annotate('node 0', (z.vs['p'][0],z.vs['z'][0]-.05), xytext=(z.vs['p'][0]+.01,z.vs['z'][0]-.3), \n",
    "            fontsize=14,\n",
    "            arrowprops = dict(  arrowstyle=\"-\",connectionstyle=\"angle3,angleA=0,angleB=-90\"))\n",
    "\n",
    "ax.annotate('node 33', (z.vs['p'][33],z.vs['z'][33]-.05), xytext=(z.vs['p'][33]-.07,z.vs['z'][33]-.3), \n",
    "            fontsize=14,\n",
    "            arrowprops = dict(  arrowstyle=\"-\",connectionstyle=\"angle3,angleA=0,angleB=-90\"))\n",
    "\n",
    "ax.annotate('node 32', (z.vs['p'][32]-.005,z.vs['z'][32]), xytext=(z.vs['p'][32]-.07,z.vs['z'][32]), \n",
    "            fontsize=14,\n",
    "            arrowprops = dict(  arrowstyle=\"-\",connectionstyle=\"angle3,angleA=0,angleB=-90\"))\n",
    "\n",
    "ax.annotate('node 1', (z.vs['p'][1],z.vs['z'][1]-.05), xytext=(z.vs['p'][1]-.07,z.vs['z'][1]-.3), \n",
    "            fontsize=14,\n",
    "            arrowprops = dict(  arrowstyle=\"-\",connectionstyle=\"angle3,angleA=0,angleB=-90\"))\n",
    "\n",
    "ax.annotate('node 3', (z.vs['p'][3],z.vs['z'][3]-.05), xytext=(z.vs['p'][3]+.07,z.vs['z'][3]-.3), \n",
    "            fontsize=14,\n",
    "            arrowprops = dict(  arrowstyle=\"-\",connectionstyle=\"angle3,angleA=0,angleB=-90\"))\n",
    "\n",
    "ax.annotate('node 2', (z.vs['p'][2],z.vs['z'][2]-.05), xytext=(z.vs['p'][2]-.07,z.vs['z'][2]-.3), \n",
    "            fontsize=14,\n",
    "            arrowprops = dict(  arrowstyle=\"-\",connectionstyle=\"angle3,angleA=0,angleB=-90\"))\n",
    "\n",
    "ax.annotate('provincial hubs',(.3,3), fontsize=18)\n",
    "ax.annotate('peripheral non-hubs',(.3,1.8), fontsize=18)\n",
    "ax.annotate('ultra peripheral non-hubs',(0.025,0.0),xytext=(.1,0), fontsize=18,\n",
    "             arrowprops = dict( arrowstyle=\"->\", connectionstyle=\"angle3,angleA=0,angleB=-90\"))\n",
    "\n",
    "plt.xlabel('participation coefficient (p)',fontsize=16)\n",
    "plt.ylabel('normalized within module degree (z)',fontsize=16);\n",
    "#plt.savefig('zachary_roles_2.eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strong and weak communities\n",
    "\n",
    "Communities are defined as strong or weak as per (5.1) and (5.2) in the book.\n",
    "For the Zachary graph, we verify if nodes within communities satisfy the strong criterion, then we verify is the two communities satisfy the weak definition.\n",
    "\n",
    "For the strong definition (internal degree larger than external degree for each node), only two nodes do not qualify.\n",
    "\n",
    "For the weak definition (total community internal degree > total community external degree), both communities satisfy this criterion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## strong criterion\n",
    "for i in range(z.vcount()):\n",
    "    c = z.vs[i]['comm']\n",
    "    n = [z.vs[v]['comm']==c for v in z.neighbors(i)]\n",
    "    if sum(n)<=len(n)-sum(n):\n",
    "        print('node',i,'has internal degree',sum(n),'external degree',len(n)-sum(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## weak criterion\n",
    "I = [0,0]\n",
    "E = [0,0]\n",
    "for i in range(z.vcount()):\n",
    "    c = z.vs[i]['comm']\n",
    "    n = [z.vs[v]['comm']==c for v in z.neighbors(i)]\n",
    "    I[c] += sum(n)\n",
    "    E[c] += len(n)-sum(n)\n",
    "print('community 0 internal degree',I[0],'external degree',E[0])\n",
    "print('community 1 internal degree',I[1],'external degree',E[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering and dendrogram\n",
    "\n",
    "Girvan-Newman algorithm is described in section 5.5 of the book. We apply it to the Zachary graph and show the results of this divisive algorithm as a dendrogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Girvan-Newman algorithm\n",
    "gn = z.community_edge_betweenness()\n",
    "#ig.plot(gn,'zachary_dendrogram.eps',bbox=(0,0,300,300))\n",
    "ig.plot(gn,bbox=(0,0,300,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a hierarchical clustering. In the next plot, we compute modularity for each possible cut of the dendrogram.\n",
    "\n",
    "We see that we get strong modularity with 2 clusters, but maximal value is obtained with 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute modularity at each possible cut and plot\n",
    "q = []\n",
    "for i in np.arange(z.vcount()):\n",
    "    q.append(z.modularity(gn.as_clustering(n=i+1)))\n",
    "plt.plot(np.arange(1,1+z.vcount()),q,'o-',color='black')\n",
    "plt.xlabel('number of clusters',fontsize=14)\n",
    "plt.ylabel('modularity',fontsize=14);\n",
    "#plt.savefig('zachary_modularity.eps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the nodes partitioned is we pick only 2 communities? How does this compare to the underlying ground truth?\n",
    "\n",
    "From the plot below, we see that only 1 node is misclassified.\n",
    "\n",
    "We also report the modularity of this partition, $q = 0.35996$. We also compare the partition with ground truth via AMI (adjusted mutual information), as defined in section 5.3 of the book; we got a high value AMI = 0.83276 showing  strong concordance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show result with 2 clusters -- \n",
    "z.vs['gn'] = gn.as_clustering(n=2).membership\n",
    "print('AMI:',AMI(z.vs['comm'],z.vs['gn']))  ## adjusted mutual information\n",
    "print('q:',z.modularity(z.vs['gn']))        ## modularity\n",
    "\n",
    "z.vs['size'] = 10\n",
    "z.vs['name'] = [str(i) for i in range(z.vcount())]\n",
    "z.vs['label'] = [str(i) for i in range(z.vcount())]\n",
    "z.vs['label_size'] = 8\n",
    "z.es['color'] = cls_edges\n",
    "z.vs['comm'] = [0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "#z.vs['color'] = [cls[i] for i in z.vs['comm']]\n",
    "z.vs['color'] = [cls[i] for i in z.vs['gn']]\n",
    "#ig.plot(z, 'zachary_2.eps',bbox=(0,0,300,200))\n",
    "ig.plot(z,bbox=(0,0,300,200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above with 5 communities. We see higher modularity, but weaker AMI value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show result with optimal modularity (5 clusters)\n",
    "z.vs['label'] = gn.as_clustering(n=5).membership\n",
    "print('AMI:',AMI(z.vs['comm'],z.vs['label']))\n",
    "print('q:',z.modularity(z.vs['label']))\n",
    "z.vs['color'] = [cls[i] for i in z.vs['comm']]\n",
    "z.vs['size'] = 10\n",
    "z.vs['label_size'] = 8\n",
    "#ig.plot(z, 'zachary_5.eps',bbox=(0,0,300,200))\n",
    "ig.plot(z,bbox=(0,0,300,200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABCD graph with 100 nodes\n",
    "\n",
    "Next we look at a slightly larger graph generated with the ABCD benchmark model, which is described in section 5.3 of the book. This graph has 3 communities. \n",
    "Using hierarchical clustering, we compare modularity and AMI for each possible cut.\n",
    "\n",
    "ABCD parameters used to generate this graph are: $\\gamma=3, \\tau=2$, degree range [5,15], community size range [25,50], $\\xi=.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read graph and communities; plot\n",
    "g = ig.Graph.Read_Ncol(datadir+'ABCD/abcd_100.dat',directed=False)\n",
    "c = np.loadtxt(datadir+'ABCD/abcd_100_comms.dat',dtype='uint16',usecols=(1))\n",
    "g.vs['comm'] = [c[int(x['name'])-1]-1 for x in g.vs]\n",
    "gt = {k:(v-1) for k,v in enumerate(g.vs['comm'])}\n",
    "## map between int(name) to key\n",
    "n2k = {int(v):k for k,v in enumerate(g.vs['name'])}\n",
    "g.vs['size'] = 7\n",
    "g.es['color'] = cls_edges\n",
    "g.vs['color'] = [cls[i] for i in g.vs['comm']]\n",
    "ig.plot(g, bbox=(0,0,300,200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Girvan-Newman algorithm -- Modularity and AMI for each cut\n",
    "\n",
    "In this case, both modularity and AMI are maximized with 3 communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = []\n",
    "a = []\n",
    "gn = g.community_edge_betweenness()\n",
    "for i in np.arange(g.vcount()):\n",
    "    q.append(g.modularity(gn.as_clustering(n=i+1)))\n",
    "    a.append(AMI(g.vs['comm'],gn.as_clustering(n=i+1).membership))\n",
    "plt.plot(np.arange(1,1+g.vcount()),q,'.-',color='black',label='modularity')\n",
    "plt.plot(np.arange(1,1+g.vcount()),a,'.-',color='grey',label='AMI')\n",
    "plt.xlabel('number of clusters',fontsize=14)\n",
    "plt.ylabel('modularity or AMI',fontsize=14)\n",
    "plt.legend();\n",
    "#plt.savefig('abcd_dendrogram.eps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with 3 communities, $q=0.502$ and AMI=1, so perfect recovery.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comm = np.arange(1,g.vcount()+1)\n",
    "D = pd.DataFrame(np.array([n_comm,q,a]).transpose(),columns=['n_comm','q','AMI'])\n",
    "df = D.head()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would we get with 4 clusters, for which AMI = 0.95?\n",
    "We see below that we have a few nodes splitted from one community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4 communities\n",
    "g.vs['gn'] = gn.as_clustering(n=4).membership\n",
    "cls = ['silver','dimgray','black','white']\n",
    "g.vs['color'] = [cls[i] for i in g.vs['gn']]\n",
    "#ig.plot(g, 'abcd_4.eps', bbox=(0,0,300,200))\n",
    "ig.plot(g, bbox=(0,0,300,200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those nodes form a triangle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = g.subgraph([v for v in g.vs() if v['gn']==3])\n",
    "ig.plot(sg, bbox=(0,0,100,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABCD with varying $\\xi$\n",
    "\n",
    "Here we show a typical way to compare graph clustering algorithms using benchmark graphs. \n",
    "We pick some model, here ABCD, and we vary the noise parameter $\\xi$. \n",
    "With ABCD, the larger $\\xi$ is, the closer we are to a random Chung-Lu or configuration model graph (i.e. where only the degree distribution matters). For $\\xi=0$, we get pure communities (all edges are internal).\n",
    "\n",
    "For each choice of $\\xi$, we generate 30 graphs, apply several different clustering algorithms,\n",
    "and compute AMI for each algorithm, comparing with griund-truth communities.\n",
    "\n",
    "The code below is commented out as it can take a while to run; a pickle file with results is included in the Data directory. To re-run from scratch, uncomment the cell below.\n",
    "\n",
    "Parameters for the ABCD benchmark graphs are:\n",
    "\n",
    "$\\gamma=2.5, \\tau=1.5$, degree range [10,50], community size range [50,100], $0.1 \\le \\xi \\le 0.8$."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## This can take a while in view of the number of runs, mainly due to the clustering algorithms\n",
    "## You can reduce the number of repeats (REP) for a faster test\n",
    "\n",
    "L = []   ## store results in a list\n",
    "REP = 30  ## number of graphs for each value of xi\n",
    "XIs = [x/100 for x in np.arange(10,81,5)] ## values for xi\n",
    "\n",
    "for rep in range(REP):\n",
    "    print('repeat number',rep)\n",
    "    ## generate new degree and community size values\n",
    "    cmd = 'julia '+abcd_path+'deg_sampler.jl deg.dat 2.5 10 50 1000 1000'\n",
    "    os.system(cmd)\n",
    "    cmd = 'julia '+abcd_path+'com_sampler.jl cs.dat 1.5 50 100 1000 1000'\n",
    "    os.system(cmd)\n",
    "    ## generate graphs for a range of xi \n",
    "    for xi in XIs:\n",
    "        cmd = 'julia '+abcd_path+'graph_sampler.jl net.dat comm.dat deg.dat cs.dat xi '\\\n",
    "                +str(xi)+' false false'\n",
    "        os.system(cmd)\n",
    "        ## compute AMI for various clustering algorithms\n",
    "        g = ig.Graph.Read_Ncol('net.dat',directed=False)\n",
    "        c = np.loadtxt('comm.dat',dtype='uint16',usecols=(1))\n",
    "        g.vs['comm'] = [c[int(x['name'])-1] for x in g.vs]\n",
    "        ## clustering\n",
    "        L.append(['ECG',xi,AMI(g.community_ecg().membership,g.vs['comm'])])\n",
    "        L.append(['Louvain',xi,AMI(g.community_multilevel().membership,g.vs['comm'])])\n",
    "        L.append(['Leiden',xi,AMI(g.community_leiden(objective_function='modularity').membership,g.vs['comm'])])\n",
    "        L.append(['Infomap',xi,AMI(g.community_infomap().membership,g.vs['comm'])])\n",
    "        L.append(['Label Prop.',xi,AMI(g.community_label_propagation().membership,g.vs['comm'])])\n",
    "# save results (list L)\n",
    "#with open( datadir+\"ABCD/abcd_study.pkl\", \"wb\" ) as f:\n",
    "#    pickle.dump(L, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data generated with the code from above cell \n",
    "with open(datadir+\"ABCD/abcd_study.pkl\",\"rb\") as f:\n",
    "    L = pickle.load(f)\n",
    "## store in dataframe and take averages\n",
    "D = pd.DataFrame(L,columns=['algo','xi','AMI'])\n",
    "## take average over 30 runs for each algorithm and every choice of xi\n",
    "X = D.groupby(by=['algo','xi']).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the results in the following 2 cells. \n",
    "We see good results with Louvain and Infomap, and even better results with ECG.\n",
    "Label propagation is a fast algortihm, but it does collapse with moderate to high level of noise.\n",
    "\n",
    "From the standard deviation plot, we see high variability around the value(s) for $\\xi$ where the different\n",
    "algorithms start to collapse. We see that this happen later and at a smaller scale with EGC, which is known to have better stability.\n",
    "\n",
    "Such studies are useful to compare algorithms; using benchmarks, we can directly control parameters such as the noise level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## plot average results foe each algorithm over range of xi\n",
    "a = ['ECG','Louvain','Infomap','Label Prop.']\n",
    "lt = ['-','--',':','-.','-.']\n",
    "cl = ['blue','green','purple','red']\n",
    "for i in range(len(a)):\n",
    "    ## pick one - color or greyscale\n",
    "    plt.plot(X.loc[(a[i])].index,X.loc[(a[i])],lt[i],label=a[i],color=cl[i])\n",
    "    #plt.plot(X.loc[(a[i])].index,X.loc[(a[i])],lt[i],label=a[i],color='black')\n",
    "plt.xlabel(r'ABCD noise ($\\xi$)',fontsize=14)\n",
    "plt.ylabel('AMI',fontsize=14)\n",
    "plt.legend();\n",
    "#plt.savefig('abcd_study.eps');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Look at standard deviations\n",
    "S = D.groupby(by=['algo','xi']).std()\n",
    "a = ['ECG','Louvain','Infomap','Label Prop.']\n",
    "#a = ['ECG','Louvain','Infomap','Label Prop.','Leiden','CNM']\n",
    "lt = ['-','--',':','-.','--',':']\n",
    "cl = ['blue','green','purple','red','red','blue']\n",
    "for i in range(len(a)):\n",
    "    ## pick one - color of greyscale\n",
    "    plt.plot(S.loc[(a[i])].index,S.loc[(a[i])],lt[i],label=a[i],color=cl[i])\n",
    "    #plt.plot(S.loc[(a[i])].index,S.loc[(a[i])],lt[i],label=a[i],color='black')\n",
    "plt.xlabel(r'ABCD noise ($\\xi$)',fontsize=14)\n",
    "plt.ylabel('Standard Deviation (AMI)',fontsize=14)\n",
    "plt.legend();\n",
    "#plt.savefig('abcd_study_stdv.eps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare stability \n",
    "\n",
    "This study is similar to the previous one, but we compare pairs of partitions for each algorithm on the same graph instead of comparing with the ground truth, so we look at the stability of algorithms. Note that an algorithm can be stable, but still be bad (ex: always cluster all nodes in a single community).\n",
    "\n",
    "The code below can take a while to run; a pickle file with results is included in the Data directory. To re-run from scratch, uncomment the cell below.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## This can take a while in view of the number of runs\n",
    "## You can reduce the number of repeats (REP) for a faster test\n",
    "\n",
    "Ls = []   ## store results in a list\n",
    "REP = 30  ## number of graphs for each value of xi\n",
    "XIs = [x/100 for x in np.arange(10,81,5)] ## values for xi\n",
    "\n",
    "for rep in range(REP):\n",
    "    print('repeat number',rep)\n",
    "    ## generate new degree and community size values\n",
    "    cmd = 'julia '+abcd_path+'deg_sampler.jl deg.dat 2.5 10 50 1000 1000'\n",
    "    os.system(cmd)\n",
    "    cmd = 'julia '+abcd_path+'com_sampler.jl cs.dat 1.5 50 100 1000 1000'\n",
    "    os.system(cmd)\n",
    "    ## generate graphs for a range of xi \n",
    "    for xi in XIs:\n",
    "        cmd = 'julia '+abcd_path+'graph_sampler.jl net.dat comm.dat deg.dat cs.dat xi '\\\n",
    "                +str(xi)+' false false'\n",
    "        os.system(cmd)\n",
    "        ## compute AMI running various clustering algorithms twice\n",
    "        g = ig.Graph.Read_Ncol('net.dat',directed=False)\n",
    "        ml1 = g.community_multilevel().membership\n",
    "        ec1 = g.community_ecg().membership\n",
    "        lp1 = g.community_label_propagation().membership\n",
    "        im1 = g.community_infomap().membership\n",
    "        ## permute vertices\n",
    "        idx = np.random.permutation(g.vcount())\n",
    "        gp = ig.Graph.Erdos_Renyi(n=g.vcount(),p=0)\n",
    "        for e in g.es():\n",
    "            gp.add_edge(idx[e.tuple[0]],idx[e.tuple[1]])\n",
    "        ml = gp.community_multilevel().membership\n",
    "        ml2 = [ml[idx[i]] for i in range(len(idx))]\n",
    "        ec = gp.community_ecg().membership\n",
    "        ec2 = [ml[idx[i]] for i in range(len(idx))]\n",
    "        lp = gp.community_label_propagation().membership\n",
    "        lp2 = [lp[idx[i]] for i in range(len(idx))]\n",
    "        im = gp.community_infomap().membership\n",
    "        im2 = [im[idx[i]] for i in range(len(idx))]        \n",
    "        Ls.append(['ECG',xi,AMI(ec1,ec2)])\n",
    "        Ls.append(['Louvain',xi,AMI(ml1,ml2)])\n",
    "        Ls.append(['Label Prop.',xi,AMI(lp1,lp2)])\n",
    "        Ls.append(['Infomap',xi,AMI(im1,im2)])\n",
    "\n",
    "# save results (list Ls)\n",
    "#with open( datadir+\"ABCD/abcd_study_stability.pkl\", \"wb\" ) as f:\n",
    "#    pickle.dump(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load L and train/val/test ids\n",
    "with open(datadir+\"ABCD/abcd_study_stability.pkl\",\"rb\") as f:\n",
    "    Ls = pickle.load(f)\n",
    "## store in dataframe \n",
    "D = pd.DataFrame(Ls,columns=['algo','xi','AMI'])\n",
    "## take averages for each algorithm and each noise value xi\n",
    "X = D.groupby(by=['algo','xi']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the results below. The behaviour of algorithms can be clustered in two groups:\n",
    "\n",
    "* For Louvain and ECG, stability is excellent and degrades gradually for high noise level, with ECG being the more stable algorithm.\n",
    "* For Infomap and Label Propagation, stability is also good until the noise value where the results start to degrade, as we saw in the previous study. We see near perfect stability for very high noise values; those are values where the results were very bad in the previous study; this typically happens when the algorithm can't get any good clustering and returns some trivial parititon, such as putting all nodes together in the same community, thus a stable but bad result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['ECG','Louvain','Infomap','Label Prop.']\n",
    "lt = ['-','--',':','-.']\n",
    "for i in range(len(a)):\n",
    "    plt.plot(X.loc[(a[i])].index,X.loc[(a[i])],lt[i],label=a[i],color='black')\n",
    "plt.xlabel(r'ABCD noise ($\\xi$)',fontsize=14)\n",
    "plt.ylabel('AMI between successive runs',fontsize=14)\n",
    "plt.legend();\n",
    "#plt.savefig('abcd_study_stability.eps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularity, resolution limit and rings of cliques\n",
    "\n",
    "We illustrate issues with modularity with the famous ring of cliques examples.\n",
    "\n",
    "For example below, we have a ring of 3-cliques connected ny a single (inter-clique) edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## n cliques of size s\n",
    "def ringOfCliques(n,s):\n",
    "    roc = ig.Graph.Erdos_Renyi(n=n*s,p=0)\n",
    "    ## cliques\n",
    "    for i in range(n):\n",
    "        for j in np.arange(s*i,s*(i+1)):\n",
    "            for k in np.arange(j+1,s*(i+1)):\n",
    "                roc.add_edge(j,k)\n",
    "    ## ring\n",
    "    for i in range(n):\n",
    "        if i>0:\n",
    "            roc.add_edge(s*i-1,s*i)\n",
    "        else:\n",
    "            roc.add_edge(n*s-1,0)\n",
    "    roc.vs['size'] = 8\n",
    "    roc.vs['color'] = cls[2]\n",
    "    roc.es['color'] = cls_edges\n",
    "    return roc\n",
    "\n",
    "## Ex: 10 3-cliques\n",
    "roc = ringOfCliques(10,3)\n",
    "#ig.plot(roc,'ring_3.eps',bbox=(0,0,300,300))     \n",
    "ig.plot(roc,bbox=(0,0,300,300))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the number of cliques (the natural parts in a partition) with the actual number of communities found via 3 modularity based algorithms (Louvain, CNM, ECG).\n",
    "\n",
    "We see that both Louvain and CNM return a smaller number of communities than the number of cliques; this is a known problem with modularity: merging cliques in the same community often lead to higher modularity.\n",
    "\n",
    "A concensus algorithm like ECG can help a lot in such cases; here we see that the cliques are correctly recovered with ECG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare number of cliques and number of clusters found\n",
    "L = []\n",
    "s = 3\n",
    "for n in np.arange(3,50,3):\n",
    "    roc = ringOfCliques(n,s)\n",
    "    ml = np.max(roc.community_multilevel().membership)+1\n",
    "    ec = np.max(roc.community_ecg().membership)+1\n",
    "    cnm = np.max(roc.community_fastgreedy().as_clustering().membership)+1\n",
    "    L.append([n,ml,ec,cnm])\n",
    "D = pd.DataFrame(L,columns=['n','Louvain','ECG','CNM'])\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(D['n'],D['Louvain'],'--o',color='black',label='Louvain')\n",
    "plt.plot(D['n'],D['ECG'],'-o',color='black',label='ECG')\n",
    "plt.plot(D['n'],D['CNM'],':o',color='black',label='CNM')\n",
    "\n",
    "plt.xlabel('number of '+str(s)+'-cliques',fontsize=14)\n",
    "plt.ylabel('number of clusters found',fontsize=14)\n",
    "plt.legend(fontsize=14);\n",
    "#plt.savefig('rings.eps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at a specific example: 10 cliques of size 3. Below we plot the communities found with Louvain; we clearly see that pairs of communities are systematically grouped into clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Louvain communities with 10 3-cliques\n",
    "roc = ringOfCliques(n=10,s=3)\n",
    "roc.vs['ml'] = roc.community_multilevel().membership\n",
    "roc.vs['color'] = [cls[x%3] for x in roc.vs['ml']]\n",
    "#ig.plot(roc,'ring_3_q.eps', bbox=(0,0,300,300))\n",
    "ig.plot(roc,bbox=(0,0,300,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is ECG solving this problem? It is due to the first step, where we run an ensemble of level-1 Louvain and assign new weights to edges based on the proportion of times those edges are internal to a community.\n",
    "We see below that there are exactly 30 edges with maximal edge weight of 1 (edges within cliques) and 10 edges with default minimal weight of 0.05 (edges between cliques). \n",
    "\n",
    "With those new weights, the last clustering in ECG can easily recover the cliques as communities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ECG weights in this case: all 30 clique edges have max score\n",
    "roc.es['W'] = roc.community_ecg().W\n",
    "Counter(roc.es['W'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ego nets and more\n",
    "\n",
    "Suppose we want to look at node \"near\" some seed node $v$. One common way to do this is to look at its ego-net, i.e. the subgraph consisting of node $v$ and all other nodes that can be reached from $v$ in $k$ hops or less, where $k$ is small, typically 1 or 2. \n",
    "\n",
    "Such subgraphs can become large quickly as we increase $k$. In the cells below, we look at ego-nets and compare with another approach to extract subgraph(s) around $v$ via clustering.\n",
    "\n",
    "We consider the airport graph we already saw several times. We consider a simple, undirected version (no loops, directions or edge weights).\n",
    "\n",
    "We compare ego-nets (1 and 2-hops subgraphs from a given node) with clusters obtained via graph clustering for some vertex $v$ with degree 11 (you can try other vertices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read edges and build simple undirected graph\n",
    "D = pd.read_csv(datadir+'Airports/connections.csv')\n",
    "g = ig.Graph.TupleList([tuple(x) for x in D.values], directed=True, edge_attrs=['weight'])\n",
    "#df = D.head()\n",
    "g = g.as_undirected()\n",
    "g = g.simplify()\n",
    "\n",
    "## read vertex attributes and add to graph\n",
    "A = pd.read_csv(datadir+'Airports/airports_loc.csv')\n",
    "lookup = {k:v for v,k in enumerate(A['airport'])}\n",
    "l = [lookup[x] for x in g.vs()['name']]\n",
    "g.vs()['layout'] = [(A['lon'][i],A['lat'][i]) for i in l]\n",
    "g.vs()['state'] = [A['state'][i] for i in l]\n",
    "g.vs()['city'] = [A['city'][i] for i in l]\n",
    "## add a few more attributes for visualization\n",
    "g.vs()['size'] = 6\n",
    "g.vs()['color'] = cls[0]\n",
    "g.es()['color'] = cls_edges\n",
    "df = A.head()\n",
    "\n",
    "## pick a vertex v\n",
    "v = 207\n",
    "print(g.vs[v])\n",
    "print('degree:',g.degree()[v])\n",
    "g.vs[v]['color'] = 'black'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show its ego-net for k=1 (vertex v in black)\n",
    "sg = g.subgraph([i for i in g.neighborhood(v,order=1)])\n",
    "print(sg.vcount(),'nodes')\n",
    "#ig.plot(sg,'airport_ego_1.eps',bbox=(0,0,300,300))\n",
    "ig.plot(sg,bbox=(0,0,300,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show its 2-hops ego-net ... this is already quite large!\n",
    "sg = g.subgraph([i for i in g.neighborhood(v,order=2)])\n",
    "sg.vs()['core'] = sg.coreness()\n",
    "sg.delete_vertices([v for v in sg.vs if v['core']<2])\n",
    "print(sg.vcount(),'nodes')\n",
    "#ig.plot(sg,'airport_ego_2.eps',bbox=(0,0,300,300))\n",
    "ig.plot(sg,bbox=(0,0,300,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply clustering and show the cluster containing the selected vertex\n",
    "## recall that we ignore edge weights\n",
    "## This result can vary somehow between runs\n",
    "ec = g.community_ecg(ens_size=16)\n",
    "g.es['W'] = ec.W\n",
    "m = ec.membership[v]\n",
    "sg = g.subgraph([i for i in range(g.vcount()) if ec.membership[i]==m])\n",
    "sg.vs()['core'] = sg.coreness()\n",
    "## display the 2-core\n",
    "sg.delete_vertices([v for v in sg.vs if v['core']<2])\n",
    "print(sg.vcount(),'nodes')\n",
    "#ig.plot(sg,'airport_ecg.eps',bbox=(0,0,300,300))\n",
    "ig.plot(sg,bbox=(0,0,300,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above that looking at the cluster with $v$ is smaller than the 2-hops ego-net, and several nodes are tightly connected.\n",
    "\n",
    "Below we go further and look at the ECG edge weights, which we can use to prune the graph above, so we can look at the nodes most tightly connected to node $v$.\n",
    "\n",
    "You can adjust the threshold below to get different zoomings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter edges w.r.t. ECG votes (weights)\n",
    "thresh = .85\n",
    "\n",
    "tmp = sg.subgraph_edges([e for e in sg.es if e['W'] > thresh])\n",
    "n = [i for i in range(tmp.vcount()) if tmp.vs[i]['color']=='black'][0]\n",
    "tmp.vs['cl'] = tmp.connected_components().membership\n",
    "cl = tmp.vs[n]['cl']\n",
    "ssg = tmp.subgraph([i for i in tmp.vs if i['cl']==cl])\n",
    "ssg.vs()['core'] = ssg.coreness()\n",
    "ssg.delete_vertices([v for v in ssg.vs if v['core']<2])\n",
    "print(ssg.vcount(),'nodes')\n",
    "#ig.plot(ssg,'airport_ecg_focus.eps',bbox=(0,0,300,300))\n",
    "ig.plot(ssg,bbox=(0,0,300,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most nodes in this subgraph are from the same state as node $v$ (MI) or nearby state (WI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## states in the above subgraph\n",
    "Counter(ssg.vs['state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA CODE\n",
    "\n",
    "The code below requires that Julia and ABCD are installed.\n",
    "\n",
    "This is extra material not in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABCD Properties\n",
    "\n",
    "The cells below are for illustration purpose only, to show some ABCD graphs with different $\\xi$ (noise) parameters,\n",
    "and to show how you can run ABCD with Julia installed. \n",
    "\n",
    "* notice the density of edges between communities as $\\xi$ increases.\n",
    "* most runs should yield 3 communities\n",
    "\n",
    "Natural layouts for noisy graphs make it hard to distinguish communities, as the nodes will overlap a lot.\n",
    "We use an ad-hoc method to \"push away\" nodes from the 3 different clusters to allow for better visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## just for visualization -- push the layout apart given 3 communities\n",
    "## adjust the 'push' factor with d\n",
    "def push_layout(d=0):\n",
    "    if np.max(g.vs['comm'])>2:\n",
    "        return -1\n",
    "    ly = g.layout()\n",
    "    g.vs['ly'] = ly\n",
    "    x = [0,0,0]\n",
    "    y = [0,0,0]\n",
    "    for v in g.vs:\n",
    "        c = v['comm']\n",
    "        x[c] += v['ly'][0]\n",
    "        y[c] += v['ly'][1]\n",
    "    delta = [-d,0,d]\n",
    "    dx = [delta[i] for i in np.argsort(x)]\n",
    "    dy = [delta[i] for i in np.argsort(y)]\n",
    "    for v in g.vs:\n",
    "        c = v['comm']\n",
    "        v['ly'][0] += dx[c]\n",
    "        v['ly'][1] += dy[c]\n",
    "    return g.vs['ly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ABCD with very strong communities (xi = 0.05)\n",
    "## results will vary, but we see 3 communities in most runs.\n",
    "xi = 0.05\n",
    "mc = 0\n",
    "while mc != 3: ## run until we get 3 communities\n",
    "    ## generate degree and community size values\n",
    "    cmd = julia+' '+abcd_path+'deg_sampler.jl deg.dat 2.5 5 15 100 1000'\n",
    "    os.system(cmd+' >/dev/null 2>&1')\n",
    "    cmd = julia+' '+abcd_path+'com_sampler.jl cs.dat 1.5 30 50 100 1000'\n",
    "    os.system(cmd+' >/dev/null 2>&1');\n",
    "    cmd = julia+' '+abcd_path+'graph_sampler.jl net.dat comm.dat deg.dat cs.dat xi '\\\n",
    "            +str(xi)+' false false'\n",
    "    os.system(cmd+' >/dev/null 2>&1')\n",
    "    g = ig.Graph.Read_Ncol('net.dat',directed=False)\n",
    "    c = np.loadtxt('comm.dat',dtype='uint16',usecols=(1))\n",
    "    mc = max(c)\n",
    "\n",
    "## plot\n",
    "g.vs['comm'] = [c[int(x['name'])-1]-1 for x in g.vs]\n",
    "g.vs['color'] = [cls[i] for i in g.vs['comm']]\n",
    "g.vs['size'] = 5\n",
    "g.es['color'] = 'lightgrey'\n",
    "ly = push_layout(d=0) ## d=0, no need to push, communities are clear\n",
    "ig.plot(g, layout=ly, bbox=(0,0,300,300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## viz: ABCD with strong communities (xi = 0.15)\n",
    "xi = 0.15\n",
    "mc = 0\n",
    "while mc != 3: ## run until we get 3 communities\n",
    "## generate degree and community size values\n",
    "    cmd = julia+' '+abcd_path+'deg_sampler.jl deg.dat 2.5 5 15 100 1000'\n",
    "    os.system(cmd+' >/dev/null 2>&1')\n",
    "    cmd = julia+' '+abcd_path+'com_sampler.jl cs.dat 1.5 30 50 100 1000'\n",
    "    os.system(cmd+' >/dev/null 2>&1');\n",
    "    cmd = julia+' '+abcd_path+'graph_sampler.jl net.dat comm.dat deg.dat cs.dat xi '\\\n",
    "            +str(xi)+' false false'\n",
    "    os.system(cmd+' >/dev/null 2>&1')\n",
    "    ## compute AMI for various clustering algorithms\n",
    "    g = ig.Graph.Read_Ncol('net.dat',directed=False)\n",
    "    c = np.loadtxt('comm.dat',dtype='uint16',usecols=(1))\n",
    "    mc = max(c)\n",
    "\n",
    "## plot\n",
    "g.vs['comm'] = [c[int(x['name'])-1]-1 for x in g.vs]\n",
    "g.vs['color'] = [cls[i] for i in g.vs['comm']]\n",
    "g.vs['size'] = 5\n",
    "g.es['color'] = 'lightgrey'\n",
    "ly = push_layout(d=1) ## slightly push clusters apart for viz\n",
    "ig.plot(g, layout=ly, bbox=(0,0,300,300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## viz: ABCD with weak communities\n",
    "## lots of edges between communities as expected\n",
    "xi = 0.33\n",
    "mc = 0\n",
    "while mc != 3: ## run until we get 3 communities\n",
    "    ## generate degree and community size values\n",
    "    cmd = julia+' '+abcd_path+'deg_sampler.jl deg.dat 2.5 5 15 100 1000'\n",
    "    os.system(cmd+' >/dev/null 2>&1')\n",
    "    cmd = julia+' '+abcd_path+'com_sampler.jl cs.dat 1.5 30 50 100 1000'\n",
    "    os.system(cmd+' >/dev/null 2>&1');\n",
    "    cmd = julia+' '+abcd_path+'graph_sampler.jl net.dat comm.dat deg.dat cs.dat xi '\\\n",
    "            +str(xi)+' false false'\n",
    "    os.system(cmd+' >/dev/null 2>&1')\n",
    "    ## compute AMI for various clustering algorithms\n",
    "    g = ig.Graph.Read_Ncol('net.dat',directed=False)\n",
    "    c = np.loadtxt('comm.dat',dtype='uint16',usecols=(1))\n",
    "    mc = max(c)\n",
    "    \n",
    "## plot    \n",
    "g.vs['comm'] = [c[int(x['name'])-1]-1 for x in g.vs]\n",
    "g.vs['color'] = [cls[i] for i in g.vs['comm']]\n",
    "g.vs['size'] = 5\n",
    "g.es['color'] = 'lightgrey'\n",
    "ly = push_layout(d=3) ## need to push more -- with d=0, communities can't be seen clearly\n",
    "ig.plot(g, layout=ly, bbox=(0,0,300,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## viz: ABCD with very weak communities\n",
    "xi = 0.5\n",
    "mc = 0\n",
    "while mc != 3: ## run until we get 3 communities\n",
    "    ## generate degree and community size values\n",
    "    cmd = julia+' '+abcd_path+'deg_sampler.jl deg.dat 2.5 5 15 100 1000'\n",
    "    os.system(cmd+' >/dev/null 2>&1')\n",
    "    cmd = julia+' '+abcd_path+'com_sampler.jl cs.dat 1.5 30 50 100 1000'\n",
    "    os.system(cmd+' >/dev/null 2>&1');\n",
    "    cmd = julia+' '+abcd_path+'graph_sampler.jl net.dat comm.dat deg.dat cs.dat xi '\\\n",
    "            +str(xi)+' false false'\n",
    "    os.system(cmd+' >/dev/null 2>&1')\n",
    "    ## compute AMI for various clustering algorithms\n",
    "    g = ig.Graph.Read_Ncol('net.dat',directed=False)\n",
    "    c = np.loadtxt('comm.dat',dtype='uint16',usecols=(1))\n",
    "    mc = max(c)\n",
    "\n",
    "## plot    \n",
    "g.vs['comm'] = [c[int(x['name'])-1]-1 for x in g.vs]\n",
    "g.vs['color'] = [cls[i] for i in g.vs['comm']]\n",
    "g.vs['size'] = 5\n",
    "g.es['color'] = 'lightgrey'\n",
    "ly = push_layout(5) ## need to push more -- with d=0, communities can't be seen clearly\n",
    "ig.plot(g, layout=ly, bbox=(0,0,300,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures to compare partitions\n",
    "\n",
    "* We illustrate the importance of using proper adjusted measures when comparing partitions; this is why we use AMI (adjusted mutual information) or ARI (adjusted Rand index) in our experiments\n",
    "* We generate some ABCD graph and compare ground truth with **random** partitions of different sizes\n",
    "* Scores for random partitions should be close to 0 regardless of the number of parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAND Index: given two clusterings u and v\n",
    "def RI(u,v):\n",
    "    ## build sets from A and B\n",
    "    a = np.max(u)+1\n",
    "    b = np.max(v)+1\n",
    "    n = len(u)\n",
    "    if n != len(v):\n",
    "        exit -1\n",
    "    A = [set() for i in range(a)]\n",
    "    B = [set() for i in range(b)]\n",
    "    for i in range(n):\n",
    "        A[u[i]].add(i)\n",
    "        B[v[i]].add(i)   \n",
    "    ## RAND index step by step\n",
    "    R = 0\n",
    "    for i in range(a):\n",
    "        for j in range(b):\n",
    "            s = len(A[i].intersection(B[j]))\n",
    "            if s>1:\n",
    "                R += s*(s-1)/2\n",
    "    R *= 2\n",
    "    for i in range(a):\n",
    "        s = len(A[i])\n",
    "        if s>1:\n",
    "            R -= s*(s-1)/2\n",
    "    for i in range(b):\n",
    "        s = len(B[i])\n",
    "        if s>1:\n",
    "            R -= s*(s-1)/2\n",
    "    R += n*(n-1)/2\n",
    "    R /= n*(n-1)/2\n",
    "    return R\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate new degree and community size values\n",
    "cmd = julia+' '+abcd_path+'deg_sampler.jl deg.dat 2.5 5 50 1000 1000'\n",
    "os.system(cmd+' >/dev/null 2>&1')\n",
    "cmd = julia+' '+abcd_path+'com_sampler.jl cs.dat 1.5 75 150 1000 1000'\n",
    "os.system(cmd+' >/dev/null 2>&1')\n",
    "xi = .1\n",
    "cmd = julia+' '+abcd_path+'graph_sampler.jl net.dat comm.dat deg.dat cs.dat xi '\\\n",
    "        +str(xi)+' false false'\n",
    "os.system(cmd+' >/dev/null 2>&1')\n",
    "g = ig.Graph.Read_Ncol('net.dat',directed=False)\n",
    "c = np.loadtxt('comm.dat',dtype='uint16',usecols=(1))\n",
    "## ground-truth communities\n",
    "gt = [c[int(x['name'])-1]-1 for x in g.vs]\n",
    "print('number of communities:',np.max(gt)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate random clusterings and compute various measures w.r.t. ground truth\n",
    "## this can take a few minutes to run\n",
    "from sklearn.metrics import mutual_info_score as MI\n",
    "from sklearn.metrics import adjusted_rand_score as ARI\n",
    "from sklearn.metrics import normalized_mutual_info_score as NMI\n",
    "L = []\n",
    "n = g.vcount()\n",
    "tc = {idx:part for idx,part in enumerate(gt)}\n",
    "ar = np.arange(2,21)\n",
    "for s in ar:\n",
    "    for i in range(100):\n",
    "        r = np.random.choice(s, size=n)\n",
    "        rd = {idx:part for idx,part in enumerate(r)}        \n",
    "        L.append([s,MI(gt,r),NMI(gt,r),AMI(gt,r),RI(gt,r),ARI(gt,r),g.gam(tc,rd,adjusted=False),g.gam(tc,rd)])\n",
    "D = pd.DataFrame(L,columns=['size','MI','NMI','AMI','RI','ARI','GRI','AGRI'])\n",
    "R = D.groupby(by='size').mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show results for 3 measures:\n",
    "* Mutual information (MI) as is has strong bian w.r.t. number of clusters\n",
    "* Normalized MI is better\n",
    "* AMI is best, no bias w.r.t. number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mutual information (MI), normalized MI (NMI) and adjusted MI (AMI)\n",
    "plt.plot(ar,R['MI'],':',color='black',label='MI')\n",
    "plt.plot(ar,R['NMI'],'--',color='black',label='NMI')\n",
    "plt.plot(ar,R['AMI'],'-',color='black',label='AMI')\n",
    "plt.xlabel('number of random clusters',fontsize=14)\n",
    "plt.legend();\n",
    "#plt.savefig('MI.eps');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same below for Rand index (RI) and adjusted version. \n",
    "\n",
    "GRI (graph RI) and AGRI (adjusted GRI) are variations of RI specifically for graph data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAND index (RI) and adjusted (ARI)\n",
    "## Also: Graph-aware RAND index (GRI) and adjusted version (AGRI)\n",
    "## those measures are included in partition-igraph \n",
    "## input are partitions of type 'igraph.clustering.VertexClustering'or a dictionaries of node:community.\n",
    "plt.plot(ar,R['RI'],':',color='black',label='RI')\n",
    "plt.plot(ar,R['GRI'],'--',color='black',label='GRI')\n",
    "\n",
    "plt.plot(ar,R['ARI'],'-',color='black',label='ARI/AGRI')\n",
    "plt.plot(ar,R['AGRI'],'-',color='black')\n",
    "\n",
    "plt.xlabel('number of random clusters',fontsize=14)\n",
    "plt.legend();\n",
    "#plt.savefig('RI.eps');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphmining",
   "language": "python",
   "name": "graphmining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
