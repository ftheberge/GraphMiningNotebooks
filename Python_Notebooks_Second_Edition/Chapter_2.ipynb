{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 - Random Graph Models\n",
    "\n",
    "In the first part of this notebook, we provide the code required to generate the Figures in Chapter 2 of the textbook.\n",
    "\n",
    "In the second part, we consider the GitHub machine learning (ml) developers graph that we introduced in Chapter 1, and compare various statistics for this graph with the values we get for the random graphs models introduced in Chapter 2.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "We use one new package in this notebook called ```powerlaw```.\n",
    "Details and examples of use can be found here: https://arxiv.org/pdf/1305.0215.pdf or here: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0085777.\n",
    "\n",
    "As with the previous notebook, make sure to set the data directory properly in the second next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import powerlaw\n",
    "from scipy.stats import poisson\n",
    "from scipy.optimize import fsolve\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"../Datasets/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Figures for Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2.1: size of the giant component\n",
    "\n",
    "We generate several binomial random graphs with $n$ nodes, where we vary the average node degree (thus, the number of edges). We consider $n=100$ below, and you can try for different $n$. Un-comment the second line to run with $n=10,000$ nodes as in the second plot in the book (this will be much slower).\n",
    "\n",
    "We plot the theoretical giant component size (black line) and the 90% confidence interval from the empirical data in grey, both as a function of the average degree; we see good agreement and we observe the various phases as described in the book. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "#n = 10000\n",
    "\n",
    "random.seed(123)\n",
    "\n",
    "gc_avg = []\n",
    "gc_std = []\n",
    "\n",
    "## range of values for average degree and number of repeats for each\n",
    "avg_deg = np.arange(0.1, 10.1, 0.1)\n",
    "Repeats = 1000\n",
    "\n",
    "## generate random graphs and gather size of giant component\n",
    "for deg in avg_deg:\n",
    "    x = []\n",
    "    p = deg / (n - 1)\n",
    "    for rep in range(Repeats):\n",
    "        g = ig.Graph.Erdos_Renyi(n=n, p=p)\n",
    "        x.append(g.connected_components().giant().vcount())\n",
    "    ## average and standard deviation for a given average degree\n",
    "    gc_avg.append(np.mean(x))\n",
    "    gc_std.append(np.std(x))\n",
    "\n",
    "## theoretical values as described in the book\n",
    "\n",
    "## small values up to 1\n",
    "th_val = [np.log(n) for i in np.arange(0.1, 1.1, 0.1)]  \n",
    "\n",
    "## larger values\n",
    "def fn(x, d):\n",
    "    return x + np.exp(-x * d) - 1\n",
    "for i in np.arange(1.1, 10.1, 0.1):\n",
    "    th_val.append(n * fsolve(fn, 1, args=(i))[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot empirical results (confidence intervals) and theoretical values\n",
    "plt.fill_between(\n",
    "    avg_deg,\n",
    "    [x[0] - 1.654 * x[1] for x in zip(gc_avg, gc_std)],\n",
    "    [x[0] + 1.645 * x[1] for x in zip(gc_avg, gc_std)],\n",
    "    color=\"lightgray\",\n",
    ")\n",
    "plt.plot(avg_deg, th_val, color=\"black\")\n",
    "plt.suptitle(\"Random graph with \" + str(n) + \" nodes\", fontsize=14)\n",
    "plt.title(\"Theoretical predictions (black) vs empirical results (grey)\", fontsize=12)\n",
    "plt.xlabel(\"average degree\", fontsize=14)\n",
    "plt.ylabel(\"giant component size\", fontsize=14)\n",
    "# plt.savefig('giant_100.eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2.2: probability that the graph is connected\n",
    "\n",
    "This is a similar experiment as above, but this time we look at the probability that the random graph is connected.\n",
    "We vary some constant $c$ introduced in the book such that the edge probability for the binomial graphs is given by $(\\log(n)+c)/n$. Once again we compare theory (black line) and experimental results (in grey) with $n=100$ nodes. Un-comment the second line to run with $n=10,000$ nodes as in the second plot in the book (this will be much slower).\n",
    "\n",
    "In the cell below, the grey area corresponds to a 90% confidence interval for proportions; for empirical proportion $x$ obtained from sample of size $n$, the formula is given by $x \\pm 1.645 \\sqrt{x(1-x)/n}$.\n",
    "\n",
    "Here also we see good agreement between theory and experimental results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "# n = 10000\n",
    "\n",
    "random.seed(123)\n",
    "Repeats = 1000  ## number of repeats for each 'c' value\n",
    "\n",
    "## set lower bound for the range of values for ‘c’ (to avoid p being negative)\n",
    "lo = -int(np.floor(np.log(n) * 10)) / 10\n",
    "if lo < -10:\n",
    "    lo = -10\n",
    "c_range = np.arange(lo, 10.1, 0.1)\n",
    "ic_avg = []\n",
    "\n",
    "## loop over 'c' values - details in the book\n",
    "for c in c_range:\n",
    "    x = []\n",
    "    p = (c + np.log(n)) / n\n",
    "    for rep in range(Repeats):\n",
    "        g = ig.Graph.Erdos_Renyi(n=n, p=p)\n",
    "        x.append(int(g.is_connected()))\n",
    "    ic_avg.append(np.mean(x))\n",
    "\n",
    "## theoretical values\n",
    "th = [np.exp(-np.exp(-c)) for c in c_range]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot\n",
    "plt.fill_between(\n",
    "    c_range,\n",
    "    [x - 1.654 * np.sqrt(x * (1 - x) / n) for x in ic_avg],\n",
    "    [x + 1.645 * np.sqrt(x * (1 - x) / n) for x in ic_avg],\n",
    "    color=\"lightgray\",\n",
    ")\n",
    "plt.plot(c_range, th, color=\"black\")\n",
    "plt.suptitle(\"Random graph with \" + str(n) + \" nodes\", fontsize=14)\n",
    "plt.title(\"Theoretical predictions (black) vs empirical results (grey)\", fontsize=12)\n",
    "plt.xlabel(r\"constant $c$\", fontsize=14)\n",
    "plt.ylabel(\"P(graph is connected)\", fontsize=14)\n",
    "# plt.savefig('connected_100.eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2.4: Distribution of shortest path lengths\n",
    "\n",
    "We consider a series of binomial random graphs with expected average degree 5, where we vary the number of nodes from $n=64$ to $n=2,048$.\n",
    "\n",
    "We see that as we double the number of nodes, the average shortest path lengths (in the giant component) increases slowly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_len = []\n",
    "\n",
    "## number of nodes\n",
    "n_range = [64, 128, 256, 512, 1024, 2048]\n",
    "random.seed(123)\n",
    "\n",
    "for n in n_range:\n",
    "    p = 5 / (n - 1)\n",
    "    ## keep giant component\n",
    "    g = ig.Graph.Erdos_Renyi(n=n, p=p).connected_components().giant()\n",
    "    ## all shortest path lengths > 0\n",
    "    z = g.distances() \n",
    "    sp_len.append([x for y in z for x in y if x > 0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show as histograms (boxplots in the first edition)\n",
    "\n",
    "bins = np.arange(0.5, 8.5, 1)\n",
    "fig, axs = plt.subplots(2, 3)\n",
    "fig.suptitle('Shortest path length distribution')\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        axs[i, j].hist(\n",
    "            sp_len[3 * i + j], bins=bins, width=0.9, density=True, color=\"darkgrey\"\n",
    "        )\n",
    "        axs[i, j].set_ylim(0, 0.48)\n",
    "        axs[i, j].set_xticks([1, 3, 5, 7])\n",
    "        axs[i, j].set_title(str(n_range[3 * i + j]) + \" nodes\", fontsize=10)\n",
    "        axs[i, j].set_xlabel(\"path length\")\n",
    "        axs[i, j].set_ylabel(\"proportion\")\n",
    "for ax in fig.get_axes():\n",
    "    ax.label_outer()\n",
    "# fig.savefig('path_len.eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2.5 Poisson vs degree distributions\n",
    "\n",
    "We plot the degree distribution for binomial random graphs with expected average degree 10, and $n=100$ nodes (the black dots), and we compare with the corresponding Poisson distribution (dashed line).\n",
    "\n",
    "Try increasing $n$; the dots should get closer to the Poisson distribution.\n",
    "\n",
    "We used $n=10,000$ for the book.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "random.seed(12345)\n",
    "p = 10 / (n - 1)\n",
    "g = ig.Graph.Erdos_Renyi(n=n, p=p)\n",
    "x = [x[0] for x in sorted(Counter(g.degree()).items())]\n",
    "pmf = [poisson.pmf(k, 10) for k in x]\n",
    "frq = [x[1] / n for x in sorted(Counter(g.degree()).items())]\n",
    "plt.plot(x, frq, \"o\", color=\"black\")\n",
    "plt.plot(x, pmf, \":\", color=\"black\")\n",
    "plt.title(\"Empirical degree distribution vs Poisson distribution\")\n",
    "plt.xlabel(\"degree\", fontsize=14)\n",
    "plt.ylabel(\"frequency/pmf\", fontsize=14)\n",
    "# plt.savefig('poisson_10000.eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(Counter(g.degree()).items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2.6 -  Power law graphs\n",
    "\n",
    "We generate a random graph with $n=10,000$ nodes following power law degree distribution with exponent $\\gamma=2.5$.\n",
    "We do so using the Chung-Lu models described in section 2.5 of the book; we generate simple graphs (no loops or multiedges) and discard 0-degree nodes.\n",
    "\n",
    "We then fit and plot the degree distribution of the obtained graph using the ```powerlaw``` package, see: https://arxiv.org/pdf/1305.0215.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fast Chung-Lu: generate m distinct edges w.r.t. distribution d, no loops\n",
    "def fast_CL(d, m, seed=123):\n",
    "    np.random.seed(seed)\n",
    "    n = len(d)  ## number of nodes\n",
    "    s = np.sum(d)\n",
    "    p = [i / s for i in d]  ## we draw nodes w.r.t. degrees\n",
    "    target = m  ## number of edges to generate\n",
    "    tples = []  ## list of generated edges\n",
    "    ## generate edges (tuples), drop collisions, until m edges are obtained.\n",
    "    while len(tples) < target:\n",
    "        s = target - len(tples)  ## number left to generate\n",
    "        e0 = np.random.choice(n, size=s, replace=True, p=p)\n",
    "        e1 = np.random.choice(n, size=s, replace=True, p=p)\n",
    "        tples.extend(\n",
    "            [(min(e0[i], e1[i]), max(e0[i], e1[i])) for i in range(len(e0)) if e0[i] != e1[i]]\n",
    "        )  ## ignore loops\n",
    "        tples = list(set(tples))  ## drop duplicates\n",
    "    return tples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate graph and fit power law model\n",
    "\n",
    "A few remarks regarding the ```powerlaw``` package:\n",
    "\n",
    "* ```xmin``` corresponds to $\\ell'$ in the book\n",
    "* ```alpha``` corresponds to $\\gamma'$ in the book\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## power law graph - details in the book\n",
    "\n",
    "# np.random.seed(23)\n",
    "gamma = 2.5\n",
    "n = 10000\n",
    "\n",
    "## min and max degrees\n",
    "delta = 1\n",
    "Delta = np.sqrt(n)\n",
    "\n",
    "## generate degrees (details in the book section 2.5)\n",
    "W = []\n",
    "for i in np.arange(1, n + 1):\n",
    "    W.append(delta * (n / (i - 1 + n / (Delta / delta) ** (gamma - 1))) ** (1 / (gamma - 1)))\n",
    "# deg = [int(np.round(w)) for w in W] ## to enforce integer weights, not an obligation\n",
    "deg = W\n",
    "\n",
    "## generate graph with Chung-Lu model\n",
    "m = int(np.mean(deg) * n / 2)\n",
    "tpl = fast_CL(deg, m, seed=23)\n",
    "g_pl = ig.Graph.TupleList(tpl)\n",
    "\n",
    "## number of isolated nodes (no edges)\n",
    "iso = n - g_pl.vcount()\n",
    "print(\"number of isolated nodes:\", iso, \"\\n\")\n",
    "\n",
    "## run powerlaw and compute Kolmogorov-Smirnov statistic (details in the book)\n",
    "## slightly different values in the first edition - different seeding\n",
    "deg = g_pl.degree()\n",
    "X = powerlaw.Fit(deg)\n",
    "print(\"\\n\\nRange of degrees in graph:\", min(deg), max(deg))\n",
    "print(\"Value of l':\", X.power_law.xmin)\n",
    "print(\"Corresponding value of gamma':\", X.power_law.alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divergence vs $\\ell$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot divergence vs 'l' (Kolmogorov-Smirnov statistic)\n",
    "x = X.xmins\n",
    "y = X.Ds\n",
    "plt.plot(x, y, \".\")\n",
    "\n",
    "## Plot min value with larger dot\n",
    "x = int(X.power_law.xmin)\n",
    "y = X.Ds[x - 1]\n",
    "plt.plot([x], [y], \"o\")\n",
    "plt.xlabel(r\"$\\ell$\", fontsize=14)\n",
    "plt.ylabel(\"Divergence\", fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot divergence vs. exponent (alphas here, gamma' in the book)\n",
    "plt.plot(X.alphas[:50], X.Ds[:50], \".\")\n",
    "\n",
    "## Plot min value with larger dot\n",
    "i = int(X.power_law.xmin)\n",
    "x = X.alphas[i - 1]\n",
    "y = X.Ds[i - 1]\n",
    "plt.plot([x], [y], \"o\")\n",
    "plt.xlabel(r\"$\\gamma$\", fontsize=14)\n",
    "plt.ylabel(\"Divergence\", fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2.6 - inverse (cumulative) cdf vs degree and fitted power law\n",
    "\n",
    "In the first plot, we look at degrees starting from $\\ell'$.\n",
    "\n",
    "In the second plot, we look at the whole range of degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Figure 2.6 - starting from l'\n",
    "fig1 = X.power_law.plot_ccdf(color=\"black\", linestyle=\"-\")\n",
    "fig1 = X.plot_ccdf(\n",
    "    ax=fig1, linewidth=2, color=\"gray\", original_data=False, linestyle=\":\"\n",
    ")\n",
    "fig1.set_xlabel(\"degree\", fontsize=13)\n",
    "fig1.set_ylabel(\"inverse cdf\", fontsize=13)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now starting from 1 - need to translate power law line manually\n",
    "fig = X.plot_ccdf(linewidth=2, color=\"dimgray\", original_data=True, linestyle=\"--\")\n",
    "fig.set_xlabel(\"degree\", fontsize=13)\n",
    "fig.set_ylabel(\"inverse cdf\", fontsize=13)\n",
    "\n",
    "## get end points for power law fitted line\n",
    "x = [\n",
    "    int(X.power_law.xmin),\n",
    "    int(X.data[-1:][0]),\n",
    "]  ## x-axis: from l' to max value in data\n",
    "delta_y = X.ccdf(original_data=True)[1][x[0] - 1]  ## translation for first point\n",
    "y = [delta_y, X.power_law.ccdf()[-1:][0] * delta_y]  ## y-axis values\n",
    "plt.plot(x, y, \"-\", linewidth=2, color=\"black\")\n",
    "print(\n",
    "    \"power law slope:\",\n",
    "    (np.log10(y[1]) - np.log10(y[0])) / (np.log10(x[1]) - np.log10(x[0])),\n",
    ")\n",
    "# plt.savefig('powerlaw.eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2.7: simple $d$-regular graphs\n",
    "\n",
    "We generate several $d$-regular graphs and count how many are simple graphs.\n",
    "We consider $d=2$ to $d=10$, with $n=100$ nodes. \n",
    "We used $n=10,000$ nodes in the book.\n",
    "\n",
    "We plot the empirical proportion of simple graphs below (black dots), and we compare with the theoretical values (dashed line). We see good agreement even for small value $n=100$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "# n = 10000\n",
    "\n",
    "random.seed(1234)\n",
    "Repeats = 1000\n",
    "Degs = np.arange(2, 11)\n",
    "simple = []\n",
    "\n",
    "## count number of simple graphs\n",
    "for deg in Degs:\n",
    "    x = 0\n",
    "    for rep in range(Repeats):\n",
    "        g = ig.Graph.Degree_Sequence([deg for i in range(n)])\n",
    "        x += int(g.is_simple())\n",
    "    simple.append(x / Repeats)\n",
    "th_simple = [np.exp(-(deg * deg - 1) / 4) for deg in Degs] ## theoretical value - details in section 2.6\n",
    "\n",
    "## plot empirical and theoretical results\n",
    "plt.plot(Degs, simple, \"o\", color=\"black\")\n",
    "plt.plot(Degs, th_simple, \":\", color=\"black\")\n",
    "plt.xlabel(\"degree\", fontsize=14)\n",
    "plt.ylabel(\"P(graph is simple)\", fontsize=14)\n",
    "# plt.savefig('d-reg_100.eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.8 - Random geometric graphs (RGG)\n",
    "\n",
    "With this model, $n$ nodes are dropped randomly on the d-dimensional space $[0,1]^d$.\n",
    "Two nodes are connected by an edge if their distance is less than some **radius** parameter $r$.\n",
    "We consider the **unit square**, so we fix $d=2$ in our examples.\n",
    "\n",
    "For RGG on the unit square, the (expected) average degree of a node in a graph with $n$ nodes is $\\pi r^2 (n-1)$ where $r$ is the radius parameter, unless the node is near the square boundary. Near the boundary, this approximation is actually slightly larger than the true expected average degree due to boundary effects (nodes close to the boundary of the square will have a smaller number of connections).\n",
    "\n",
    "We can slightly modify this model by considering a unit **torus**. This will eliminate boundary effects so all nodes have (expected) average degree $\\pi r^2 (n-1).$ \n",
    "\n",
    "In all experiments below, you can use either the unit square by setting ```torus=False```, or the torus model by setting ```torus=True```.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at some RGGs\n",
    "\n",
    "We plot some geometric random graphs with $n=100$ nodes and varying radius threshold $r$.\n",
    "\n",
    "Note that the ```igraph``` function to generate such graphs is called ```Graph.GRG```. When generating such a graph, the position of the nodes in the square or torus are saved as **vertex attributes** 'x' and 'y'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting a few random geometric graphs with 100 nodes and varying radius threshold\n",
    "n = 100\n",
    "torus = False  ## Set to True to see a torus-based RGG, else we use the unit square\n",
    "\n",
    "## select a value for the radius:\n",
    "#radius = 0.1\n",
    "radius = 0.15\n",
    "#radius = 0.2\n",
    "\n",
    "random.seed(1234)\n",
    "g = ig.Graph.GRG(n=n, radius=radius, torus=torus)\n",
    "print(\"\\nGeometric random graph with radius =\", radius)\n",
    "ly = [\n",
    "    (x, y) for x, y in zip(g.vs[\"x\"], g.vs[\"y\"])\n",
    "]  ## position of the nodes are saved as attributes\n",
    "ig.plot(g, layout=ly, bbox=(300, 300), vertex_size=5, vertex_color=\"dimgrey\")\n",
    "\n",
    "# to save the plot:\n",
    "# fn = 'grg_'+str(int(100*radius))+'.eps'\n",
    "# ig.plot(g, fn, bbox=(300,300), vertex_size=5, vertex_color='dimgrey')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGG - size of the giant component\n",
    "\n",
    "Next we look at the size of the giant component for geometric random graphs on the unit square as we vary the radius parameter.\n",
    "\n",
    "For RGG on the unit square, the (expected) average degree of a node in a graph with $n$ nodes is $\\pi r^2 (n-1)$ where $r$ is the radius parameter, unless the node is near the square boundary. Near the boundary, this approximation is actually slightly larger than the true expected average degree due to boundary effects (nodes close to the boundary of the square will have a smaller number of connections).\n",
    "\n",
    "In the experiment below, we fix some degree range and compute the corresponding radius parameters $r$.\n",
    "For each value $r$, we generate 1,000 GRGs and compute the mean and standard deviation for the size of the \n",
    "giant component.\n",
    "\n",
    "We see a similar shape as with binomial (Erdos-Renyi) random graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment - size of the giant component vs radius threshold\n",
    "\n",
    "## Set to True to see a torus-based RGG, else we use the unit square\n",
    "torus = True\n",
    "## number of nodes:\n",
    "n = 100\n",
    "# n = 10000 ## this takes a few minutes\n",
    "\n",
    "repeats = 1000\n",
    "random.seed(1234)\n",
    "average_degree = np.arange(0.5, 15.1, 0.5)\n",
    "\n",
    "## range of values for the radius parameter\n",
    "radii = [np.sqrt(i / (np.pi * (n - 1))) for i in average_degree]\n",
    "\n",
    "gc_avg = []  ## store average giant component sizes\n",
    "gc_std = []  ## ... and the standard deviations\n",
    "deg_avg = []  ## we also store the true mean empirical average degrees\n",
    "\n",
    "## generate random graphs and gather the sizes of the giant component\n",
    "for radius in radii:\n",
    "    x = []\n",
    "    y = []\n",
    "    for rep in range(repeats):\n",
    "        g = ig.Graph.GRG(n=n, radius=radius, torus=torus)\n",
    "        x.append(g.connected_components().giant().vcount())\n",
    "        y.append(np.mean(g.degree()))\n",
    "    ## average and standard deviation for a given radius\n",
    "    gc_avg.append(np.mean(x))\n",
    "    gc_std.append(np.std(x))\n",
    "    deg_avg.append(np.mean(y))\n",
    "\n",
    "## plot the above empirical results (confidence intervals and mean values)\n",
    "plt.fill_between(\n",
    "    average_degree,\n",
    "    [x[0] - 1.654 * x[1] for x in zip(gc_avg, gc_std)],\n",
    "    [min(n, x[0] + 1.645 * x[1]) for x in zip(gc_avg, gc_std)],\n",
    "    color=\"lightgray\",\n",
    ")\n",
    "\n",
    "plt.plot(average_degree, gc_avg, color=\"black\", linestyle=\":\")\n",
    "plt.xlabel(\"Average degree (approximate)\", fontsize=14)\n",
    "# plt.suptitle('Geometric random graphs with '+str(n)+' nodes',fontsize=14)\n",
    "# plt.title('Empirical results',fontsize=12)\n",
    "plt.ylabel(\"giant component size\", fontsize=14);\n",
    "\n",
    "## to save the plot:\n",
    "# fn = 'grg_giant_'+str(n)+'.eps'\n",
    "# plt.savefig(fn)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree formula vs empirical results\n",
    "\n",
    "For the experiment above, we compare the empirical average degrees with the formula presented earlier.\n",
    "If the experiment was conducted on the unit square (with ```torus=False```), then the empirical degrees are slightly smaller than the ones computes with the formula -- this is clearly seen by comparing with a unit slope line. However if the experiment was conducted on torus (with ```torus=True```), then the values are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare the empirical average degrees for the previous experiment with the approximation formula\n",
    "## we also plot a line with slope = 1 for easier comparison\n",
    "plt.plot(average_degree, deg_avg, color=\"black\")\n",
    "plt.plot(deg_avg, deg_avg, \":\", color=\"dimgray\")\n",
    "plt.ylabel(\"empirical average degree\", fontsize=14)\n",
    "plt.xlabel(\"average degree formula\", fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connectivity of RGGs\n",
    "\n",
    "This time we look at the probability that the random graph is connected.\n",
    "We vary some constant $c$ introduced in the book such that the radius $r$ for the RGGs is given by $n\\pi r^2 = \\ln n + c$. We compare theory (black line) and experimental results (in grey) with $n=100$ nodes. \n",
    "We used $n=10,000$ nodes in the book (this will be much slower).\n",
    "\n",
    "In the cell below, the grey area corresponds to a 90% confidence interval for proportions; for empirical proportion $x$ obtained from sample of size $n$, the formula is given by $x \\pm 1.645 \\sqrt{x(1-x)/n}$.\n",
    "\n",
    "In this case, if we generate RGGs on a square, boundary nodes will have a higher chance of being isolated, so the convergence to the theoretical result with respect to $n$ is slow, as we observe in the results below.\n",
    "Working on a torus, we see faster convergence, but even with $n = 10,000$, there are still some small differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pr(graph is connected)\n",
    "\n",
    "torus = False\n",
    "n = 100\n",
    "# n = 10000 ## this will take several minutes\n",
    "\n",
    "random.seed(123)\n",
    "Repeats = 1000  ## number of repeats for each 'c' value\n",
    "\n",
    "## set lower bound for the range of values for 'c'\n",
    "lo = -int(np.floor(np.log(n) * 10)) / 10\n",
    "if lo < -5:\n",
    "    lo = -5\n",
    "c_range = np.arange(lo, 10.1, 0.1)\n",
    "ic_avg = []\n",
    "\n",
    "## loop over 'c' values\n",
    "for c in c_range:\n",
    "    x = []\n",
    "    r = np.sqrt((c + np.log(n)) / (np.pi * (n)))\n",
    "    for rep in range(Repeats):\n",
    "        g = ig.Graph.GRG(n=n, radius=r, torus=torus)\n",
    "        x.append(int(g.is_connected()))\n",
    "    ic_avg.append(np.mean(x))\n",
    "\n",
    "## theoretical values\n",
    "th = [np.exp(-np.exp(-c)) for c in c_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot\n",
    "plt.fill_between(\n",
    "    c_range,\n",
    "    [x - 1.654 * np.sqrt(x * (1 - x) / n) for x in ic_avg],\n",
    "    [x + 1.645 * np.sqrt(x * (1 - x) / n) for x in ic_avg],\n",
    "    color=\"lightgrey\",\n",
    ")\n",
    "# plt.plot(c_range, ic_avg, '-', color='black')\n",
    "plt.plot(c_range, th, \"-\", color=\"black\")\n",
    "# plt.suptitle('Geometric random graph with '+str(n)+' nodes',fontsize=14)\n",
    "# plt.title('Theoretical predictions (black) vs empirical results (grey)',fontsize=12)\n",
    "plt.xlabel(r\"constant $c$\", fontsize=14)\n",
    "plt.ylabel(\"P(graph is connected)\", fontsize=14)\n",
    "## to save the plot:\n",
    "# if torus:\n",
    "#     fn = 'grg_connected_torus_'+str(n)+'.eps'\n",
    "# else:\n",
    "#     fn = 'grg_connected_square_'+str(n)+'.eps'\n",
    "# plt.savefig(fn)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Experiments with random graphs\n",
    "\n",
    "We use the giant component of the **GitHub machine learning (ml) developers** subgraph that we introduced in Chapter 1. Recall this graph has 7,083 nodes and 19,491 edges. \n",
    "\n",
    "We compute several graphs statistics for this \"base graph\", as reported in the first column of **Table 2.8** from the book.\n",
    "\n",
    "We then generate **random** graphs with the same number of nodes and edges using 4 different models:\n",
    "* binomial or Erdos-Renyi: only average degree is used\n",
    "* Chung-Lu: expected degree distribution\n",
    "* Configuration: exact degree distribution\n",
    "* Configuration with Viger method: connected, simple graph is obtained\n",
    "\n",
    "See **section 2.8** of the book for a more complete discussion of the results, but as a general observation, more complex models (such as the configuration model with Viger method) tend to preserve more characteristics of the reference graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the GitHub edge list as tuples and build undirected graph (as in Chapter 1)\n",
    "df = pd.read_csv(datadir + \"GitHubDevelopers/musae_git_edges.csv\")\n",
    "GitHubGraph = ig.Graph.TupleList(\n",
    "    [tuple(x) for x in df.values], directed=False, vertex_name_attr=\"id\"\n",
    ")\n",
    "\n",
    "## read node attributes\n",
    "Attr = pd.read_csv(datadir + \"GitHubDevelopers/musae_git_target.csv\")\n",
    "## build attribute dictionaries\n",
    "Names = dict(zip(Attr.id, Attr.name))\n",
    "ML = dict(zip(Attr.id, Attr.ml_target))\n",
    "## add name attributes to graph\n",
    "GitHubGraph.vs[\"name\"] = [Names[i] for i in GitHubGraph.vs[\"id\"]]\n",
    "## add a class: 'ml' or 'web' depending on attribute 'ml_label'\n",
    "labels = [\"web\", \"ml\"]\n",
    "GitHubGraph.vs[\"class\"] = [labels[ML[i]] for i in GitHubGraph.vs[\"id\"]]\n",
    "\n",
    "## for github, 9739 are ml developers, build the subgraph and keep the giant component\n",
    "subgraph_ml = GitHubGraph.subgraph([v for v in GitHubGraph.vs() if v[\"class\"] == \"ml\"])\n",
    "subgraph_ml = subgraph_ml.connected_components().giant()\n",
    "print(subgraph_ml.vcount(), \"nodes and\", subgraph_ml.ecount(), \"edges\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## return statistics in Table 2.8\n",
    "def baseStats(G):\n",
    "    deg = G.degree()\n",
    "    return [\n",
    "        G.vcount(),\n",
    "        G.ecount(),\n",
    "        np.min(deg),\n",
    "        np.mean(deg),\n",
    "        np.median(deg),\n",
    "        np.max(deg),\n",
    "        G.diameter(),\n",
    "        np.max(G.connected_components().membership) + 1,\n",
    "        G.connected_components().giant().vcount(),\n",
    "        sum([x == 0 for x in G.degree()]),\n",
    "        G.transitivity_undirected(),\n",
    "        G.transitivity_avglocal_undirected(),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "\n",
    "## Compute and store statistics for Base (subgraph_ml) graphs\n",
    "S = []\n",
    "S.append([\"Base Graph\"] + baseStats(subgraph_ml))\n",
    "\n",
    "## Append statistics for Erdos-Renyi graph with same number of nodes and edges\n",
    "g_er = ig.Graph.Erdos_Renyi(n=subgraph_ml.vcount(), m=subgraph_ml.ecount())\n",
    "S.append([\"Erdos-Renyi\"] + baseStats(g_er))\n",
    "\n",
    "## Append statistics for Chung-Lu graph with same (expected) degree distribution\n",
    "tuples = fast_CL(subgraph_ml.degree(), subgraph_ml.ecount(), seed=123)\n",
    "g_cl = ig.Graph.Erdos_Renyi(n=subgraph_ml.vcount(), m=0)\n",
    "g_cl.add_edges(tuples)\n",
    "S.append([\"Chung-Lu\"] + baseStats(g_cl))\n",
    "\n",
    "## Append statistics for configuration model graph with same degree distribution\n",
    "g_cm = ig.Graph.Degree_Sequence(subgraph_ml.degree(), method=\"simple\")\n",
    "S.append([\"Configuration\"] + baseStats(g_cm))\n",
    "\n",
    "## Append statistics for configuration model simple graph with same degree distribution\n",
    "g_cmv = ig.Graph.Degree_Sequence(subgraph_ml.degree(), method=\"vl\")\n",
    "S.append([\"Configuration (V)\"] + baseStats(g_cmv))\n",
    "\n",
    "## Store in dataframe and show results\n",
    "df = pd.DataFrame(\n",
    "    S,\n",
    "    columns=[\n",
    "        \"graph\",\n",
    "        \"nodes\",\n",
    "        \"edges\",\n",
    "        r\"$\\delta = d_{min}$\",\n",
    "        r\"$d_{mean}$\",\n",
    "        r\"$d_{median}$\",\n",
    "        r\"$\\Delta = d_{max}$\",\n",
    "        \"diameter\",\n",
    "        \"components\",\n",
    "        \"largest\",\n",
    "        \"isolates\",\n",
    "        r\"$C_{glob}$\",\n",
    "        r\"$C_{loc}$\",\n",
    "    ],\n",
    ").transpose()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortest path length distribution\n",
    "\n",
    "We compute and compare the shortest path length distribution for several node pairs and for the 5 graphs we have (GitHub ml reference graph, and 4 random graphs). Sampling is used to speed-up the process.\n",
    "\n",
    "We consider the giant component for disconnected graphs.\n",
    "\n",
    "We see a reasonably high similarity for all graphs, with the binomial random graph having slightly longer path lengths due to the absence of high degree (hub) nodes in that model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sampling -- doing all vertices is slow\n",
    "sample_size = 200\n",
    "np.random.seed(123)\n",
    "\n",
    "## using the giant component for disconnected graphs\n",
    "g_er_gcc = g_er.connected_components().giant()\n",
    "g_cl_gcc = g_cl.connected_components().giant()\n",
    "g_cm_gcc = g_cm.connected_components().giant()\n",
    "\n",
    "## compute shortest paths (exclude 0-length, i.e. distance to self)\n",
    "## n.b.: we sample separately since we use the giant components and graphs may\n",
    "##       have a different number of nodes (except the first and last one)\n",
    "sp_sg = []\n",
    "for v in np.random.choice(subgraph_ml.vcount(), size=sample_size, replace=False):\n",
    "    sp_sg.extend([i for i in subgraph_ml.distances(source=v)[0] if i > 0])\n",
    "sp_er = []\n",
    "for v in np.random.choice(g_er_gcc.vcount(), size=sample_size, replace=False):\n",
    "    sp_er.extend([i for i in g_er_gcc.distances(source=v)[0] if i > 0])\n",
    "sp_cl = []\n",
    "for v in np.random.choice(g_cl_gcc.vcount(), size=sample_size, replace=False):\n",
    "    sp_cl.extend([i for i in g_cl_gcc.distances(source=v)[0] if i > 0])\n",
    "sp_cm = []\n",
    "for v in np.random.choice(g_cm_gcc.vcount(), size=sample_size, replace=False):\n",
    "    sp_cm.extend([i for i in g_cm_gcc.distances(source=v)[0] if i > 0])\n",
    "sp_cmv = []\n",
    "for v in np.random.choice(g_cmv.vcount(), size=sample_size, replace=False):\n",
    "    sp_cmv.extend([i for i in g_cmv.distances(source=v)[0] if i > 0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare shortest path length distributions\n",
    "bins = np.arange(0.5, 11.5, 1)\n",
    "fig, axs = plt.subplots(2, 3)\n",
    "# fig.suptitle('Shortest path length distribution')\n",
    "\n",
    "## plot the 5 histograms\n",
    "axs[0, 0].hist(sp_sg, bins=bins, width=0.9, density=True, color=\"darkgrey\")\n",
    "axs[0, 0].set_title(\"Base (GitHub ml)\", fontsize=10)\n",
    "axs[0, 1].hist(sp_er, bins=bins, width=0.9, density=True, color=\"darkgrey\")\n",
    "axs[0, 1].set_title(\"Binomial\", fontsize=10)\n",
    "axs[0, 2].hist(sp_cl, bins=bins, width=0.9, density=True, color=\"darkgrey\")\n",
    "axs[0, 2].set_title(\"Chung-Lu\", fontsize=10)\n",
    "axs[1, 0].hist(sp_cm, bins=bins, width=0.9, density=True, color=\"darkgrey\")\n",
    "axs[1, 0].set_title(\"Config.\", fontsize=10)\n",
    "axs[1, 1].hist(sp_cmv, bins=bins, width=0.9, density=True, color=\"darkgrey\")\n",
    "axs[1, 1].set_title(\"Config.(V)\", fontsize=10)\n",
    "\n",
    "## set uniform y-range and ticks\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        axs[i, j].set_ylim(0, 0.5)\n",
    "        axs[i, j].set_xticks([2, 4, 6, 8, 10])\n",
    "\n",
    "## adjust 3-2 format\n",
    "axs[1, 2].set_visible(False)\n",
    "axs[1, 0].set_position([0.24, 0.08, 0.228, 0.343])\n",
    "axs[1, 1].set_position([0.55, 0.08, 0.228, 0.343])\n",
    "\n",
    "## labels only on the outer axis\n",
    "axs[0, 0].set_ylabel(\"proportion\")\n",
    "axs[1, 0].set_ylabel(\"proportion\")\n",
    "axs[1, 0].set_xlabel(\"path length\")\n",
    "axs[1, 1].set_xlabel(\"path length\")\n",
    "axs[0, 1].get_yaxis().set_ticklabels([])\n",
    "axs[0, 2].get_yaxis().set_ticklabels([])\n",
    "\n",
    "## add mean values\n",
    "axs[0, 0].text(6, 0.42, \"mean: \" + str(float(\"%.3g\" % np.mean(sp_sg))), fontsize=8)\n",
    "axs[0, 1].text(6, 0.42, \"mean: \" + str(float(\"%.3g\" % np.mean(sp_er))), fontsize=8)\n",
    "axs[0, 2].text(6, 0.42, \"mean: \" + str(float(\"%.3g\" % np.mean(sp_cl))), fontsize=8)\n",
    "axs[1, 0].text(6, 0.42, \"mean: \" + str(float(\"%.3g\" % np.mean(sp_cm))), fontsize=8)\n",
    "axs[1, 1].text(6, 0.42, \"mean: \" + str(float(\"%.3g\" % np.mean(sp_cmv))), fontsize=8)\n",
    "\n",
    "#fig.savefig('pathlens_random.eps',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More power law tests - GitHub subgraphs and Grid graph\n",
    "\n",
    "We try to fit power law for the degree distributions as we did before, this time for 3 real graphs:\n",
    "* GitHub ml developers (giant component)\n",
    "* GitHub web developers (giant component)\n",
    "* Grid (Europe power grid graph, giant component)\n",
    "\n",
    "While the first two exhibit power law degree distribution, this is not the case for the Grid graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GitHub ml subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build the subgraphs\n",
    "subgraph_ml = GitHubGraph.subgraph([v for v in GitHubGraph.vs() if v[\"class\"] == \"ml\"])\n",
    "subgraph_ml = subgraph_ml.connected_components().giant()\n",
    "\n",
    "## estimates for l' (xmin) and gamma (alpha)\n",
    "deg = subgraph_ml.degree()\n",
    "X = powerlaw.Fit(deg)\n",
    "print(\"\\ngamma:\", X.power_law.alpha)\n",
    "print(\"l':\", X.power_law.xmin)\n",
    "print(\"KS statistic:\", X.power_law.D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Starting from l'\n",
    "fig1 = X.power_law.plot_ccdf(color=\"black\", linestyle=\"-\")\n",
    "fig1 = X.plot_ccdf(ax=fig1, linewidth=2, color=\"gray\", original_data=False, linestyle=\":\")\n",
    "fig1.set_xlabel(\"degree\", fontsize=13)\n",
    "fig1.set_ylabel(\"inverse cdf\", fontsize=13)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GitHub web subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subgraph_web = GitHubGraph.subgraph([v for v in GitHubGraph.vs() if v[\"class\"] == \"web\"])\n",
    "subgraph_web = subgraph_web.connected_components().giant()\n",
    "\n",
    "## estimates for l' (xmin) and gamma (alpha)\n",
    "deg = subgraph_web.degree()\n",
    "X = powerlaw.Fit(deg)\n",
    "print(\"\\ngamma:\", X.power_law.alpha)\n",
    "print(\"l':\", X.power_law.xmin)\n",
    "print(\"KS statistic:\", X.power_law.D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Starting from l'\n",
    "fig1 = X.power_law.plot_ccdf(color=\"black\", linestyle=\"-\")\n",
    "fig1 = X.plot_ccdf(ax=fig1, linewidth=2, color=\"gray\", original_data=False, linestyle=\":\")\n",
    "fig1.set_xlabel(\"degree\", fontsize=13)\n",
    "fig1.set_ylabel(\"inverse cdf\", fontsize=13)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid = ig.Graph.Read_Ncol(datadir + \"GridEurope/gridkit_europe-highvoltage.edges\", directed=False)\n",
    "Grid = Grid.simplify()\n",
    "## keep the giant component\n",
    "Grid = Grid.connected_components().giant()\n",
    "\n",
    "## estimates for l' (xmin) and gamma (alpha)\n",
    "deg = Grid.degree()\n",
    "X = powerlaw.Fit(deg)\n",
    "print(\"\\ngamma:\", X.power_law.alpha)\n",
    "print(\"l':\", X.power_law.xmin)\n",
    "print(\"KS statistic:\", X.power_law.D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Starting from l'\n",
    "fig1 = X.power_law.plot_ccdf(color=\"black\", linestyle=\"-\")\n",
    "fig1 = X.plot_ccdf(ax=fig1, linewidth=2, color=\"gray\", original_data=False, linestyle=\":\")\n",
    "fig1.set_xlabel(\"degree\", fontsize=13)\n",
    "fig1.set_ylabel(\"inverse cdf\", fontsize=13)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent sets\n",
    "\n",
    "Illustrating a few functions to find independent sets (a set of vertices no two of which are adjacent).\n",
    "The concept was defined in Chapter 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate random graph with (at least one) independent set\n",
    "## n: nodes, s: independent set size, d: avg degree\n",
    "def indepSet(n, s, d, seed=123):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    N = n - s\n",
    "    di = n * d // 2 - s * d\n",
    "    ## random graph with N nodes\n",
    "    g = ig.Graph.Erdos_Renyi(n=N, m=di)\n",
    "    ## extra nodes\n",
    "    g.add_vertices(s)\n",
    "    ## assign remaining degree to extra nodes\n",
    "    z = np.random.choice(np.arange(N, n), size=s * d)\n",
    "    deg = [x[1] for x in sorted(Counter(z).items())]\n",
    "    for i in range(len(deg)):\n",
    "        e = np.random.choice(N, deg[i], replace=False)\n",
    "        for j in e:\n",
    "            g.add_edge(j, i + N)\n",
    "    p = list(np.random.permutation(n))\n",
    "    G = g.permute_vertices(p)\n",
    "    return G\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 50 nodes, set size 10, average degree 20\n",
    "g = indepSet(50, 10, 20, seed=123)\n",
    "\n",
    "## every set of size min or more\n",
    "# ivs = g.independent_vertex_sets(min=9)\n",
    "\n",
    "## largest set(s) only\n",
    "ivs = g.largest_independent_vertex_sets()\n",
    "\n",
    "## maximal sets (that can't be extended)\n",
    "# ivs = g.maximal_independent_vertex_sets()\n",
    "\n",
    "print(g.independence_number())\n",
    "\n",
    "ivs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.vs['color'] = 'grey'\n",
    "for v in ivs:\n",
    "    g.vs[v]['color'] = 'red'\n",
    "ig.plot(g, bbox=(350,350), vertex_size=8, edge_color='lightgrey')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complexnetworks",
   "language": "python",
   "name": "complexnetworks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
