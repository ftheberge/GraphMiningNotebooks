{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - Centrality Measures\n",
    "\n",
    "In this notebook, we explore various centrality measures on a **weighted**, **directed** graph which represents the volume of passengers between US airports in 2008. This dataset is available at: https://www.kaggle.com/flashgordon/usa-airport-dataset#Airports2.csv which is part of the Kaggle Public Datasets: https://www.kaggle.com/datasets\n",
    "\n",
    "As with the previous notebooks, make sure to set the data directory properly in the second next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mode\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"../Datasets/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define colors from pale to dark\n",
    "colors = [\"gainsboro\", \"silver\", \"darkgray\", \"dimgray\", \"black\"]\n",
    "\n",
    "## we will use 3 node sizes:\n",
    "node_sizes = [6, 9, 12]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US Airport Graph\n",
    "\n",
    "### Volume of Passengers\n",
    "\n",
    "The nodes are represented by the 3-letter airport codes such as LAX (Los Angeles); \n",
    "each line below represents the number of passenges from ```orig_airport``` to ```dest_airport```.\n",
    "The last column is the volume of passengers that we use as **edge weights**. Thus we will build a weighted,  directed graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read edges and build weighted directed graph\n",
    "df = pd.read_csv(datadir + \"Airports/connections.csv\")\n",
    "g_airport = ig.Graph.TupleList([tuple(x) for x in df.values], directed=True, edge_attrs=[\"weight\"])\n",
    "df.head()  ## look at a few edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node attributes\n",
    "\n",
    "We read the node attributes in data frame ```Attr```:\n",
    "* lat/lon, which we will use as the graph layout\n",
    "* state (2-letter code)\n",
    "* city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read vertex attributes and add to graph\n",
    "Attr = pd.read_csv(datadir + \"Airports/airports_loc.csv\")\n",
    "\n",
    "## map airports in Attr to the node order in graph g\n",
    "lookup = {k: v for v, k in enumerate(Attr[\"airport\"])}\n",
    "l = [lookup[x] for x in g_airport.vs()[\"name\"]]\n",
    "\n",
    "## save lat/lon as tuples for each node:\n",
    "g_airport.vs()[\"layout\"] = [(Attr[\"lon\"][i], Attr[\"lat\"][i]) for i in l]\n",
    "g_airport.vs()[\"state\"] = [Attr[\"state\"][i] for i in l]\n",
    "g_airport.vs()[\"city\"] = [Attr[\"city\"][i] for i in l]\n",
    "Attr.head()  ## first few rows in Attr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add a few more attributes for visualization\n",
    "g_airport.vs()[\"size\"] = node_sizes[1]\n",
    "g_airport.vs()[\"color\"] = colors[3]\n",
    "g_airport.es()[\"color\"] = colors[0]\n",
    "g_airport.es()[\"arrow_size\"] = 0.5\n",
    "print(\"Airport graph:\", g_airport.vcount(), \"nodes and\", g_airport.ecount(), \"directed edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for loops and multiple edges\n",
    "\n",
    "There are no multiedges (not surprising, edges are weighted here), but there are some loops in the raw data,\n",
    "i.e. same origin and destination airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of loops:\", sum(g_airport.is_loop()))\n",
    "print(\"number of multi-edges:\", sum(g_airport.is_multiple()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.orig_airport==df.dest_airport]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connected components\n",
    "\n",
    "A (sub)graph is **weakly connected** if there is a path between any pair of nodes when we ignore the edge direction (i.e. treat the directed graph as undirected). The airport graph is weakly connected (that is, ignoring directionality) except for 2 airports: DET and WVL that are connected by a single directed edge.\n",
    "\n",
    "A (sub)graph is **strongly connected** if there is a directed path from each node to every other node. The airport graph is not strongly connected. The largest stongly connected component has size 425.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## count the number of nodes in the giant component (weak connectivity)\n",
    "print(\n",
    "    g_airport.connected_components(mode=\"weak\").giant().vcount(),\n",
    "    \"out of\",\n",
    "    g_airport.vcount(),\n",
    "    \"are in giant (weak) component\",\n",
    ")\n",
    "print(\n",
    "    g_airport.connected_components(mode=\"strong\").giant().vcount(),\n",
    "    \"out of\",\n",
    "    g_airport.vcount(),\n",
    "    \"are in giant (strong) component\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## which two airports are NOT weakly connected to the rest of the graph?\n",
    "conn_comp = g_airport.connected_components(mode=\"weak\").membership\n",
    "giant = mode(conn_comp)  ## giant component\n",
    "print(\"Disconnected airports:\")\n",
    "for i in range(g_airport.vcount()):\n",
    "    if conn_comp[i] != giant:\n",
    "        print(\n",
    "            g_airport.vs[i][\"name\"],\n",
    "            \"has in degree\",\n",
    "            g_airport.degree(i, mode=\"in\"),\n",
    "            \"and out degree\",\n",
    "            g_airport.degree(i, mode=\"out\"),\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coreness\n",
    "\n",
    "Looking at coreness (mode = 'all' means that we consider both in and out edges).\n",
    "We see a group of nodes with very high coreness: highly connected hub airports (such as 'SFO', 'LAX', 'ATL', etc.).\n",
    "There are also several nodes with low coreness: peripherial airports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_airport.vs[\"core\"] = g_airport.coreness(mode=\"all\")\n",
    "plt.hist(g_airport.vs[\"core\"], bins=20, width=3, color=colors[2])\n",
    "plt.xlabel(\"Coreness\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## print the airports with maximal coreness:\n",
    "max_core = np.max(g_airport.vs[\"core\"])\n",
    "print([v[\"name\"] for v in g_airport.vs if v[\"core\"] == max_core])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Degree distribution\n",
    "\n",
    "Below we plot the degree distribution, again with mode='all' (total degree, in and out).\n",
    "Which airport has maximal degree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## degree distribution\n",
    "g_airport.vs[\"degree\"] = g_airport.degree(mode=\"all\")\n",
    "plt.hist(g_airport.vs[\"degree\"], bins=20, width=14, color=colors[2])\n",
    "plt.xlabel(\"Total degree\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is different than \"merging\" in/out edges:\n",
    "max(g_airport.as_undirected().degree())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## max degree airport\n",
    "print(\"Airport with maximal degree:\", g_airport.vs[np.argmax(g_airport.vs[\"degree\"])][\"name\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## California Subgraph \n",
    "\n",
    "We will look at several **centrality** measures. To speed up the computation and plotting, we consider only the airports in **California**, and the edges within the state.\n",
    "You can try other states by changing the first line below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build smaller subgraph 'G' for California\n",
    "g_CA = g_airport.subgraph([v for v in g_airport.vs() if v[\"state\"] == \"CA\"])\n",
    "\n",
    "## drop isolated vertices (i.e. without in-state connections)\n",
    "g_CA = g_CA.subgraph([v for v in g_CA.vs() if v.degree() > 0])\n",
    "\n",
    "## remove loops if any\n",
    "g_CA = g_CA.simplify(loops=True, multiple=False)\n",
    "print(g_CA.vcount(), \"nodes and\", g_CA.ecount(), \"directed edges\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## The graph is weakly connected except for 2 airports\n",
    "conn_comp = g_CA.connected_components(mode=\"weak\").membership\n",
    "giant_comp = mode(conn_comp)\n",
    "g_CA.vs[\"color\"] = colors[2]\n",
    "print(\"Nodes outside the giant component:\")\n",
    "for i in range(g_CA.vcount()):\n",
    "    if conn_comp[i] != giant:\n",
    "        print(\n",
    "            g_CA.vs[i][\"name\"],\n",
    "            \"has in degree\",\n",
    "            g_CA.degree(i, mode=\"in\"),\n",
    "            \"and out degree\",\n",
    "            g_CA.degree(i, mode=\"out\"),\n",
    "        )\n",
    "        g_CA.vs[i][\"color\"] = colors[4]  ## darker color\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot this subgraph using lat/lon as layout\n",
    "ly = ig.Layout(g_CA.vs[\"layout\"])\n",
    "## y-axis goes top-down thus the inversion\n",
    "ly.mirror(1)\n",
    "ig.plot(g_CA, bbox=(0, 0, 350, 400), layout=ly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## same subgraph using a force directed layout\n",
    "ly = g_CA.layout_fruchterman_reingold()\n",
    "ig.plot(g_CA, bbox=(0, 0, 350, 400), layout=ly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrality measures\n",
    "\n",
    "Most measures defined in Chapter 3 of the book are available directly in ```igraph```.\n",
    "\n",
    "We compute the following centrality measures for the weighted graph g_CA:\n",
    "**PageRank**, **Authority** and **Hub**.\n",
    "For **degree centrality**, we define our own function below and we normalize the weights to get values bounded above by 1. \n",
    "\n",
    "For the distance based centrality measures **closeness**, **harmonic**, **eccentricity** and **betweenness**, we do not use the edges weights, so the distance between nodes is the number of hops, and is not based on the number of passengers. This is a natural choice here, since distance between airports (cities) can be viewed as the number of flights needed to travel between those cities.\n",
    "\n",
    "We compute the above centrality for every node in the g_CA subgraph.\n",
    "\n",
    "#### Warning for disconnected graphs\n",
    "\n",
    "The ```igraph``` function to compute **closeness centrality** considers connected components separately. Thus for example the two isolated nodes (with an edge between them) would get maximal value 1 for their centrality. We define our own function to compute closeness centrality as defined in the book, replacing distance (number of hops) with the total number of nodes when no path exists between two nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute the normalized edge weights\n",
    "max_weight = np.max(g_CA.es[\"weight\"])\n",
    "g_CA.es()[\"normalized_weight\"] = [w / max_weight for w in g_CA.es()[\"weight\"]]\n",
    "\n",
    "## directed degree centrality\n",
    "def degree_centrality(g, weights=None):\n",
    "    n = g.vcount()\n",
    "    if g.is_directed():\n",
    "        dc = [\n",
    "            sum(x) / (2 * (n - 1))\n",
    "            for x in zip(\n",
    "                g.strength(mode=\"in\", weights=weights), g.strength(mode=\"out\", weights=weights)\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        dc = [x / (n - 1) for x in g.strength(weights=weights)]\n",
    "    return dc\n",
    "\n",
    "\n",
    "## use distance = number of nodes below if disconnected\n",
    "def closeness_centrality(g):\n",
    "    n = g.vcount()\n",
    "    D = np.array(g.distances(mode=\"all\"))\n",
    "    D[D == np.inf] = n\n",
    "    return [(n - 1) / sum(D[i]) for i in range(len(D))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute several centrality measures for the California subgraph g_CA\n",
    "df_central = pd.DataFrame(\n",
    "    {\n",
    "        \"airport\": g_CA.vs()[\"name\"],\n",
    "        \"degree\": degree_centrality(g_CA, weights=\"normalized_weight\"),\n",
    "        \"pagerank\": g_CA.pagerank(weights=\"weight\"),\n",
    "        \"authority\": g_CA.authority_score(weights=\"weight\"),\n",
    "        \"hub\": g_CA.hub_score(weights=\"weight\"),\n",
    "        \"between\": g_CA.betweenness(),\n",
    "        \"harmonic\": g_CA.harmonic_centrality(),\n",
    "        \"closeness\": closeness_centrality(g_CA),\n",
    "        \"eccentricity\": g_CA.eccentricity(),\n",
    "    }\n",
    ")\n",
    "\n",
    "## normalize the betweenness values\n",
    "n = g_CA.vcount()\n",
    "df_central[\"between\"] = [2 * x / ((n - 1) * (n - 2)) for x in df_central[\"between\"]]\n",
    "\n",
    "## sort w.r.t. degree centrality, look at top airports\n",
    "df_central = df_central.sort_values(by=\"degree\", ascending=False)\n",
    "df_central.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bottom ones\n",
    "df_central.tail(5)\n",
    "# print(df_central.tail(5).to_latex(index=False,float_format=\"%.2e\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top airports\n",
    "\n",
    "The above results agree with intuition in terms of the most central airports in California.\n",
    "Note however that **SAN** (San Diego) has high values *except* for betweenness, an indication that connecting flights transit mainly via LAX or SFO. \n",
    "\n",
    "Below, we plot the California graph again, highlighting the top-3 airports w.r.t. **pagerank**: LAX, SFO, SAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reset node colours\n",
    "g_CA.vs()[\"color\"] = colors[2]\n",
    "\n",
    "## highlight top-3 airports w.r.t. pagerank\n",
    "g_CA.vs()[\"pagerank\"] = df_central[\"pagerank\"]\n",
    "for x in np.argsort(g_CA.vs()[\"pagerank\"])[-3:]:\n",
    "    g_CA.vs()[x][\"color\"] = colors[4]\n",
    "    g_CA.vs()[x][\"size\"] = node_sizes[2]\n",
    "ly = ig.Layout(g_CA.vs[\"layout\"])\n",
    "ly.mirror(1)\n",
    "ig.plot(g_CA, bbox=(0, 0, 350, 400), layout=ly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between measures\n",
    "\n",
    "We use the rank-based **Kendall-tau** correlation to compare the different centrality measures.\n",
    "\n",
    "We observe high agreement between all measures. In particular, degree-centrality, hub and authority measures are very highly correlated, and so are the distance-based measures (betweenness, closeness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## rank-based correlation between measures\n",
    "df_kt = df_central.corr(\"kendall\", numeric_only=True)\n",
    "df_kt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmonic vs closeness centrality\n",
    "\n",
    "By default, closeness centrality is computed **separately** on each **connected component**, which is why we defined our own function earlier, setting the distance equal to the number of nodes when no path exists between two nodes.\n",
    "This is one advantage of harmonic centrality, which works as is even with disconnected graphs.\n",
    "We illustrate this below, where we compute the 3 measures (harmonic, closeness with default behavior, closeness with our own definition). We report the results for 5 airports:\n",
    "\n",
    "* 3 major airports (LAX, SFO, SAN): all values are high\n",
    "* 2 disconnected airports (MCE, VIS): we see low values except when using the closeness centrality with default behavior, in which case the value is maximal (1). This can be misleading!\n",
    "\n",
    "There is a similar concern when computing **eccentricity** (maximum shortest distance), which is done separately for each connected component. For the California subgraph, all nodes have value 2 or 3, except the to disconnected airports, which have value of 1, as we see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Harmonic vs closeness centralities\n",
    "look_at = [\"LAX\", \"SFO\", \"SAN\", \"MCE\", \"VIS\"]\n",
    "df_central[\"closeness_default\"] = g_CA.closeness()\n",
    "df_sub = df_central[df_central.airport.isin(look_at)][\n",
    "    [\"airport\", \"harmonic\", \"closeness\", \"closeness_default\", \"eccentricity\"]\n",
    "]\n",
    "df_sub\n",
    "# print(df_sub.to_latex(index=False,float_format=\"%.3f\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at coreness\n",
    "\n",
    "We already looked at coreness for the whole airports graph, now we look at the California subgraph, again with mode='all'. Below we show nodes with maximal coreness as larger black dots, and nodes with small coreness as smaller dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot nodes w.r.t. coreness\n",
    "g_CA.vs[\"color\"] = colors[2]\n",
    "g_CA.vs[\"size\"] = node_sizes[1]\n",
    "g_CA.vs()[\"core\"] = g_CA.coreness()\n",
    "max_core = np.max(g_CA.vs()[\"core\"])\n",
    "min_core = np.min(g_CA.vs()[\"core\"])\n",
    "for v in g_CA.vs():\n",
    "    if v[\"core\"] == max_core:\n",
    "        v[\"color\"] = colors[4]\n",
    "        v[\"size\"] = node_sizes[2]\n",
    "    if v[\"core\"] <= min_core + 1:\n",
    "        v[\"color\"] = colors[0]\n",
    "        v[\"size\"] = node_sizes[0]\n",
    "ly = ig.Layout(g_CA.vs[\"layout\"])\n",
    "ly.mirror(1)\n",
    "ig.plot(g_CA, bbox=(0, 0, 350, 400), layout=ly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above uses the geographical layout, so it is not clear what is going on.\n",
    "\n",
    "Let's use a force directed layout to make the difference between high and low core number clearer. \n",
    "\n",
    "The high coreness nodes are clearly seen, and we also observe the small 2-node connected component that was buried in the previous visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Coreness is more clear here\n",
    "ly = g_CA.layout_kamada_kawai()\n",
    "ig.plot(g_CA, bbox=(0, 0, 300, 250), layout=ly)\n",
    "# ig.plot(g_CA, 'california_kamada.eps', bbox=(0,0,300,250),layout=ly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vertices with max coreness (13-core)\n",
    "## note that there are less than 14 nodes, this is an interesting remark and\n",
    "## it is because we consider both in and out-going edges by default for directed graph.\n",
    "V = [v[\"name\"] for v in g_CA.vs() if v[\"core\"] == max_core]\n",
    "print(\"max core value:\", max_core, \"\\nairports:\", V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at harmonic centrality\n",
    "\n",
    "Using the same layout as above (with high coreness nodes in the middle), we display the harmonic centrality scores.\n",
    "We clearly see higher values for central nodes, and small values for the small 2-node component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show harmonic centralities, same layout\n",
    "ix = np.round(g_CA.harmonic_centrality(), decimals=2)\n",
    "g_CA.vs[\"size\"] = 2\n",
    "ig.plot(g_CA, vertex_label=ix, layout=ly, bbox=(0, 0, 300, 250))\n",
    "# ig.plot(g_CA, 'california_harmonic.eps', vertex_label=ix, layout=ly, bbox=(0,0,300,250))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing coreness with other centrality measures\n",
    "\n",
    "We add coreness to data frame with centrality measures ```df_central```.\n",
    "We then group the data in 3 categories: high coreness (value of max_core), low (value of min_core+1 or less) or mid-range, and we compute and plot the mean for every other measure.\n",
    "\n",
    "We see that for all centrality measures except closeness centrality, the values are much higher for nodes with high coreness. The pagerank value for 'low' coreness nodes (close to 'mid' ones) is due to the two airports that are not part of the giant component.\n",
    "\n",
    "As expected, nodes with small coreness generally have smaller centrality scores. \n",
    "This is why for example we can often remove the small core nodes (for example, keeping only the 2-core) to reduce\n",
    "the size of large graphs without destroying its main structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## group in 3 categories\n",
    "g_CA.vs()[\"Core\"] = [\n",
    "    \"low\" if v[\"core\"] <= min_core+1 else \"high\" if v[\"core\"] == max_core else \"mid\" for v in g_CA.vs()\n",
    "]\n",
    "df_central[\"Coreness\"] = g_CA.vs[\"Core\"]\n",
    "df = df_central.groupby(\"Coreness\").median(numeric_only=True)\n",
    "df.sort_values(by=\"degree\", inplace=True, ascending=False)\n",
    "df = df.drop([\"closeness\", \"eccentricity\", \"closeness_default\"], axis=1)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## grouped barplot\n",
    "bh = np.array(df.loc[[\"high\"]])[0]\n",
    "bm = np.array(df.loc[[\"mid\"]])[0]\n",
    "bl = np.array(df.loc[[\"low\"]])[0]\n",
    "barWidth = 0.25\n",
    "\n",
    "# Set position of bar on X axis\n",
    "r1 = np.arange(len(bh))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "\n",
    "# Make the plot\n",
    "plt.bar(r1, bh, color=colors[4], width=barWidth, edgecolor=\"white\", label=\"high coreness\")\n",
    "plt.bar(r2, bm, color=colors[2], width=barWidth, edgecolor=\"white\", label=\"mid coreness\")\n",
    "plt.bar(r3, bl, color=colors[0], width=barWidth, edgecolor=\"white\", label=\"low coreness\")\n",
    "\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel(\"measure\", fontsize=14)\n",
    "plt.xticks([r + barWidth for r in range(len(bh))], df.columns, fontsize=10)\n",
    "plt.ylabel(\"score\", fontsize=14)\n",
    "\n",
    "# Create legend & Show graphic\n",
    "plt.legend(fontsize=12)\n",
    "# plt.savefig('California_core_vs_measures.eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta-centrality example\n",
    "\n",
    "This is the simple ''pandemic'' spread model as detailed in the book:\n",
    "\n",
    "*The ''pandemic'' starts at exactly one airport selected uniformly at random from all the airports. Then, the following rules for spreading are applied: (i) in a given airport pandemic lasts only for one round and (ii) in the next round, with probability $\\alpha$, the pandemic spreads independently along the flight routes to the destination airports for all connections starting from this airport. Airports can interact with the pandemic many times, and the process either goes on forever or the pandemic eventually dies out. \n",
    "Our goal is to find the sum over all airports of the expected number of times this airport has the pandemic.*\n",
    "\n",
    "We use $\\alpha$ = 0.1 and plot the (decreasing) delta centrality values in a barplot, using the same 3 colors are with the coreness plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Delta-centrality with a simple pandemic spread model\n",
    "\n",
    "## This measures the sum over all airports of the expected number of times\n",
    "## this airport has the pandemic\n",
    "def spread(g, alpha=0.1):\n",
    "    n = g.vcount()\n",
    "    I = np.diag(np.repeat(1, n))\n",
    "    A = np.array(g.get_adjacency().data)\n",
    "    One = np.ones((n, 1))\n",
    "    X = np.linalg.inv(I - alpha * np.transpose(A))\n",
    "    Y = np.reshape(X.dot(One) / n, n)\n",
    "    s = np.sum(Y)\n",
    "    if s>0:\n",
    "        return s\n",
    "    else:\n",
    "        return np.inf\n",
    "\n",
    "def spread_delta_centrality(g, alpha=0.1):\n",
    "    dc = []\n",
    "    spr = spread(g, alpha=alpha)  # P(G) in the book\n",
    "    for v in g.vs():\n",
    "        G = g.copy()\n",
    "        el = g.incident(v, mode=\"all\")\n",
    "        G.delete_edges(el)\n",
    "        dc.append((spr - spread(G, alpha=alpha)) / spr)\n",
    "    return dc\n",
    "\n",
    "print(\"Overall CA subgraph:\", spread(g_CA, alpha=.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the spread model with alpha = 0.1 and show the top airports\n",
    "g_CA.vs[\"delta\"] = spread_delta_centrality(g_CA, alpha=0.1)\n",
    "df_spread = pd.DataFrame(\n",
    "    np.transpose([g_CA.vs[\"name\"], g_CA.vs[\"delta\"], g_CA.vs[\"color\"]]),\n",
    "    columns=[\"airport\", \"delta\", \"color\"],\n",
    ")\n",
    "df_spread.sort_values(by=\"delta\", ascending=False, inplace=True)\n",
    "df_spread.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot using the same colors as with coreness plot\n",
    "heights = [float(x) for x in df_spread[\"delta\"]]\n",
    "bars = df_spread[\"airport\"]\n",
    "y_pos = range(len(bars))\n",
    "plt.bar(y_pos, heights, color=df_spread[\"color\"])\n",
    "\n",
    "# Rotation of the bars names\n",
    "plt.ylabel(\"Delta Centrality\", fontsize=12)\n",
    "plt.xticks(y_pos, bars, rotation=90)\n",
    "plt.yticks()\n",
    "\n",
    "# legend\n",
    "color_dict = {\"high coreness\": colors[4], \"med coreness\": colors[2], \"low coreness\": colors[0]}\n",
    "labels = list(color_dict.keys())\n",
    "handles = [plt.Rectangle((0, 0), 1, 1, color=color_dict[label]) for label in labels]\n",
    "plt.legend(handles, labels)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group centrality and centralization\n",
    "\n",
    "We go back to the full airports graph, and we ask the following questions:\n",
    "\n",
    "* which states have highest delta centralities with respect to efficiency?\n",
    "* what about centralization for each state subgraph?\n",
    "\n",
    "Computing efficiency involves the computation of shortest path lengths, which will cause a warning if the graph is disconnected. Warnings can be turned off by un-commenting the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## efficiency function given g\n",
    "def efficiency(g):\n",
    "    n = g.vcount()\n",
    "    s = 0\n",
    "    for i in range(n):\n",
    "        v = g.get_shortest_paths(i)\n",
    "        s += np.sum([1 / (len(x) - 1) for x in v if len(x) > 1])\n",
    "    return s / (n * (n - 1))\n",
    "\n",
    "## group delta centrality -- we compute for each state\n",
    "states = list(set(g_airport.vs()[\"state\"]))\n",
    "eff_us = efficiency(g_airport)\n",
    "dc = []\n",
    "for s in states:\n",
    "    v = [x for x in g_airport.vs() if x[\"state\"] == s]\n",
    "    G = g_airport.copy()\n",
    "    e = []\n",
    "    for x in v:\n",
    "        e.extend(g_airport.incident(x, mode=\"all\"))\n",
    "    G.delete_edges(list(set(e)))\n",
    "    dc.append((eff_us - efficiency(G)) / eff_us)\n",
    "\n",
    "## sort and show top states\n",
    "DC = pd.DataFrame({\"state\": states, \"delta_centrality\": dc})\n",
    "DC = DC.sort_values(by=\"delta_centrality\", ascending=False)\n",
    "DC.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ... and bottom states\n",
    "DC.tail(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For group centralization, we use the PageRank measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## group centralization (using PageRank) -- by state\n",
    "## look at states with more than 5 airports only\n",
    "states = list(set(g_airport.vs()[\"state\"]))\n",
    "page_rank = []\n",
    "st = []\n",
    "for s in states:\n",
    "    v = [x for x in g_airport.vs() if x[\"state\"] == s]\n",
    "    if len(v) > 5:  ## look at states with more than 5 airports only\n",
    "        G = g_airport.subgraph(v)\n",
    "        G = G.simplify(multiple=False)  ## drop self-loops\n",
    "        p = G.pagerank(weights=\"weight\")\n",
    "        page_rank.append(np.max(p) - np.mean(p))\n",
    "        st.append(s)\n",
    "\n",
    "## sort and show top\n",
    "DC = pd.DataFrame({\"State\": st, \"Pagerank Centralization\": page_rank})\n",
    "DC = DC.sort_values(by=\"Pagerank Centralization\", ascending=False)\n",
    "DC.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the state with highest PageRank centralization (Michigan).\n",
    "\n",
    "This is a state with one high degree airport (DTW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [x for x in g_airport.vs() if x[\"state\"] == \"MI\"]\n",
    "G = g_airport.subgraph(v)\n",
    "G = G.subgraph([v for v in G.vs() if v.degree() > 0])\n",
    "G = G.simplify(multiple=False)\n",
    "# ig.plot(G, 'central_MI.eps', bbox=(0,0,300,300))\n",
    "ig.plot(G, bbox=(0, 0, 300, 300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## one big hub airport: DTW (Detroit)\n",
    "G.vs[\"deg\"] = G.degree()  # overall degree\n",
    "for v in G.vs:\n",
    "    print(v[\"city\"], v[\"name\"], \"has degree\", v[\"deg\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the state with lowest PageRank centralization (ND).\n",
    "\n",
    "This is a state without high degree (hub) airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now the bottom  states\n",
    "DC.tail(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## North Dakota\n",
    "v = [x for x in g_airport.vs() if x[\"state\"] == \"ND\"]\n",
    "G = g_airport.subgraph(v)\n",
    "G = G.subgraph([v for v in G.vs() if v.degree() > 0])\n",
    "G = G.simplify(multiple=False)\n",
    "\n",
    "# ig.plot(G, 'central_ND.eps', bbox=(0,0,300,300))\n",
    "ig.plot(G, bbox=(0, 0, 300, 300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## no big hub city here\n",
    "G.vs[\"city\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should we expect for California? There are hub airports, but several ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## what about California?\n",
    "DC[DC[\"State\"] == \"CA\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3.1 - empirical tests\n",
    "\n",
    "The code below can be used to obtain the equivalent of Figure 3.1 in the book for different values of $n$, the number of nodes. Large $n$ values will generate a plot like the one in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## G(n,p) graph and k-cores - fraction of nodes in k-core vs average degree\n",
    "n = 25000\n",
    "random.seed(123)\n",
    "\n",
    "## Generate the graphs and store coreness \n",
    "## Vary average degree (thus, number of edges to generate)\n",
    "avg_deg = np.arange(0, 16.1, 0.5)\n",
    "n_edges = [int(n * i / 2) for i in avg_deg]\n",
    "\n",
    "C = []\n",
    "for m in n_edges:\n",
    "    g = ig.Graph.Erdos_Renyi(n=n, m=m)\n",
    "    C.extend(g.coreness())\n",
    "C = np.array(C).reshape(len(avg_deg), n)\n",
    "\n",
    "## Plot\n",
    "fig, ax = plt.subplots(1)\n",
    "S = [sum(C[i] >= 1) / n for i in range(len(avg_deg))]\n",
    "X = [avg_deg[i] for i in range(len(avg_deg)) if S[i] >= 0]\n",
    "Y = [S[i] for i in range(len(avg_deg)) if S[i] >= 0]\n",
    "ax.plot(X, Y)\n",
    "ax.text(0.2, 0, \"1\")\n",
    "\n",
    "for k in np.arange(2, 11, 1):\n",
    "    S = [sum(C[i] >= k) / n for i in range(len(avg_deg))]\n",
    "    X = [avg_deg[i] for i in range(len(avg_deg)) if S[i] > 0]\n",
    "    Y = [S[i] for i in range(len(avg_deg)) if S[i] > 0]\n",
    "    ax.plot(X, Y)\n",
    "    ax.text(np.min(X) + 0.2, np.min(Y), str(k))\n",
    "\n",
    "ax.set_xlabel(\"average degree\", fontsize=14)\n",
    "ax.set_ylabel(\"fraction of nodes\", fontsize=14)\n",
    "ax.set_title(\"Order of k-cores with \" + str(n) + \" nodes\", fontsize=14)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complexnetworks",
   "language": "python",
   "name": "complexnetworks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
