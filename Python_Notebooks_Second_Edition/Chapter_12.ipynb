{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness in complex network analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import igraph as ig\n",
    "import partition_igraph\n",
    "import random \n",
    "from abcd_graph import ABCDGraph, ABCDParams\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.genmod import families\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Web developers graph\n",
    "\n",
    "We look at the Web developers graph already introduced in Chapter 1.\n",
    "Recall that there are two classes of developers: machine learning (ml) and web.\n",
    "\n",
    "Variables:\n",
    "* **Y** = \"is a ml developer\", the target variable \n",
    "* **D_web** = number of neighbours that are web developers (\"web degree\")\n",
    "* **D_ml** = number of neighbours that are ml developers (\"ml degree\")\n",
    "* **X** = Protected variable (binary) ; we pick one the the supplied binary features from ther dataset ; this feature is correlated with Y.\n",
    "\n",
    "For the prediction models, we use logistic regression and pick a decision threshold $\\tau$ to maximize accuracy. \n",
    "\n",
    "The protected variable is significantly correlated with the target, so we expect the model to be unfair.\n",
    "It is also correlated with the degree-based features, so fairness by unawareness (removing X) will likely not be enough.\n",
    "\n",
    "We demonstrate the existence of unfairness w.r.t. X when predicting Y via the two measures \"DI\" and \"SP\" detailed in the textbook.\n",
    "\n",
    "The unfairness can be lowered using two different decision thresholds depending on the value of X, $\\tau_0$ and $\\tau_1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting the path to the datasets\n",
    "datadir = \"../Datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the GitHub developers edge list as tuples and build undirected graph\n",
    "## each node index is stored in vertex attribute \"id\"\n",
    "df = pd.read_csv(datadir + \"GitHubDevelopers/musae_git_edges.csv\")\n",
    "G = ig.Graph.TupleList(\n",
    "    [tuple(x) for x in df.values], directed=False, vertex_name_attr=\"id\"\n",
    ")\n",
    "\n",
    "## read node class (ml or web) and save: Y=1 for ml dev. and Y=0 for web dev. \n",
    "G_attr = pd.read_csv(datadir + \"GitHubDevelopers/musae_git_target.csv\")\n",
    "_ml = dict(zip(G_attr.id, G_attr.ml_target))\n",
    "G.vs[\"Y\"] = [_ml[i] for i in G.vs[\"id\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read feature 1793, set to 1 if present, 0 else\n",
    "## this will be the \"protected\" attribute \"X\", which is correlated with \"Y\"\n",
    "with open(datadir + \"GitHubDevelopers/musae_git_features.json\") as fp:\n",
    "    features = json.load(fp)\n",
    "G.vs['X'] = [int(1793 in features[str(i)]) for i in G.vs['id']]\n",
    "print(np.corrcoef(G.vs['X'], G.vs['Y'])[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build degree features from each node's neighbours\n",
    "for v in range(G.vcount()):\n",
    "    x = np.array([G.vs[i]['Y'] for i in G.neighbors(v)])\n",
    "    G.vs[v]['D_web'] = sum(x==0)\n",
    "    G.vs[v]['D_ml'] = sum(x==1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EDA - averages w.r.t. protected attribute\n",
    "data = pd.DataFrame({'D_web':[i for i in G.vs['D_web']], 'D_ml':[i for i in G.vs['D_ml']], 'X':G.vs['X'], 'Y':G.vs['Y']})\n",
    "data.groupby(by='X').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build logistic regression models \n",
    "\n",
    "* **m1**: including the protected attribute X\n",
    "* **m2**: excluding the protected attribute X\n",
    "\n",
    "For the degree feature, we transform all values from $d$ to $\\log(1+d)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## model (m1) with protected variable X - single threshold\n",
    "\n",
    "## first re-build data, taking 1+log for the degree-based features\n",
    "data = pd.DataFrame({'X_web':[np.log(1+i) for i in G.vs['D_web']], \\\n",
    "                     'X_ml':[np.log(1+i) for i in G.vs['D_ml']],  \\\n",
    "                     'X':G.vs['X'], 'Y':G.vs['Y']})\n",
    "\n",
    "## build model (m1)\n",
    "X_m1 = sm.add_constant(data[['X_web', 'X_ml','X']])\n",
    "model_m1 = sm.GLM(data['Y'], X_m1, family=sm.families.Binomial(link=families.links.Logit()))\n",
    "results_m1 = model_m1.fit()\n",
    "\n",
    "## model (m2) without protected variable X - single threshold\n",
    "\n",
    "## build model (m2)\n",
    "X_m2 = sm.add_constant(data[['X_web', 'X_ml']])\n",
    "model_m2 = sm.GLM(data['Y'], X_m2, family=sm.families.Binomial(link=families.links.Logit()))\n",
    "results_m2 = model_m2.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximize accuracy w.r.t. single threshold for both models\n",
    "\n",
    "We see that even if we drop the protected variable X (model m2), i.e. \"fairness through unawareness\", there is still unfairness as indicated by the DI and SP scores, albeit slightly lower.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to compute ACCuracy, DI and SP scores\n",
    "## data: contains at least 'X' (protected attribute) and 'Y' (response)\n",
    "## pred: predictions for the response\n",
    "def eval_pred(data, pred):\n",
    "    ACC = np.mean(data['Y'] == pred)\n",
    "    DI = np.abs(np.mean(pred[data['X']==1]) - np.mean(pred[data['X']==0]))\n",
    "    SP = np.sum([np.abs( np.mean( pred[(data['Y']==1-i) & (data['X']==1)]==i ) - \\\n",
    "                         np.mean( pred[(data['Y']==1-i) & (data['X']==0)]==i ) ) for i in [0,1] ])/2\n",
    "    return ACC, DI, SP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## threshold with best accuracy\n",
    "t = np.arange(0,1,.01)\n",
    "accs = [np.mean(data['Y'] == (results_m1.predict() >= i)) for i in t]\n",
    "idx = np.argmax(accs)\n",
    "pred = (results_m1.predict() >= t[idx])\n",
    "\n",
    "## print resulting stats\n",
    "acc, di, sp = eval_pred(data, pred)\n",
    "print('\\033[1m'+'model m1:','\\033[0m'+f'accuracy: {acc:.5f}',f'DI: {di:.5f}',f'SP: {sp:.5f}',f'tau: {t[idx]:.2f}')\n",
    "\n",
    "## threshold with best accuracy\n",
    "accs = [np.mean(data['Y'] == (results_m2.predict() >= i)) for i in t]\n",
    "idx = np.argmax(accs)\n",
    "pred = (results_m2.predict() > t[idx])\n",
    "\n",
    "## print resulting stats\n",
    "acc, di, sp = eval_pred(data, pred)\n",
    "print('\\033[1m'+'model m2:','\\033[0m'+f'accuracy: {acc:.5f}',f'DI: {di:.5f}',f'SP: {sp:.5f}',f'tau: {t[idx]:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now with separate thresholds using model m1\n",
    "\n",
    "Below is one-time work, we save (accuracy, DI, SP) for each value on a 100x100 grid covering thresholds $(\\tau_0, \\tau_1)$ for the two possible values of the protected variable.\n",
    "\n",
    "Next we can query/print those results as needed, setting upper bounds for the two measures DI and SP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## one-time work - save arrays for further queries ( < 1 min on MAC Pro 2022 )\n",
    "M_acc = np.zeros(shape=(100,100))\n",
    "M_di = np.zeros(shape=(100,100))\n",
    "M_sp = np.zeros(shape=(100,100))\n",
    "_pred = results_m1.predict() ## save the predicted probabilities from model m1\n",
    "\n",
    "## 100x100 grid\n",
    "for i in np.arange(0,100):\n",
    "    for j in np.arange(0,100):\n",
    "        tau = [i/100,j/100] ## the two thresholds\n",
    "        pred = np.array([v >= (tau[1] if c == 1 else tau[0]) for v, c in zip(_pred, data['X'])])\n",
    "        M_acc[i,j],M_di[i,j],M_sp[i,j] = eval_pred(data, pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query w.r.t. a few pairs of bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to query w.r.t. a pair of thresholds\n",
    "def query_results(M_acc, M_di, M_sp, th_di, th_sp):\n",
    "    mask = ((M_di < th_di) & (M_sp < th_sp))\n",
    "    _x, _y = np.where(mask)\n",
    "    acc = M_acc[mask]\n",
    "    idx = np.argmax(acc)\n",
    "    return (_x[idx]/100, _y[idx]/100, acc[idx], M_di[mask][idx], M_sp[mask][idx])\n",
    "\n",
    "## printing a few results\n",
    "_bounds = [(1,1),(.65,1),(.05,1),(1,.48),(1,.05),(.05,.05)]\n",
    "for (i,j) in _bounds:\n",
    "    t0,t1,acc,di,sp = query_results(M_acc, M_di, M_sp, th_di=i, th_sp=j)\n",
    "    print('\\033[1m'+f'DI<={i:.2f} and SP<={j:.2f}:',\n",
    "          '\\033[0m'+f' tau0: {t0:.2f}',f' tau1: {t1:.2f}',\n",
    "          f' accuracy: {acc:.5f}',f' DI: {di:.5f}',f' SP: {sp:.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting all grid for: accuracy, DI and SP respectively\n",
    "\n",
    "We see very similar pattern for the DI and SP scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(M_acc, cmap='gray_r')\n",
    "ax.invert_yaxis()\n",
    "ax.set_xticks(np.arange(0,100,4),np.arange(0,1,.04))\n",
    "ax.set_yticks(np.arange(0,100,4),np.arange(0,1,.04))\n",
    "plt.title('Accuracy', fontsize=20)\n",
    "plt.ylabel(r'$\\tau_0$', fontsize=16)\n",
    "plt.xlabel(r'$\\tau_1$', fontsize=16)\n",
    "plt.show()\n",
    "#plt.savefig('github_acc.png')\n",
    "plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(M_di, cmap='gray')\n",
    "ax.invert_yaxis()\n",
    "ax.set_xticks(np.arange(0,100,4),np.arange(0,1,.04))\n",
    "ax.set_yticks(np.arange(0,100,4),np.arange(0,1,.04))\n",
    "plt.title('DI', fontsize=20)\n",
    "plt.ylabel(r'$\\tau_0$', fontsize=16)\n",
    "plt.xlabel(r'$\\tau_1$', fontsize=16)\n",
    "plt.show()\n",
    "#plt.savefig('github_di.png')\n",
    "plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(M_sp, cmap='gray')\n",
    "ax.invert_yaxis()\n",
    "ax.set_xticks(np.arange(0,100,4),np.arange(0,1,.04))\n",
    "ax.set_yticks(np.arange(0,100,4),np.arange(0,1,.04))\n",
    "plt.title('SP', fontsize=20)\n",
    "plt.ylabel(r'$\\tau_0$', fontsize=16)\n",
    "plt.xlabel(r'$\\tau_1$', fontsize=16)\n",
    "plt.show()\n",
    "#plt.savefig('github_sp.png')\n",
    "plt.clf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) ABCD graph - predicting large community membership\n",
    "\n",
    "We generate an ABCD graph with one large community (size 1,000) and 10 small ones (size 100)\n",
    "\n",
    "Variables:\n",
    "* **Y** = \"is a member of the large community\", the target variable \n",
    "* **D** = node degree; the degree range is [30, 130)\n",
    "* **X** = Protected variable (binary) ; engineered to be negatively correlated with degree\n",
    "\n",
    "The prediction model is a simple threshold $\\tau$ on thre dergees D, and we optimize w.r.t. accuracy. We demonstrate the existence of unfairness w.r.t. X when predicting Y. \n",
    "\n",
    "The unfairness can be lowered using two different thresholds $(\\tau_0,\\tau_1)$ for D depending on the value of X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build ABCD graph, return igraph 'Graph' object\n",
    "\n",
    "## for reproducibility\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "## parameters - community sizes and degree sequence\n",
    "large_comm = 1000\n",
    "small_comm = 100\n",
    "n_small_comms = 10\n",
    "c = np.concatenate( (np.array([large_comm]),np.repeat(small_comm,n_small_comms)) )\n",
    "n_total = large_comm + (n_small_comms * small_comm)\n",
    "min_deg = 30\n",
    "degs = np.repeat(np.arange(min_deg,min_deg+100),20)\n",
    "\n",
    "## generate graph\n",
    "params = ABCDParams(degree_sequence=degs, community_size_sequence=c, vcount=n_total, xi=.1, num_outliers=0)\n",
    "g = ABCDGraph(params).build()\n",
    "G = g.exporter.to_igraph()\n",
    "\n",
    "## build dataframe\n",
    "df = pd.DataFrame()\n",
    "df['Y'] = (np.array(G.vs['ground_truth_community'])==0)\n",
    "df['D'] = G.degree()\n",
    "\n",
    "## add negatively correlated attribute X, i.e. lower degrees when True\n",
    "nodes = np.argsort(G.degree()) ## node from low to high degree\n",
    "proba = np.linspace(.75,.25,n_total)\n",
    "attribute = np.random.random(n_total) < proba\n",
    "G.vs[nodes]['attr']  = attribute\n",
    "df['X'] = G.vs['attr']\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## correlations\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximize accuracy w.r.t. single threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## single threshold\n",
    "t = np.arange(min_deg,min_deg+100)\n",
    "accs = [np.mean(df['Y'] == (df['D'] >= i)) for i in t]\n",
    "idx = np.argmax(accs)\n",
    "pred = (df['D'] >= t[idx])\n",
    "acc, di, sp = eval_pred(df, pred)\n",
    "print(f'accuracy: {acc:.3f}',f'DI: {di:.5f}',f'SP: {sp:.5f}','tau:',t[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now using two thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## one-time work - save for further queries ( < 7 sec on MAC Pro 2022 )\n",
    "M_acc = np.zeros(shape=(100,100))\n",
    "M_di = np.zeros(shape=(100,100))\n",
    "M_sp = np.zeros(shape=(100,100))\n",
    "\n",
    "## 100x100 grid\n",
    "for i in np.arange(0,100):\n",
    "    for j in np.arange(0,100):\n",
    "        tau = [i+min_deg,j+min_deg] ## the two thresholds\n",
    "        pred = np.array([v >= (tau[1] if c == 1 else tau[0]) for v, c in zip(df['D'], df['X'])])\n",
    "        M_acc[i,j],M_di[i,j],M_sp[i,j] = eval_pred(df, pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query w.r.t. a few pairs of bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_bounds = [(1,1),(.05,1),(1,.05),(.05,.05)]\n",
    "for (i,j) in _bounds:\n",
    "    t0,t1,acc,di,sp = query_results(M_acc, M_di, M_sp, th_di=i, th_sp=j)\n",
    "    tau = ( int(100*t0+min_deg), int(100*t1+min_deg) )\n",
    "    print('\\033[1m'+f'DI<={i:.2f} and SP<={j:.2f}:',\n",
    "          '\\033[0m'+f' accuracy: {acc:.5f}',f' DI: {di:.5f}',f' SP: {sp:.5f}','tau:',tau)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting all grid for: accuracy, DI and SP respectively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(M_acc, cmap='gray_r')\n",
    "ax.invert_yaxis()\n",
    "ax.set_xticks(np.arange(0,100,4),np.arange(min_deg,min_deg+100,4))\n",
    "ax.set_yticks(np.arange(0,100,4),np.arange(min_deg,min_deg+100,4))\n",
    "plt.title('Accuracy', fontsize=20)\n",
    "plt.ylabel(r'$\\tau_0$', fontsize=16)\n",
    "plt.xlabel(r'$\\tau_1$', fontsize=16)\n",
    "plt.show()\n",
    "#plt.savefig('abcd_acc.png')\n",
    "plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(M_di, cmap='gray')\n",
    "ax.invert_yaxis()\n",
    "ax.set_xticks(np.arange(0,100,4),np.arange(min_deg,min_deg+100,4))\n",
    "ax.set_yticks(np.arange(0,100,4),np.arange(min_deg,min_deg+100,4))\n",
    "plt.title('DI', fontsize=20)\n",
    "plt.ylabel(r'$\\tau_0$', fontsize=16)\n",
    "plt.xlabel(r'$\\tau_1$', fontsize=16)\n",
    "plt.show()\n",
    "#plt.savefig('abcd_di.png')\n",
    "plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(M_sp, cmap='gray')\n",
    "ax.invert_yaxis()\n",
    "ax.set_xticks(np.arange(0,100,4),np.arange(min_deg,min_deg+100,4))\n",
    "ax.set_yticks(np.arange(0,100,4),np.arange(min_deg,min_deg+100,4))\n",
    "plt.title('SP', fontsize=20)\n",
    "plt.ylabel(r'$\\tau_0$', fontsize=16)\n",
    "plt.xlabel(r'$\\tau_1$', fontsize=16)\n",
    "plt.show()\n",
    "#plt.savefig('abcd_sp.png')\n",
    "plt.clf()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complexnetworks",
   "language": "python",
   "name": "complexnetworks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
